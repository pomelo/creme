{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! \u00b6 creme is a Python library for online machine learning . All the tools in the library can be updated with a single observation at a time, and can therefore be used to process streaming data. This is general-purpose library, and therefore caters to different machine learning problems, including regression, classification, and unsupervised learning. It can also be used for adhoc tasks, such as computing online metrics, as well as performing drift detection. We recommend that new users begin with the getting started guide . Afterwards, you can check out the user guide , which contains recipes and tutorials on specific topics. The API reference is a catalogue of all the available modules, classes, and functions that are available. Finally, check out the examples section for comprehensive usage examples. Feel free to contribute in any way you like, we're always open to new ideas and approaches. You can also take a look at the issue tracker and the icebox to see if anything takes your fancy. Please check out the contribution guidelines if you want to bring modifications to the code base. You can view the list of people who have contributed here .","title":"Welcome"},{"location":"#welcome","text":"creme is a Python library for online machine learning . All the tools in the library can be updated with a single observation at a time, and can therefore be used to process streaming data. This is general-purpose library, and therefore caters to different machine learning problems, including regression, classification, and unsupervised learning. It can also be used for adhoc tasks, such as computing online metrics, as well as performing drift detection. We recommend that new users begin with the getting started guide . Afterwards, you can check out the user guide , which contains recipes and tutorials on specific topics. The API reference is a catalogue of all the available modules, classes, and functions that are available. Finally, check out the examples section for comprehensive usage examples. Feel free to contribute in any way you like, we're always open to new ideas and approaches. You can also take a look at the issue tracker and the icebox to see if anything takes your fancy. Please check out the contribution guidelines if you want to bring modifications to the code base. You can view the list of people who have contributed here .","title":"Welcome!"},{"location":"faq/","text":"FAQ \u00b6 Do all classifiers support multi-class classification? No, they don't. Although binary classification can be seen as a special case of multi-class classification, there are many optimizations that can be performed if we know that there are only two classes. It would be annoying to have to check whether this is the case in an online setting. All in all we find that separating both cases leads to much cleaner code. Note that the multiclass module contains wrapper models that enable you to perform multi-class classification with binary classifiers. How do I know if a classifier supports multi-class classification? Each classifier that is part of creme is either a base.BinaryClassifier or a base.MultiClassifier . You can use Python's isinstance function to check for a particular classifier, as so: >>> from creme import base >>> from creme import linear_model >>> classifier = linear_model . LogisticRegression () >>> isinstance ( classifier , base . BinaryClassifier ) True >>> isinstance ( classifier , base . MultiClassifier ) False Why doesn't creme do any input validation? Python encourages a coding style called EAFP , which stands for \"Easier to Ask for Forgiveness than Permission\". The idea is to assume that runtime errors don't occur, and instead use try/expects to catch errors. The great benefit is that we don't have to drown our code with if statements, which is symptomatic of the LBYL style , which stands for \"look before you leap\". This makes our implementations much more readable than, say, scikit-learn, which does a lot of input validation. The catch is that users have to be careful to use sane inputs. As always, there is no free lunch ! What about reinforcement learning? Reinforcement learning works in an online manner because of the nature of the task. Reinforcement learning can be therefore be seen as a subcase of online machine learning. However, we prefer not to support it because there are already many existing opensource libraries dedicated to it. What are the differences between scikit-learn's online learning algorithm which have a partial_fit method and their equivalents in creme? The algorithms from sklearn that support incremental learning are mostly meant for mini-batch learning. In a pure streaming context where the observations arrive one by one, then creme is much faster than sklearn . This is mostly because sklearn incurs a lot of overhead by performing data checks. Also, sklearn assumes that you're always using the same number of features. This is not the case with creme because it use dictionaries which allows you to drop and add features as you wish. How do I save and load models? >>> from creme import tree >>> import pickle >>> model = tree . RandomForestClassifier () # save >>> with open ( 'model.pkl' , 'wb' ) as f : ... pickle . dump ( model , f ) # load >>> with open ( 'model.pkl' , 'rb' ) as f : ... model = pickle . load ( f ) We also encourage you to try out dill and cloudpickle . What about neural networks? There are many great open-source libraries for building neural network models. We don't feel that we can bring anything of value to the existing Python ecosystem. However, we are open to implementing compatibility wrappers for popular libraries such as PyTorch and Keras. Who are the authors of this library? Most of the development is coming from a group of friends from the city of Toulouse, in France. Some of us are doing a PhD, while the rest are research engineers.","title":"FAQ"},{"location":"faq/#faq","text":"Do all classifiers support multi-class classification? No, they don't. Although binary classification can be seen as a special case of multi-class classification, there are many optimizations that can be performed if we know that there are only two classes. It would be annoying to have to check whether this is the case in an online setting. All in all we find that separating both cases leads to much cleaner code. Note that the multiclass module contains wrapper models that enable you to perform multi-class classification with binary classifiers. How do I know if a classifier supports multi-class classification? Each classifier that is part of creme is either a base.BinaryClassifier or a base.MultiClassifier . You can use Python's isinstance function to check for a particular classifier, as so: >>> from creme import base >>> from creme import linear_model >>> classifier = linear_model . LogisticRegression () >>> isinstance ( classifier , base . BinaryClassifier ) True >>> isinstance ( classifier , base . MultiClassifier ) False Why doesn't creme do any input validation? Python encourages a coding style called EAFP , which stands for \"Easier to Ask for Forgiveness than Permission\". The idea is to assume that runtime errors don't occur, and instead use try/expects to catch errors. The great benefit is that we don't have to drown our code with if statements, which is symptomatic of the LBYL style , which stands for \"look before you leap\". This makes our implementations much more readable than, say, scikit-learn, which does a lot of input validation. The catch is that users have to be careful to use sane inputs. As always, there is no free lunch ! What about reinforcement learning? Reinforcement learning works in an online manner because of the nature of the task. Reinforcement learning can be therefore be seen as a subcase of online machine learning. However, we prefer not to support it because there are already many existing opensource libraries dedicated to it. What are the differences between scikit-learn's online learning algorithm which have a partial_fit method and their equivalents in creme? The algorithms from sklearn that support incremental learning are mostly meant for mini-batch learning. In a pure streaming context where the observations arrive one by one, then creme is much faster than sklearn . This is mostly because sklearn incurs a lot of overhead by performing data checks. Also, sklearn assumes that you're always using the same number of features. This is not the case with creme because it use dictionaries which allows you to drop and add features as you wish. How do I save and load models? >>> from creme import tree >>> import pickle >>> model = tree . RandomForestClassifier () # save >>> with open ( 'model.pkl' , 'wb' ) as f : ... pickle . dump ( model , f ) # load >>> with open ( 'model.pkl' , 'rb' ) as f : ... model = pickle . load ( f ) We also encourage you to try out dill and cloudpickle . What about neural networks? There are many great open-source libraries for building neural network models. We don't feel that we can bring anything of value to the existing Python ecosystem. However, we are open to implementing compatibility wrappers for popular libraries such as PyTorch and Keras. Who are the authors of this library? Most of the development is coming from a group of friends from the city of Toulouse, in France. Some of us are doing a PhD, while the rest are research engineers.","title":"FAQ"},{"location":"getting-started/","text":"Getting started \u00b6 First things first, make sure you have installed creme . In creme , features are represented with dictionaries, where the keys correspond to the features names. For instance: import datetime as dt x = { 'shop' : 'Ikea' , 'city' : 'Stockholm' , 'date' : dt . datetime ( 2020 , 6 , 1 ), 'sales' : 42 } It is up to you, the user, to decide how to stream your data. creme offers a stream module which has various utilities for handling streaming data, such as stream.iter_csv . For the sake of example, creme also provides a datasets module which contains various streaming datasets. For example, the datasets.Phishing dataset contains records of phishing attempts on web pages. from creme import datasets dataset = datasets . Phishing () print ( dataset ) <creme.datasets.phishing.Phishing at 0x11561f3d0> The dataset is a streaming dataset, and therefore doesn't sit in memory. Instead, we can loop over each sample with a for loop: for x , y in dataset : pass print ( x ) {'empty_server_form_handler': 1.0, 'popup_window': 0.5, 'https': 1.0, 'request_from_other_domain': 1.0, 'anchor_from_other_domain': 1.0, 'is_popular': 0.5, 'long_url': 0.0, 'age_of_domain': 0, 'ip_in_url': 0} print ( y ) False Typically, models learn via a learn_one(x, y) method, which takes as input some features and a target value. Being able to learn with a single instance gives a lot of flexibility. For instance, a model can be updated whenever a new sample arrives from a stream. To exemplify this, let's train a logistic regression on the above dataset. from creme import linear_model model = linear_model . LogisticRegression () for x , y in dataset : model . learn_one ( x , y ) Predictions can be obtained by calling a model's predict_one method. In the case of a classifier, we can also use predict_proba_one to produce probability estimates. model = linear_model . LogisticRegression () for x , y in dataset : y_pred = model . predict_proba_one ( x ) model . learn_one ( x , y ) print ( y_pred ) {False: 0.7731541581376543, True: 0.22684584186234572} The metrics module gives access to many metrics that are commonly used in machine learning. Like the rest of creme , these metrics can be updated with one element at a time: from creme import metrics model = linear_model . LogisticRegression () metric = metrics . ROCAUC () for x , y in dataset : y_pred = model . predict_proba_one ( x ) model . learn_one ( x , y ) metric . update ( y , y_pred ) metric ROCAUC: 0.893565 A common way to improve the performance of a logistic regression is to scale the data. This can be done by using a preprocessing.StandardScaler . In particular, we can define a pipeline to organise our model into a sequence of steps: from creme import compose from creme import preprocessing model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) model . draw () metric = metrics . ROCAUC () for x , y in datasets . Phishing (): y_pred = model . predict_proba_one ( x ) model . learn_one ( x , y ) metric . update ( y , y_pred ) metric ROCAUC: 0.950224 As we can see, the model is performing much better now that the data is being scaled. Under the hood, the standard scaler maintains a running average and a running variance for each feature in the dataset. Each feature is thus scaled according to the average and the variance seen up to every given point in time. This concludes this short guide to getting started with creme . There is a lot more to discover and understand. Head towards the user guide for recipes on how to perform common machine learning tasks. You may also consult the API reference , which is a catalogue of all the modules that creme exposes. Finally, the examples section contains comprehensive examples for various usecases.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"First things first, make sure you have installed creme . In creme , features are represented with dictionaries, where the keys correspond to the features names. For instance: import datetime as dt x = { 'shop' : 'Ikea' , 'city' : 'Stockholm' , 'date' : dt . datetime ( 2020 , 6 , 1 ), 'sales' : 42 } It is up to you, the user, to decide how to stream your data. creme offers a stream module which has various utilities for handling streaming data, such as stream.iter_csv . For the sake of example, creme also provides a datasets module which contains various streaming datasets. For example, the datasets.Phishing dataset contains records of phishing attempts on web pages. from creme import datasets dataset = datasets . Phishing () print ( dataset ) <creme.datasets.phishing.Phishing at 0x11561f3d0> The dataset is a streaming dataset, and therefore doesn't sit in memory. Instead, we can loop over each sample with a for loop: for x , y in dataset : pass print ( x ) {'empty_server_form_handler': 1.0, 'popup_window': 0.5, 'https': 1.0, 'request_from_other_domain': 1.0, 'anchor_from_other_domain': 1.0, 'is_popular': 0.5, 'long_url': 0.0, 'age_of_domain': 0, 'ip_in_url': 0} print ( y ) False Typically, models learn via a learn_one(x, y) method, which takes as input some features and a target value. Being able to learn with a single instance gives a lot of flexibility. For instance, a model can be updated whenever a new sample arrives from a stream. To exemplify this, let's train a logistic regression on the above dataset. from creme import linear_model model = linear_model . LogisticRegression () for x , y in dataset : model . learn_one ( x , y ) Predictions can be obtained by calling a model's predict_one method. In the case of a classifier, we can also use predict_proba_one to produce probability estimates. model = linear_model . LogisticRegression () for x , y in dataset : y_pred = model . predict_proba_one ( x ) model . learn_one ( x , y ) print ( y_pred ) {False: 0.7731541581376543, True: 0.22684584186234572} The metrics module gives access to many metrics that are commonly used in machine learning. Like the rest of creme , these metrics can be updated with one element at a time: from creme import metrics model = linear_model . LogisticRegression () metric = metrics . ROCAUC () for x , y in dataset : y_pred = model . predict_proba_one ( x ) model . learn_one ( x , y ) metric . update ( y , y_pred ) metric ROCAUC: 0.893565 A common way to improve the performance of a logistic regression is to scale the data. This can be done by using a preprocessing.StandardScaler . In particular, we can define a pipeline to organise our model into a sequence of steps: from creme import compose from creme import preprocessing model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) model . draw () metric = metrics . ROCAUC () for x , y in datasets . Phishing (): y_pred = model . predict_proba_one ( x ) model . learn_one ( x , y ) metric . update ( y , y_pred ) metric ROCAUC: 0.950224 As we can see, the model is performing much better now that the data is being scaled. Under the hood, the standard scaler maintains a running average and a running variance for each feature in the dataset. Each feature is thus scaled according to the average and the variance seen up to every given point in time. This concludes this short guide to getting started with creme . There is a lot more to discover and understand. Head towards the user guide for recipes on how to perform common machine learning tasks. You may also consult the API reference , which is a catalogue of all the modules that creme exposes. Finally, the examples section contains comprehensive examples for various usecases.","title":"Getting started"},{"location":"installation/","text":"Installation \u00b6 creme is intended to work with Python 3.6 or above . Installation can be done via pip : pip install creme There are wheels available for Linux, MacOS, and Windows, which means that in most cases you won't have to build creme from source. You can install the latest development version from GitHub as so: pip install git+https://github.com/creme-ml/creme --upgrade Or, through SSH: pip install git+ssh://git@github.com/creme-ml/creme.git --upgrade Feel welcome to open an issue on GitHub if you are having any trouble.","title":"Installation"},{"location":"installation/#installation","text":"creme is intended to work with Python 3.6 or above . Installation can be done via pip : pip install creme There are wheels available for Linux, MacOS, and Windows, which means that in most cases you won't have to build creme from source. You can install the latest development version from GitHub as so: pip install git+https://github.com/creme-ml/creme --upgrade Or, through SSH: pip install git+ssh://git@github.com/creme-ml/creme.git --upgrade Feel welcome to open an issue on GitHub if you are having any trouble.","title":"Installation"},{"location":"api-reference/overview/","text":"Overview \u00b6 evaluate \u00b6 Model evaluation. progressive_val_score linear_model \u00b6 Linear models. LinearRegression optim \u00b6 Stochastic optimization. Optimizer initializers \u00b6 Weight initializers. losses \u00b6 Loss functions. Each loss function is intented to work with both single values as well as numpy vectors. schedulers \u00b6 Learning rate schedulers.","title":"Overview"},{"location":"api-reference/overview/#overview","text":"","title":"Overview"},{"location":"api-reference/overview/#evaluate","text":"Model evaluation. progressive_val_score","title":"evaluate"},{"location":"api-reference/overview/#linear_model","text":"Linear models. LinearRegression","title":"linear_model"},{"location":"api-reference/overview/#optim","text":"Stochastic optimization. Optimizer","title":"optim"},{"location":"api-reference/overview/#initializers","text":"Weight initializers.","title":"initializers"},{"location":"api-reference/overview/#losses","text":"Loss functions. Each loss function is intented to work with both single values as well as numpy vectors.","title":"losses"},{"location":"api-reference/overview/#schedulers","text":"Learning rate schedulers.","title":"schedulers"},{"location":"api-reference/evaluate/progressive-val-score/","text":"progressive_val_score \u00b6 Evaluates the performance of a model on a streaming dataset. This method is the canonical way to evaluate a model's performance. When used correctly, it allows you to exactly assess how a model would have performed in a production scenario. dataset is converted into a stream of questions and answers. At each step the model is either asked to predict an observation, or is either updated. The target is only revealed to the model after a certain amount of time, which is determined by the delay parameter. Note that under the hood this uses the stream.simulate_qa function to go through the data in arrival order. By default, there is no delay, which means that the samples are processed one after the other. When there is no delay, this function essentially performs progressive validation. When there is a delay, then we refer to it as delayed progressive validation. It is recommended to use this method when you want to determine a model's performance on a dataset. In particular, it is advised to use the delay parameter in order to get a reliable assessment. Indeed, in a production scenario, it is often the case that ground truths are made available after a certain amount of time. By using this method, you can reproduce this scenario and therefore truthfully assess what would have been the performance of a model on a given dataset. Parameters \u00b6 dataset ( Iterator[Tuple[dict, Any]] ) The stream of observations against which the model will be evaluated. model ( base.Predictor ) The model to evaluate. metric ( creme.metrics.base.Metric ) The metric used to evaluate the model's predictions. moment ( Union[str, Callable] ) \u2013 defaults to None The attribute used for measuring time. If a callable is passed, then it is expected to take as input a dict of features. If None , then the observations are implicitely timestamped in the order in which they arrive. delay ( Union[str, int, datetime.timedelta, Callable] ) \u2013 defaults to None The amount to wait before revealing the target associated with each observation to the model. This value is expected to be able to sum with the moment value. For instance, if moment is a datetime.date , then delay is expected to be a datetime.timedelta . If a callable is passed, then it is expected to take as input a dict of features and the target. If a str is passed, then it will be used to access the relevant field from the features. If None is passed, then no delay will be used, which leads to doing standard online validation. print_every \u2013 defaults to 0 Iteration number at which to print the current metric. This only takes into account the predictions, and not the training steps. show_time \u2013 defaults to False Whether or not to display the elapsed time. show_memory \u2013 defaults to False Whether or not to display the memory usage of the model. print_kwargs Extra keyword arguments are passed to the print function. For instance, this allows providing a file argument, which indicates where to output progress. Examples \u00b6 Take the following model: >>> from creme import linear_model >>> from creme import preprocessing >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ) We can evaluate it on the Phishing dataset as so: >>> from creme import datasets >>> from creme import evaluate >>> from creme import metrics >>> evaluate . progressive_val_score ( ... model = model , ... dataset = datasets . Phishing (), ... metric = metrics . ROCAUC (), ... print_every = 200 ... ) [ 200 ] ROCAUC : 0.897995 [ 400 ] ROCAUC : 0.920896 [ 600 ] ROCAUC : 0.931339 [ 800 ] ROCAUC : 0.939909 [ 1 , 000 ] ROCAUC : 0.947417 [ 1 , 200 ] ROCAUC : 0.950304 ROCAUC : 0.950363 We haven't specified a delay, therefore this is strictly equivalent to the following piece of code: >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ) >>> metric = metrics . ROCAUC () >>> for x , y in datasets . Phishing (): ... y_pred = model . predict_proba_one ( x ) ... metric = metric . update ( y , y_pred ) ... model = model . learn_one ( x , y ) >>> metric ROCAUC : 0.950363 When print_every is specified, the current state is printed at regular intervals. Under the hood, Python's print method is being used. You can pass extra keyword arguments to modify its behavior. For instance, you may use the file argument if you want to log the progress to a file of your choice. >>> with open ( 'progress.log' , 'w' ) as f : ... metric = evaluate . progressive_val_score ( ... model = model , ... dataset = datasets . Phishing (), ... metric = metrics . ROCAUC (), ... print_every = 200 , ... file = f ... ) >>> with open ( 'progress.log' ) as f : ... for line in f . read () . splitlines (): ... print ( line ) [ 200 ] ROCAUC : 0.94 [ 400 ] ROCAUC : 0.946969 [ 600 ] ROCAUC : 0.9517 [ 800 ] ROCAUC : 0.954238 [ 1 , 000 ] ROCAUC : 0.958207 [ 1 , 200 ] ROCAUC : 0.96002 Note that the performance is slightly better than above because we haven't used a fresh copy of the model. Instead, we've reused the existing model which has already done a full pass on the data. >>> import os ; os . remove ( 'progress.log' ) References Beating the Hold-Out: Bounds for K-fold and Progressive Cross-Validation \u21a9 Grzenda, M., Gomes, H.M. and Bifet, A., 2019. Delayed labelling evaluation for data streams. Data Mining and Knowledge Discovery, pp.1-30 \u21a9","title":"progressive_val_score"},{"location":"api-reference/evaluate/progressive-val-score/#progressive_val_score","text":"Evaluates the performance of a model on a streaming dataset. This method is the canonical way to evaluate a model's performance. When used correctly, it allows you to exactly assess how a model would have performed in a production scenario. dataset is converted into a stream of questions and answers. At each step the model is either asked to predict an observation, or is either updated. The target is only revealed to the model after a certain amount of time, which is determined by the delay parameter. Note that under the hood this uses the stream.simulate_qa function to go through the data in arrival order. By default, there is no delay, which means that the samples are processed one after the other. When there is no delay, this function essentially performs progressive validation. When there is a delay, then we refer to it as delayed progressive validation. It is recommended to use this method when you want to determine a model's performance on a dataset. In particular, it is advised to use the delay parameter in order to get a reliable assessment. Indeed, in a production scenario, it is often the case that ground truths are made available after a certain amount of time. By using this method, you can reproduce this scenario and therefore truthfully assess what would have been the performance of a model on a given dataset.","title":"progressive_val_score"},{"location":"api-reference/evaluate/progressive-val-score/#parameters","text":"dataset ( Iterator[Tuple[dict, Any]] ) The stream of observations against which the model will be evaluated. model ( base.Predictor ) The model to evaluate. metric ( creme.metrics.base.Metric ) The metric used to evaluate the model's predictions. moment ( Union[str, Callable] ) \u2013 defaults to None The attribute used for measuring time. If a callable is passed, then it is expected to take as input a dict of features. If None , then the observations are implicitely timestamped in the order in which they arrive. delay ( Union[str, int, datetime.timedelta, Callable] ) \u2013 defaults to None The amount to wait before revealing the target associated with each observation to the model. This value is expected to be able to sum with the moment value. For instance, if moment is a datetime.date , then delay is expected to be a datetime.timedelta . If a callable is passed, then it is expected to take as input a dict of features and the target. If a str is passed, then it will be used to access the relevant field from the features. If None is passed, then no delay will be used, which leads to doing standard online validation. print_every \u2013 defaults to 0 Iteration number at which to print the current metric. This only takes into account the predictions, and not the training steps. show_time \u2013 defaults to False Whether or not to display the elapsed time. show_memory \u2013 defaults to False Whether or not to display the memory usage of the model. print_kwargs Extra keyword arguments are passed to the print function. For instance, this allows providing a file argument, which indicates where to output progress.","title":"Parameters"},{"location":"api-reference/evaluate/progressive-val-score/#examples","text":"Take the following model: >>> from creme import linear_model >>> from creme import preprocessing >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ) We can evaluate it on the Phishing dataset as so: >>> from creme import datasets >>> from creme import evaluate >>> from creme import metrics >>> evaluate . progressive_val_score ( ... model = model , ... dataset = datasets . Phishing (), ... metric = metrics . ROCAUC (), ... print_every = 200 ... ) [ 200 ] ROCAUC : 0.897995 [ 400 ] ROCAUC : 0.920896 [ 600 ] ROCAUC : 0.931339 [ 800 ] ROCAUC : 0.939909 [ 1 , 000 ] ROCAUC : 0.947417 [ 1 , 200 ] ROCAUC : 0.950304 ROCAUC : 0.950363 We haven't specified a delay, therefore this is strictly equivalent to the following piece of code: >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ) >>> metric = metrics . ROCAUC () >>> for x , y in datasets . Phishing (): ... y_pred = model . predict_proba_one ( x ) ... metric = metric . update ( y , y_pred ) ... model = model . learn_one ( x , y ) >>> metric ROCAUC : 0.950363 When print_every is specified, the current state is printed at regular intervals. Under the hood, Python's print method is being used. You can pass extra keyword arguments to modify its behavior. For instance, you may use the file argument if you want to log the progress to a file of your choice. >>> with open ( 'progress.log' , 'w' ) as f : ... metric = evaluate . progressive_val_score ( ... model = model , ... dataset = datasets . Phishing (), ... metric = metrics . ROCAUC (), ... print_every = 200 , ... file = f ... ) >>> with open ( 'progress.log' ) as f : ... for line in f . read () . splitlines (): ... print ( line ) [ 200 ] ROCAUC : 0.94 [ 400 ] ROCAUC : 0.946969 [ 600 ] ROCAUC : 0.9517 [ 800 ] ROCAUC : 0.954238 [ 1 , 000 ] ROCAUC : 0.958207 [ 1 , 200 ] ROCAUC : 0.96002 Note that the performance is slightly better than above because we haven't used a fresh copy of the model. Instead, we've reused the existing model which has already done a full pass on the data. >>> import os ; os . remove ( 'progress.log' ) References Beating the Hold-Out: Bounds for K-fold and Progressive Cross-Validation \u21a9 Grzenda, M., Gomes, H.M. and Bifet, A., 2019. Delayed labelling evaluation for data streams. Data Mining and Knowledge Discovery, pp.1-30 \u21a9","title":"Examples"},{"location":"api-reference/linear-model/LinearRegression/","text":"LinearRegression \u00b6 Linear regression. This estimator supports learning with mini-batches. On top of the single instance methods, it provides the following methods: learn_many , predict_many , predict_proba_many . Each method takes as input a pandas.DataFrame where each column represents a feature. It is generally a good idea to scale the data beforehand in order for the optimizer to converge. You can do this online with a preprocessing.StandardScaler . Parameters \u00b6 optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the weights. Note that the intercept updates are handled separately. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function to optimize for. l2 \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. A optim.schedulers.Constant is used if a float is provided. The intercept is not updated when this is set to 0. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Attributes \u00b6 weights ( dict ) The current weights. Examples \u00b6 >>> from creme import datasets >>> from creme import evaluate >>> from creme import linear_model >>> from creme import metrics >>> from creme import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LinearRegression ( intercept_lr =. 1 ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 0.555971 >>> model [ 'LinearRegression' ] . intercept 35.617670 You can call the debug_one method to break down a prediction. This works even if the linear regression is part of a pipeline. >>> x , y = next ( iter ( dataset )) >>> report = model . debug_one ( x ) >>> print ( report ) 0. Input -------- gallup : 43.84321 ( float ) ipsos : 46.19925 ( float ) morning_consult : 48.31875 ( float ) ordinal_date : 736389 ( int ) rasmussen : 44.10469 ( float ) you_gov : 43.63691 ( float ) < BLANKLINE > 1. StandardScaler ----------------- gallup : 1.18810 ( float ) ipsos : 2.10348 ( float ) morning_consult : 2.73545 ( float ) ordinal_date : - 1.73032 ( float ) rasmussen : 1.26872 ( float ) you_gov : 1.48391 ( float ) < BLANKLINE > 2. LinearRegression ------------------- Name Value Weight Contribution Intercept 1.00000 35.61767 35.61767 ipsos 2.10348 0.62689 1.31866 morning_consult 2.73545 0.24180 0.66144 gallup 1.18810 0.43568 0.51764 rasmussen 1.26872 0.28118 0.35674 you_gov 1.48391 0.03123 0.04634 ordinal_date - 1.73032 3.45162 - 5.97242 < BLANKLINE > Prediction : 32.54607 Methods \u00b6 debug_one Debugs the output of the linear regression. x ( dict ) decimals \u2013 defaults to 5 learn_many learn_one Fits to a set of features x and a real-valued target y . x y w \u2013 defaults to 1.0 predict_many predict_one Predicts the target value of a set of features x . x","title":"LinearRegression"},{"location":"api-reference/linear-model/LinearRegression/#linearregression","text":"Linear regression. This estimator supports learning with mini-batches. On top of the single instance methods, it provides the following methods: learn_many , predict_many , predict_proba_many . Each method takes as input a pandas.DataFrame where each column represents a feature. It is generally a good idea to scale the data beforehand in order for the optimizer to converge. You can do this online with a preprocessing.StandardScaler .","title":"LinearRegression"},{"location":"api-reference/linear-model/LinearRegression/#parameters","text":"optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the weights. Note that the intercept updates are handled separately. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function to optimize for. l2 \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. A optim.schedulers.Constant is used if a float is provided. The intercept is not updated when this is set to 0. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme.","title":"Parameters"},{"location":"api-reference/linear-model/LinearRegression/#attributes","text":"weights ( dict ) The current weights.","title":"Attributes"},{"location":"api-reference/linear-model/LinearRegression/#examples","text":">>> from creme import datasets >>> from creme import evaluate >>> from creme import linear_model >>> from creme import metrics >>> from creme import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LinearRegression ( intercept_lr =. 1 ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 0.555971 >>> model [ 'LinearRegression' ] . intercept 35.617670 You can call the debug_one method to break down a prediction. This works even if the linear regression is part of a pipeline. >>> x , y = next ( iter ( dataset )) >>> report = model . debug_one ( x ) >>> print ( report ) 0. Input -------- gallup : 43.84321 ( float ) ipsos : 46.19925 ( float ) morning_consult : 48.31875 ( float ) ordinal_date : 736389 ( int ) rasmussen : 44.10469 ( float ) you_gov : 43.63691 ( float ) < BLANKLINE > 1. StandardScaler ----------------- gallup : 1.18810 ( float ) ipsos : 2.10348 ( float ) morning_consult : 2.73545 ( float ) ordinal_date : - 1.73032 ( float ) rasmussen : 1.26872 ( float ) you_gov : 1.48391 ( float ) < BLANKLINE > 2. LinearRegression ------------------- Name Value Weight Contribution Intercept 1.00000 35.61767 35.61767 ipsos 2.10348 0.62689 1.31866 morning_consult 2.73545 0.24180 0.66144 gallup 1.18810 0.43568 0.51764 rasmussen 1.26872 0.28118 0.35674 you_gov 1.48391 0.03123 0.04634 ordinal_date - 1.73032 3.45162 - 5.97242 < BLANKLINE > Prediction : 32.54607","title":"Examples"},{"location":"api-reference/linear-model/LinearRegression/#methods","text":"debug_one Debugs the output of the linear regression. x ( dict ) decimals \u2013 defaults to 5 learn_many learn_one Fits to a set of features x and a real-valued target y . x y w \u2013 defaults to 1.0 predict_many predict_one Predicts the target value of a set of features x . x","title":"Methods"},{"location":"api-reference/optim/Optimizer/","text":"Optimizer \u00b6 Optimizer interface. Every optimizer inherits from this base interface. Parameters \u00b6 lr ( Union[ optim.schedulers.Scheduler , float] ) Attributes \u00b6 learning_rate ( float ) Returns the current learning rate value. Methods \u00b6 update_after_pred Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. w ( dict ) g ( dict ) update_before_pred Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. w ( dict )","title":"Optimizer"},{"location":"api-reference/optim/Optimizer/#optimizer","text":"Optimizer interface. Every optimizer inherits from this base interface.","title":"Optimizer"},{"location":"api-reference/optim/Optimizer/#parameters","text":"lr ( Union[ optim.schedulers.Scheduler , float] )","title":"Parameters"},{"location":"api-reference/optim/Optimizer/#attributes","text":"learning_rate ( float ) Returns the current learning rate value.","title":"Attributes"},{"location":"api-reference/optim/Optimizer/#methods","text":"update_after_pred Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. w ( dict ) g ( dict ) update_before_pred Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. w ( dict )","title":"Methods"},{"location":"examples/batch-to-online/","text":"From batch to online \u00b6 A quick overview of batch learning \u00b6 If you've already delved into machine learning, then you shouldn't have any difficulty in getting to use incremental learning. If you are somewhat new to machine learning, then do not worry! The point of this notebook in particular is to introduce simple notions. We'll also start to show how creme fits in and explain how to use it. The whole point of machine learning is to learn from data . In supervised learning you want to learn how to predict a target y y given a set of features X X . Meanwhile in an unsupervised learning there is no target, and the goal is rather to identify patterns and trends in the features X X . At this point most people tend to imagine X X as a somewhat big table where each row is an observation and each column is a feature, and they would be quite right. Learning from tabular data is part of what's called batch learning , which basically that all of the data is available to our learning algorithm at once. A lot of libraries have been created to handle the batch learning regime, with one of the most prominent being Python's scikit-learn . As a simple example of batch learning let's say we want to learn to predict if a women has breast cancer or not. We'll use the breast cancer dataset available with scikit-learn . We'll learn to map a set of features to a binary decision using a logistic regression . Like many other models based on numerical weights, logisitc regression is sensitive to the scale of the features. Rescaling the data so that each feature has mean 0 and variance 1 is generally considered good practice. We can apply the rescaling and fit the logistic regression sequentially in an elegant manner using a Pipeline . To measure the performance of the model we'll evaluate the average ROC AUC score using a 5 fold cross-validation . from sklearn import datasets from sklearn import linear_model from sklearn import metrics from sklearn import model_selection from sklearn import pipeline from sklearn import preprocessing # Load the data dataset = datasets . load_breast_cancer () X , y = dataset . data , dataset . target # Define the steps of the model model = pipeline . Pipeline ([ ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LogisticRegression ( solver = 'lbfgs' )) ]) # Define a determistic cross-validation procedure cv = model_selection . KFold ( n_splits = 5 , shuffle = True , random_state = 42 ) # Compute the MSE values scorer = metrics . make_scorer ( metrics . roc_auc_score ) scores = model_selection . cross_val_score ( model , X , y , scoring = scorer , cv = cv ) # Display the average score and it's standard deviation print ( f 'ROC AUC: { scores . mean () : .3f } (\u00b1 { scores . std () : .3f } )' ) ROC AUC: 0.975 (\u00b1 0.011) This might be a lot to take in if you're not accustomed to scikit-learn, but it probably isn't if you are. Batch learning basically boils down to: Loading the data Fitting a model to the data Computing the performance of the model on unseen data This is pretty standard and is maybe how most people imagine a machine learning pipeline. However this way of proceding has certain downsides. First of all your laptop would crash if the load_boston function returned a dataset who's size exceeds your available amount of RAM. Sometimes you can use some tricks to get around this. For example by optimizing the data types and by using sparse representations when applicable you can potentially save precious gigabytes of RAM. However like many tricks this only goes so far. If your dataset weighs hundreds of gigabytes then you won't go far without some special hardware. One solution is to do out-of-core learning; that is, algorithms that can learning by being presented the data in chunks. If you want to go down this road then take a look at Dask and Spark's MLlib . Another issue with the batch learning regime is that can't elegantly learn from new data. Indeed if new data is made available, then the model has to learn from scratch with a new dataset composed of the old data and the new data. This is particularly annoying in a real situation where you might have new incoming data every week, day, hour, minute, or even setting. For example if you're building a recommendation engine for an e-commerce app, then you're probably training your model from 0 every week or so. As your app grows in popularity, so does the dataset you're training on. This will lead to longer and longer training times and might require a hardware upgrade. A final downside that isn't very easy to grasp concerns the manner in which features are extracted. Everytime you want to train your model you first have to extract features. The trick is that some features might not be accessible at the particular point in time you are at. For example maybe that some attributes in your data warehouse get overwritten with time. In other words maybe that all the features pertaining to a particular observations are not available, whereas they were a week ago. This happens more often than not in real scenarios, and apart if you have a sophisticated data engineering pipeline then you will encounter these issues at some point. A hands-on introduction to incremental learning \u00b6 Incremental learning is also often called online learning , but if you google online learning a lot of the results will point to educational websites. Hence we prefer the name \"incremental learning\", from which creme derives it's name. The point of incremental learning is to fit a model to a stream of data. In other words, the data isn't available in it's entirety, but rather the observations are provided one by one. As an example let's stream through the dataset used previously. for xi , yi in zip ( X , y ): # This where the model learns pass In this case we're iterating over a dataset that is already in memory, but we could just as well stream from a CSV file, a Kafka stream, an SQL query, etc. If we look at x we can notice that it is a numpy.ndarray . xi array([7.760e+00, 2.454e+01, 4.792e+01, 1.810e+02, 5.263e-02, 4.362e-02, 0.000e+00, 0.000e+00, 1.587e-01, 5.884e-02, 3.857e-01, 1.428e+00, 2.548e+00, 1.915e+01, 7.189e-03, 4.660e-03, 0.000e+00, 0.000e+00, 2.676e-02, 2.783e-03, 9.456e+00, 3.037e+01, 5.916e+01, 2.686e+02, 8.996e-02, 6.444e-02, 0.000e+00, 0.000e+00, 2.871e-01, 7.039e-02]) creme on the other hand works with dict s. We believe that dict s are more enjoyable to program with than numpy.ndarray s, at least for when single observations are concerned. dict 's bring the added benefit that each feature can be accessed by name rather than by position. for xi , yi in zip ( X , y ): xi = dict ( zip ( dataset . feature_names , xi )) pass xi {'mean radius': 7.76, 'mean texture': 24.54, 'mean perimeter': 47.92, 'mean area': 181.0, 'mean smoothness': 0.05263, 'mean compactness': 0.04362, 'mean concavity': 0.0, 'mean concave points': 0.0, 'mean symmetry': 0.1587, 'mean fractal dimension': 0.05884, 'radius error': 0.3857, 'texture error': 1.428, 'perimeter error': 2.548, 'area error': 19.15, 'smoothness error': 0.007189, 'compactness error': 0.00466, 'concavity error': 0.0, 'concave points error': 0.0, 'symmetry error': 0.02676, 'fractal dimension error': 0.002783, 'worst radius': 9.456, 'worst texture': 30.37, 'worst perimeter': 59.16, 'worst area': 268.6, 'worst smoothness': 0.08996, 'worst compactness': 0.06444, 'worst concavity': 0.0, 'worst concave points': 0.0, 'worst symmetry': 0.2871, 'worst fractal dimension': 0.07039} creme 's stream module has an iter_sklearn_dataset convenience function that we can use instead. from creme import stream for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer ()): pass The simple fact that we are getting the data in a stream means that we can't do a lot of things the same way as in a batch setting. For example let's say we want to scale the data so that it has mean 0 and variance 1, as we did earlier. To do so we simply have to subtract the mean of each feature to each value and then divide the result by the standard deviation of the feature. The problem is that we can't possible known the values of the mean and the standard deviation before actually going through all the data! One way to procede would be to do a first pass over the data to compute the necessary values and then scale the values during a second pass. The problem is that defeats our purpose, which is to learn by only looking at the data once. Although this might seem rather restrictive, it reaps sizable benefits down the road. The way we do feature scaling in creme involves computing running statistics . The idea is that we use a data structure that estimates the mean and updates itself when it is provided with a value. The same goes for the variance (and thus the standard deviation). For example, if we denote \\mu_t \\mu_t the mean and n_t n_t the count at any moment t t , then updating the mean can be done as so: \\begin{cases} n_{t+1} = n_t + 1 \\\\ \\mu_{t+1} = \\mu_t + \\frac{x - \\mu_t}{n_{t+1}} \\end{cases} \\begin{cases} n_{t+1} = n_t + 1 \\\\ \\mu_{t+1} = \\mu_t + \\frac{x - \\mu_t}{n_{t+1}} \\end{cases} Likewhise a running variance can be computed as so: \\begin{cases} n_{t+1} = n_t + 1 \\\\ \\mu_{t+1} = \\mu_t + \\frac{x - \\mu_t}{n_{t+1}} \\\\ s_{t+1} = s_t + (x - \\mu_t) \\times (x - \\mu_{t+1}) \\\\ \\sigma_{t+1} = \\frac{s_{t+1}}{n_{t+1}} \\end{cases} \\begin{cases} n_{t+1} = n_t + 1 \\\\ \\mu_{t+1} = \\mu_t + \\frac{x - \\mu_t}{n_{t+1}} \\\\ s_{t+1} = s_t + (x - \\mu_t) \\times (x - \\mu_{t+1}) \\\\ \\sigma_{t+1} = \\frac{s_{t+1}}{n_{t+1}} \\end{cases} where s_t s_t is a running sum of squares and \\sigma_t \\sigma_t is the running variance at time t t . This might seem a tad more involved than the batch algorithms you learn in school, but it is rather elegant. Implementing this in Python is not too difficult. For example let's compute the running mean and variance of the 'mean area' variable. n , mean , sum_of_squares , variance = 0 , 0 , 0 , 0 for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer ()): n += 1 old_mean = mean mean += ( xi [ 'mean area' ] - mean ) / n sum_of_squares += ( xi [ 'mean area' ] - old_mean ) * ( xi [ 'mean area' ] - mean ) variance = sum_of_squares / n print ( f 'Running mean: { mean : .3f } ' ) print ( f 'Running variance: { variance : .3f } ' ) Running mean: 654.889 Running variance: 123625.903 Let's compare this with numpy . import numpy as np i = list ( dataset . feature_names ) . index ( 'mean area' ) print ( f 'True mean: { np . mean ( X [:, i ]) : .3f } ' ) print ( f 'True variance: { np . var ( X [:, i ]) : .3f } ' ) True mean: 654.889 True variance: 123625.903 The results seem to be exactly the same! The twist is that the running statistics won't be very accurate for the first few observations. In general though this doesn't matter too much. Some would even go as far as to say that this descrepancy is beneficial and acts as some sort of regularization... Now the idea is that we can compute the running statistics of each feature and scale them as they come along. The way to do this with creme is to use the StandardScaler class from the preprocessing module, as so: from creme import preprocessing scaler = preprocessing . StandardScaler () for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer ()): xi = scaler . learn_one ( xi ) This is quite terse but let's break it down nonetheless. Every class in creme has a learn_one(x, y) method where all the magic happens. Now the important thing to notice is that the learn_one actually returns the output for the given input. This is one of the nice properties of online learning: inference can be done immediatly. In creme each call to a Transformer 's learn_one will return the transformed output. Meanwhile calling learn_one with a Classifier or a Regressor will return the predicted target for the given set of features. The twist is that the prediction is made before looking at the true target y . This means that we get a free hold-out prediction every time we call learn_one . This can be used to monitor the performance of the model as it trains, which is obviously nice to have. Now that we are scaling the data, we can start doing some actual machine learning. We're going to implement an online linear regression. Because all the data isn't available at once, we are obliged to do what is called stochastic gradient descent , which is a popular research topic and has a lot of variants. SGD is commonly used to train neural networks. The idea is that at each step we compute the loss between the target prediction and the truth. We then calculate the gradient, which is simply a set of derivatives with respect to each weight from the linear regression. Once we have obtained the gradient, we can update the weights by moving them in the opposite direction of the gradient. The amount by which the weights are moved typically depends on a learning rate , which is typically set by the user. Different optimizers have different ways of managing the weight update, and some handle the learning rate implicitely. Online linear regression can be done in creme with the LinearRegression class from the linear_model module. We'll be using plain and simple SGD using the SGD optimizer from the optim module. During training we'll measure the squared error between the truth and the predictions. from creme import linear_model from creme import optim scaler = preprocessing . StandardScaler () optimizer = optim . SGD ( lr = 0.01 ) log_reg = linear_model . LogisticRegression ( optimizer ) y_true = [] y_pred = [] for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer (), shuffle = True , seed = 42 ): # Scale the features xi_scaled = scaler . learn_one ( xi ) . transform_one ( xi ) # Fit the linear regression yi_pred = log_reg . predict_proba_one ( xi_scaled ) log_reg . learn_one ( xi_scaled , yi ) # Store the truth and the prediction y_true . append ( yi ) y_pred . append ( yi_pred [ True ]) print ( f 'ROC AUC: { metrics . roc_auc_score ( y_true , y_pred ) : .3f } ' ) ROC AUC: 0.990 The ROC AUC is significantly better than the one obtained from the cross-validation of scikit-learn's logisitic regression. However to make things really comparable it would be nice to compare with the same cross-validation procedure. creme has a compat module that contains utilities for making creme compatible with other Python libraries. Because we're doing regression we'll be using the SKLRegressorWrapper . We'll also be using Pipeline to encapsulate the logic of the StandardScaler and the LogisticRegression in one single object. from creme import compat from creme import compose # We define a Pipeline, exactly like we did earlier for sklearn model = compose . Pipeline ( ( 'scale' , preprocessing . StandardScaler ()), ( 'log_reg' , linear_model . LogisticRegression ()) ) # We make the Pipeline compatible with sklearn model = compat . convert_creme_to_sklearn ( model ) # We compute the CV scores using the same CV scheme and the same scoring scores = model_selection . cross_val_score ( model , X , y , scoring = scorer , cv = cv ) # Display the average score and it's standard deviation print ( f 'ROC AUC: { scores . mean () : .3f } (\u00b1 { scores . std () : .3f } )' ) ROC AUC: 0.964 (\u00b1 0.016) This time the ROC AUC score is lower, which is what we would expect. Indeed online learning isn't as accurate as batch learning. However it all depends in what you're interested in. If you're only interested in predicting the next observation then the online learning regime would be better. That's why it's a bit hard to compare both approaches: they're both suited to different scenarios. Going further \u00b6 There's a lot more to learn, and it all depends on what kind on your use case. Feel free to have a look at the documentation to know what creme has available, and have a look the example notebook . Here a few resources if you want to do some reading: Online learning -- Wikipedia What is online machine learning? -- Max Pagels Introduction to Online Learning -- USC course Online Methods in Machine Learning -- MIT course Online Learning: A Comprehensive Survey Streaming 101: The world beyond batch Machine learning for data streams Data Stream Mining: A Practical Approach","title":"From batch to online"},{"location":"examples/batch-to-online/#from-batch-to-online","text":"","title":"From batch to online"},{"location":"examples/batch-to-online/#a-quick-overview-of-batch-learning","text":"If you've already delved into machine learning, then you shouldn't have any difficulty in getting to use incremental learning. If you are somewhat new to machine learning, then do not worry! The point of this notebook in particular is to introduce simple notions. We'll also start to show how creme fits in and explain how to use it. The whole point of machine learning is to learn from data . In supervised learning you want to learn how to predict a target y y given a set of features X X . Meanwhile in an unsupervised learning there is no target, and the goal is rather to identify patterns and trends in the features X X . At this point most people tend to imagine X X as a somewhat big table where each row is an observation and each column is a feature, and they would be quite right. Learning from tabular data is part of what's called batch learning , which basically that all of the data is available to our learning algorithm at once. A lot of libraries have been created to handle the batch learning regime, with one of the most prominent being Python's scikit-learn . As a simple example of batch learning let's say we want to learn to predict if a women has breast cancer or not. We'll use the breast cancer dataset available with scikit-learn . We'll learn to map a set of features to a binary decision using a logistic regression . Like many other models based on numerical weights, logisitc regression is sensitive to the scale of the features. Rescaling the data so that each feature has mean 0 and variance 1 is generally considered good practice. We can apply the rescaling and fit the logistic regression sequentially in an elegant manner using a Pipeline . To measure the performance of the model we'll evaluate the average ROC AUC score using a 5 fold cross-validation . from sklearn import datasets from sklearn import linear_model from sklearn import metrics from sklearn import model_selection from sklearn import pipeline from sklearn import preprocessing # Load the data dataset = datasets . load_breast_cancer () X , y = dataset . data , dataset . target # Define the steps of the model model = pipeline . Pipeline ([ ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LogisticRegression ( solver = 'lbfgs' )) ]) # Define a determistic cross-validation procedure cv = model_selection . KFold ( n_splits = 5 , shuffle = True , random_state = 42 ) # Compute the MSE values scorer = metrics . make_scorer ( metrics . roc_auc_score ) scores = model_selection . cross_val_score ( model , X , y , scoring = scorer , cv = cv ) # Display the average score and it's standard deviation print ( f 'ROC AUC: { scores . mean () : .3f } (\u00b1 { scores . std () : .3f } )' ) ROC AUC: 0.975 (\u00b1 0.011) This might be a lot to take in if you're not accustomed to scikit-learn, but it probably isn't if you are. Batch learning basically boils down to: Loading the data Fitting a model to the data Computing the performance of the model on unseen data This is pretty standard and is maybe how most people imagine a machine learning pipeline. However this way of proceding has certain downsides. First of all your laptop would crash if the load_boston function returned a dataset who's size exceeds your available amount of RAM. Sometimes you can use some tricks to get around this. For example by optimizing the data types and by using sparse representations when applicable you can potentially save precious gigabytes of RAM. However like many tricks this only goes so far. If your dataset weighs hundreds of gigabytes then you won't go far without some special hardware. One solution is to do out-of-core learning; that is, algorithms that can learning by being presented the data in chunks. If you want to go down this road then take a look at Dask and Spark's MLlib . Another issue with the batch learning regime is that can't elegantly learn from new data. Indeed if new data is made available, then the model has to learn from scratch with a new dataset composed of the old data and the new data. This is particularly annoying in a real situation where you might have new incoming data every week, day, hour, minute, or even setting. For example if you're building a recommendation engine for an e-commerce app, then you're probably training your model from 0 every week or so. As your app grows in popularity, so does the dataset you're training on. This will lead to longer and longer training times and might require a hardware upgrade. A final downside that isn't very easy to grasp concerns the manner in which features are extracted. Everytime you want to train your model you first have to extract features. The trick is that some features might not be accessible at the particular point in time you are at. For example maybe that some attributes in your data warehouse get overwritten with time. In other words maybe that all the features pertaining to a particular observations are not available, whereas they were a week ago. This happens more often than not in real scenarios, and apart if you have a sophisticated data engineering pipeline then you will encounter these issues at some point.","title":"A quick overview of batch learning"},{"location":"examples/batch-to-online/#a-hands-on-introduction-to-incremental-learning","text":"Incremental learning is also often called online learning , but if you google online learning a lot of the results will point to educational websites. Hence we prefer the name \"incremental learning\", from which creme derives it's name. The point of incremental learning is to fit a model to a stream of data. In other words, the data isn't available in it's entirety, but rather the observations are provided one by one. As an example let's stream through the dataset used previously. for xi , yi in zip ( X , y ): # This where the model learns pass In this case we're iterating over a dataset that is already in memory, but we could just as well stream from a CSV file, a Kafka stream, an SQL query, etc. If we look at x we can notice that it is a numpy.ndarray . xi array([7.760e+00, 2.454e+01, 4.792e+01, 1.810e+02, 5.263e-02, 4.362e-02, 0.000e+00, 0.000e+00, 1.587e-01, 5.884e-02, 3.857e-01, 1.428e+00, 2.548e+00, 1.915e+01, 7.189e-03, 4.660e-03, 0.000e+00, 0.000e+00, 2.676e-02, 2.783e-03, 9.456e+00, 3.037e+01, 5.916e+01, 2.686e+02, 8.996e-02, 6.444e-02, 0.000e+00, 0.000e+00, 2.871e-01, 7.039e-02]) creme on the other hand works with dict s. We believe that dict s are more enjoyable to program with than numpy.ndarray s, at least for when single observations are concerned. dict 's bring the added benefit that each feature can be accessed by name rather than by position. for xi , yi in zip ( X , y ): xi = dict ( zip ( dataset . feature_names , xi )) pass xi {'mean radius': 7.76, 'mean texture': 24.54, 'mean perimeter': 47.92, 'mean area': 181.0, 'mean smoothness': 0.05263, 'mean compactness': 0.04362, 'mean concavity': 0.0, 'mean concave points': 0.0, 'mean symmetry': 0.1587, 'mean fractal dimension': 0.05884, 'radius error': 0.3857, 'texture error': 1.428, 'perimeter error': 2.548, 'area error': 19.15, 'smoothness error': 0.007189, 'compactness error': 0.00466, 'concavity error': 0.0, 'concave points error': 0.0, 'symmetry error': 0.02676, 'fractal dimension error': 0.002783, 'worst radius': 9.456, 'worst texture': 30.37, 'worst perimeter': 59.16, 'worst area': 268.6, 'worst smoothness': 0.08996, 'worst compactness': 0.06444, 'worst concavity': 0.0, 'worst concave points': 0.0, 'worst symmetry': 0.2871, 'worst fractal dimension': 0.07039} creme 's stream module has an iter_sklearn_dataset convenience function that we can use instead. from creme import stream for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer ()): pass The simple fact that we are getting the data in a stream means that we can't do a lot of things the same way as in a batch setting. For example let's say we want to scale the data so that it has mean 0 and variance 1, as we did earlier. To do so we simply have to subtract the mean of each feature to each value and then divide the result by the standard deviation of the feature. The problem is that we can't possible known the values of the mean and the standard deviation before actually going through all the data! One way to procede would be to do a first pass over the data to compute the necessary values and then scale the values during a second pass. The problem is that defeats our purpose, which is to learn by only looking at the data once. Although this might seem rather restrictive, it reaps sizable benefits down the road. The way we do feature scaling in creme involves computing running statistics . The idea is that we use a data structure that estimates the mean and updates itself when it is provided with a value. The same goes for the variance (and thus the standard deviation). For example, if we denote \\mu_t \\mu_t the mean and n_t n_t the count at any moment t t , then updating the mean can be done as so: \\begin{cases} n_{t+1} = n_t + 1 \\\\ \\mu_{t+1} = \\mu_t + \\frac{x - \\mu_t}{n_{t+1}} \\end{cases} \\begin{cases} n_{t+1} = n_t + 1 \\\\ \\mu_{t+1} = \\mu_t + \\frac{x - \\mu_t}{n_{t+1}} \\end{cases} Likewhise a running variance can be computed as so: \\begin{cases} n_{t+1} = n_t + 1 \\\\ \\mu_{t+1} = \\mu_t + \\frac{x - \\mu_t}{n_{t+1}} \\\\ s_{t+1} = s_t + (x - \\mu_t) \\times (x - \\mu_{t+1}) \\\\ \\sigma_{t+1} = \\frac{s_{t+1}}{n_{t+1}} \\end{cases} \\begin{cases} n_{t+1} = n_t + 1 \\\\ \\mu_{t+1} = \\mu_t + \\frac{x - \\mu_t}{n_{t+1}} \\\\ s_{t+1} = s_t + (x - \\mu_t) \\times (x - \\mu_{t+1}) \\\\ \\sigma_{t+1} = \\frac{s_{t+1}}{n_{t+1}} \\end{cases} where s_t s_t is a running sum of squares and \\sigma_t \\sigma_t is the running variance at time t t . This might seem a tad more involved than the batch algorithms you learn in school, but it is rather elegant. Implementing this in Python is not too difficult. For example let's compute the running mean and variance of the 'mean area' variable. n , mean , sum_of_squares , variance = 0 , 0 , 0 , 0 for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer ()): n += 1 old_mean = mean mean += ( xi [ 'mean area' ] - mean ) / n sum_of_squares += ( xi [ 'mean area' ] - old_mean ) * ( xi [ 'mean area' ] - mean ) variance = sum_of_squares / n print ( f 'Running mean: { mean : .3f } ' ) print ( f 'Running variance: { variance : .3f } ' ) Running mean: 654.889 Running variance: 123625.903 Let's compare this with numpy . import numpy as np i = list ( dataset . feature_names ) . index ( 'mean area' ) print ( f 'True mean: { np . mean ( X [:, i ]) : .3f } ' ) print ( f 'True variance: { np . var ( X [:, i ]) : .3f } ' ) True mean: 654.889 True variance: 123625.903 The results seem to be exactly the same! The twist is that the running statistics won't be very accurate for the first few observations. In general though this doesn't matter too much. Some would even go as far as to say that this descrepancy is beneficial and acts as some sort of regularization... Now the idea is that we can compute the running statistics of each feature and scale them as they come along. The way to do this with creme is to use the StandardScaler class from the preprocessing module, as so: from creme import preprocessing scaler = preprocessing . StandardScaler () for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer ()): xi = scaler . learn_one ( xi ) This is quite terse but let's break it down nonetheless. Every class in creme has a learn_one(x, y) method where all the magic happens. Now the important thing to notice is that the learn_one actually returns the output for the given input. This is one of the nice properties of online learning: inference can be done immediatly. In creme each call to a Transformer 's learn_one will return the transformed output. Meanwhile calling learn_one with a Classifier or a Regressor will return the predicted target for the given set of features. The twist is that the prediction is made before looking at the true target y . This means that we get a free hold-out prediction every time we call learn_one . This can be used to monitor the performance of the model as it trains, which is obviously nice to have. Now that we are scaling the data, we can start doing some actual machine learning. We're going to implement an online linear regression. Because all the data isn't available at once, we are obliged to do what is called stochastic gradient descent , which is a popular research topic and has a lot of variants. SGD is commonly used to train neural networks. The idea is that at each step we compute the loss between the target prediction and the truth. We then calculate the gradient, which is simply a set of derivatives with respect to each weight from the linear regression. Once we have obtained the gradient, we can update the weights by moving them in the opposite direction of the gradient. The amount by which the weights are moved typically depends on a learning rate , which is typically set by the user. Different optimizers have different ways of managing the weight update, and some handle the learning rate implicitely. Online linear regression can be done in creme with the LinearRegression class from the linear_model module. We'll be using plain and simple SGD using the SGD optimizer from the optim module. During training we'll measure the squared error between the truth and the predictions. from creme import linear_model from creme import optim scaler = preprocessing . StandardScaler () optimizer = optim . SGD ( lr = 0.01 ) log_reg = linear_model . LogisticRegression ( optimizer ) y_true = [] y_pred = [] for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer (), shuffle = True , seed = 42 ): # Scale the features xi_scaled = scaler . learn_one ( xi ) . transform_one ( xi ) # Fit the linear regression yi_pred = log_reg . predict_proba_one ( xi_scaled ) log_reg . learn_one ( xi_scaled , yi ) # Store the truth and the prediction y_true . append ( yi ) y_pred . append ( yi_pred [ True ]) print ( f 'ROC AUC: { metrics . roc_auc_score ( y_true , y_pred ) : .3f } ' ) ROC AUC: 0.990 The ROC AUC is significantly better than the one obtained from the cross-validation of scikit-learn's logisitic regression. However to make things really comparable it would be nice to compare with the same cross-validation procedure. creme has a compat module that contains utilities for making creme compatible with other Python libraries. Because we're doing regression we'll be using the SKLRegressorWrapper . We'll also be using Pipeline to encapsulate the logic of the StandardScaler and the LogisticRegression in one single object. from creme import compat from creme import compose # We define a Pipeline, exactly like we did earlier for sklearn model = compose . Pipeline ( ( 'scale' , preprocessing . StandardScaler ()), ( 'log_reg' , linear_model . LogisticRegression ()) ) # We make the Pipeline compatible with sklearn model = compat . convert_creme_to_sklearn ( model ) # We compute the CV scores using the same CV scheme and the same scoring scores = model_selection . cross_val_score ( model , X , y , scoring = scorer , cv = cv ) # Display the average score and it's standard deviation print ( f 'ROC AUC: { scores . mean () : .3f } (\u00b1 { scores . std () : .3f } )' ) ROC AUC: 0.964 (\u00b1 0.016) This time the ROC AUC score is lower, which is what we would expect. Indeed online learning isn't as accurate as batch learning. However it all depends in what you're interested in. If you're only interested in predicting the next observation then the online learning regime would be better. That's why it's a bit hard to compare both approaches: they're both suited to different scenarios.","title":"A hands-on introduction to incremental learning"},{"location":"examples/batch-to-online/#going-further","text":"There's a lot more to learn, and it all depends on what kind on your use case. Feel free to have a look at the documentation to know what creme has available, and have a look the example notebook . Here a few resources if you want to do some reading: Online learning -- Wikipedia What is online machine learning? -- Max Pagels Introduction to Online Learning -- USC course Online Methods in Machine Learning -- MIT course Online Learning: A Comprehensive Survey Streaming 101: The world beyond batch Machine learning for data streams Data Stream Mining: A Practical Approach","title":"Going further"},{"location":"examples/bike-sharing-forecasting/","text":"Bike-sharing forecasting \u00b6 In this tutorial we're going to forecast the number of bikes in 5 bike stations from the city of Toulouse. We'll do so by building a simple model step by step. The dataset contains 182,470 observations. Let's first take a peak at the data. from pprint import pprint from creme import datasets X_y = datasets . Bikes () for x , y in X_y : pprint ( x ) print ( f 'Number of available bikes: { y } ' ) break {'clouds': 75, 'description': 'light rain', 'humidity': 81, 'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'pressure': 1017.0, 'station': 'metro-canal-du-midi', 'temperature': 6.54, 'wind': 9.3} Number of available bikes: 1 Let's start by using a simple linear regression on the numeric features. We can select the numeric features and discard the rest of the features using a Select . Linear regression is very likely to go haywire if we don't scale the data, so we'll use a StandardScaler to do just that. We'll evaluate the model by measuring the mean absolute error. Finally we'll print the score every 20,000 observations. from creme import compose from creme import linear_model from creme import metrics from creme import model_selection from creme import preprocessing from creme import optim X_y = datasets . Bikes () model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression ( optimizer = optim . SGD ( 0.001 )) metric = metrics . MAE () model_selection . progressive_val_score ( X_y , model , metric , print_every = 20_000 ) [20,000] MAE: 4.912727 [40,000] MAE: 5.333554 [60,000] MAE: 5.330948 [80,000] MAE: 5.392313 [100,000] MAE: 5.423059 [120,000] MAE: 5.541223 [140,000] MAE: 5.613023 [160,000] MAE: 5.622428 [180,000] MAE: 5.567824 MAE: 5.563893 The model doesn't seem to be doing that well, but then again we didn't provide a lot of features. Generally, a good idea for this kind of problem is to look at an average of the previous values. For example, for each station we can look at the average number of bikes per hour. To do so we first have to extract the hour from the moment field. We can then use a TargetAgg to aggregate the values of the target. from creme import feature_extraction from creme import stats X_y = iter ( datasets . Bikes ()) def get_hour ( x ): x [ 'hour' ] = x [ 'moment' ] . hour return x model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression ( optimizer = optim . SGD ( 0.001 )) metric = metrics . MAE () model_selection . progressive_val_score ( X_y , model , metric , print_every = 20_000 ) [20,000] MAE: 3.721246 [40,000] MAE: 3.829972 [60,000] MAE: 3.845068 [80,000] MAE: 3.910259 [100,000] MAE: 3.888652 [120,000] MAE: 3.923727 [140,000] MAE: 3.980953 [160,000] MAE: 3.950034 [180,000] MAE: 3.934545 MAE: 3.933498 By adding a single feature, we've managed to significantly reduce the mean absolute error. At this point you might think that the model is getting slightly complex, and is difficult to understand and test. Pipelines have the advantage of being terse, but they aren't always to debug. Thankfully creme has some ways to relieve the pain. The first thing we can do it to draw the pipeline, to get an idea of how the data flows through it. model . draw () We can also use the debug_one method to see what happens to one particular instance. Let's train the model on the first 10,000 observations and then call debug_one on the next one. To do this, we will turn the Bike object into a Python generator with iter() function. The Pythonic way to read the first 10,000 elements of a generator is to use itertools.islice . import itertools X_y = iter ( datasets . Bikes ()) model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () for x , y in itertools . islice ( X_y , 10000 ): y_pred = model . predict_one ( x ) model . learn_one ( x , y ) x , y = next ( X_y ) model . debug_one ( x ) '0. Input\\n--------\\nclouds: 0 (int)\\ndescription: clear sky (str)\\nhumidity: 52 (int)\\nmoment: 2016-04-10 19:03:27 (datetime)\\npressure: 1,001.00000 (float)\\nstation: place-esquirol (str)\\ntemperature: 19.00000 (float)\\nwind: 7.70000 (float)\\n\\n1. Transformer union\\n--------------------\\n 1.0 Select\\n ----------\\n clouds: 0 (int)\\n humidity: 52 (int)\\n pressure: 1,001.00000 (float)\\n temperature: 19.00000 (float)\\n wind: 7.70000 (float)\\n\\n 1.1 get_hour | target_mean_by_station_and_hour\\n ----------------------------------------------\\n target_mean_by_station_and_hour: 7.97175 (float)\\n\\nclouds: 0 (int)\\nhumidity: 52 (int)\\npressure: 1,001.00000 (float)\\ntarget_mean_by_station_and_hour: 7.97175 (float)\\ntemperature: 19.00000 (float)\\nwind: 7.70000 (float)\\n\\n2. StandardScaler\\n-----------------\\nclouds: -1.36138 (float)\\nhumidity: -1.73083 (float)\\npressure: -1.26076 (float)\\ntarget_mean_by_station_and_hour: 0.05496 (float)\\ntemperature: 1.76232 (float)\\nwind: 1.45841 (float)\\n\\n3. LinearRegression\\n-------------------\\nName Value Weight Contribution \\n Intercept 1.00000 6.58252 6.58252 \\n temperature 1.76232 2.47030 4.35345 \\n clouds -1.36138 -1.92255 2.61732 \\ntarget_mean_by_station_and_hour 0.05496 0.54167 0.02977 \\n wind 1.45841 -0.77720 -1.13348 \\n humidity -1.73083 1.44921 -2.50833 \\n pressure -1.26076 3.78529 -4.77234 \\n\\nPrediction: 5.16889' The debug_one method shows what happens to an input set of features, step by step. And now comes the catch. Up until now we've been using the online_score method from the model_selection module. What this does it that it sequentially predicts the output of an observation and updates the model immediately afterwards. This way of doing is often used for evaluating online learning models, but in some cases it is the wrong approach. The following paragraph is extremely important. When evaluating a machine learning model, the goal is to simulate production conditions in order to get a trust-worthy assessment of the performance of the model. In our case, we typically want to forecast the number of bikes available in a station, say, 30 minutes ahead. Then, once the 30 minutes have passed, the true number of available bikes will be available and we will be able to update the model using the features available 30 minutes ago. If you think about, this is exactly how a real-time machine learning system should work. The problem is that this isn't at all what the online_score method, indeed it is simply asking the model to predict the next observation, which is only a few minutes ahead, and then updates the model immediately. We can prove that this is flawed by adding a feature that measures a running average of the very recent values. X_y = datasets . Bikes () model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) + feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ( 0.5 )) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () metric = metrics . MAE () model_selection . progressive_val_score ( X_y , model , metric , print_every = 20_000 ) [20,000] MAE: 20.159286 [40,000] MAE: 10.458898 [60,000] MAE: 7.2759 [80,000] MAE: 5.715397 [100,000] MAE: 4.775094 [120,000] MAE: 4.138421 [140,000] MAE: 3.682591 [160,000] MAE: 3.35015 [180,000] MAE: 3.091398 MAE: 3.06414 The score we got is too good to be true. This is simply because the problem is too easy. What we really want is to evaluate the model by forecasting 30 minutes ahead and only updating the model once the true values are available. This can be done using the online_qa_score method, also from the model_selection module. The \"qa\" part stands for \"question/answer\". The idea is that each observation of the stream of the data is shown twice to the model: once for making a prediction, and once for updating the model when the true value is revealed. The on parameter determines which variable should be used as a timestamp, while the lag parameter controls the duration to wait before revealing the true values to the model. import datetime as dt model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) + feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ( 0.5 )) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () model_selection . progressive_val_score ( X_y = datasets . Bikes (), model = model , metric = metrics . MAE (), moment = 'moment' , delay = dt . timedelta ( minutes = 30 ), print_every = 20_000 ) [20,000] MAE: 2.24812 [40,000] MAE: 2.240287 [60,000] MAE: 2.270287 [80,000] MAE: 2.28649 [100,000] MAE: 2.294264 [120,000] MAE: 2.275891 [140,000] MAE: 2.261411 [160,000] MAE: 2.285978 [180,000] MAE: 2.289353 MAE: 2.29304 The score we now have is much more realistic, as it is comparable with related data science competitions . Moreover, we can see that the model gets better with time, which feels better than the previous situations. The point is that online_qa_score method can be used to simulate a production scenario, and is thus extremely valuable. Now that we have a working pipeline in place, we can attempt to make it more accurate. As a simple example, we'll using a HedgeRegressor from the ensemble module to combine 3 linear regression model trained with different optimizers. The HedgeRegressor will run the 3 models in parallel and assign weights to each model based on their individual performance. from creme import ensemble from creme import optim model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) ) model += feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ( 0.5 )) model |= preprocessing . StandardScaler () model |= ensemble . HedgeRegressor ([ linear_model . LinearRegression ( optim . SGD ()), linear_model . LinearRegression ( optim . RMSProp ()), linear_model . LinearRegression ( optim . Adam ()) ]) model_selection . progressive_val_score ( X_y = datasets . Bikes (), model = model , metric = metrics . MAE (), moment = 'moment' , delay = dt . timedelta ( minutes = 30 ), print_every = 20_000 ) [20,000] MAE: 2.253263 [40,000] MAE: 2.242859 [60,000] MAE: 2.272001 [80,000] MAE: 2.287776 [100,000] MAE: 2.295292 [120,000] MAE: 2.276748 [140,000] MAE: 2.262146 [160,000] MAE: 2.286621 [180,000] MAE: 2.289925 MAE: 2.293604","title":"Bike-sharing forecasting"},{"location":"examples/bike-sharing-forecasting/#bike-sharing-forecasting","text":"In this tutorial we're going to forecast the number of bikes in 5 bike stations from the city of Toulouse. We'll do so by building a simple model step by step. The dataset contains 182,470 observations. Let's first take a peak at the data. from pprint import pprint from creme import datasets X_y = datasets . Bikes () for x , y in X_y : pprint ( x ) print ( f 'Number of available bikes: { y } ' ) break {'clouds': 75, 'description': 'light rain', 'humidity': 81, 'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'pressure': 1017.0, 'station': 'metro-canal-du-midi', 'temperature': 6.54, 'wind': 9.3} Number of available bikes: 1 Let's start by using a simple linear regression on the numeric features. We can select the numeric features and discard the rest of the features using a Select . Linear regression is very likely to go haywire if we don't scale the data, so we'll use a StandardScaler to do just that. We'll evaluate the model by measuring the mean absolute error. Finally we'll print the score every 20,000 observations. from creme import compose from creme import linear_model from creme import metrics from creme import model_selection from creme import preprocessing from creme import optim X_y = datasets . Bikes () model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression ( optimizer = optim . SGD ( 0.001 )) metric = metrics . MAE () model_selection . progressive_val_score ( X_y , model , metric , print_every = 20_000 ) [20,000] MAE: 4.912727 [40,000] MAE: 5.333554 [60,000] MAE: 5.330948 [80,000] MAE: 5.392313 [100,000] MAE: 5.423059 [120,000] MAE: 5.541223 [140,000] MAE: 5.613023 [160,000] MAE: 5.622428 [180,000] MAE: 5.567824 MAE: 5.563893 The model doesn't seem to be doing that well, but then again we didn't provide a lot of features. Generally, a good idea for this kind of problem is to look at an average of the previous values. For example, for each station we can look at the average number of bikes per hour. To do so we first have to extract the hour from the moment field. We can then use a TargetAgg to aggregate the values of the target. from creme import feature_extraction from creme import stats X_y = iter ( datasets . Bikes ()) def get_hour ( x ): x [ 'hour' ] = x [ 'moment' ] . hour return x model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression ( optimizer = optim . SGD ( 0.001 )) metric = metrics . MAE () model_selection . progressive_val_score ( X_y , model , metric , print_every = 20_000 ) [20,000] MAE: 3.721246 [40,000] MAE: 3.829972 [60,000] MAE: 3.845068 [80,000] MAE: 3.910259 [100,000] MAE: 3.888652 [120,000] MAE: 3.923727 [140,000] MAE: 3.980953 [160,000] MAE: 3.950034 [180,000] MAE: 3.934545 MAE: 3.933498 By adding a single feature, we've managed to significantly reduce the mean absolute error. At this point you might think that the model is getting slightly complex, and is difficult to understand and test. Pipelines have the advantage of being terse, but they aren't always to debug. Thankfully creme has some ways to relieve the pain. The first thing we can do it to draw the pipeline, to get an idea of how the data flows through it. model . draw () We can also use the debug_one method to see what happens to one particular instance. Let's train the model on the first 10,000 observations and then call debug_one on the next one. To do this, we will turn the Bike object into a Python generator with iter() function. The Pythonic way to read the first 10,000 elements of a generator is to use itertools.islice . import itertools X_y = iter ( datasets . Bikes ()) model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () for x , y in itertools . islice ( X_y , 10000 ): y_pred = model . predict_one ( x ) model . learn_one ( x , y ) x , y = next ( X_y ) model . debug_one ( x ) '0. Input\\n--------\\nclouds: 0 (int)\\ndescription: clear sky (str)\\nhumidity: 52 (int)\\nmoment: 2016-04-10 19:03:27 (datetime)\\npressure: 1,001.00000 (float)\\nstation: place-esquirol (str)\\ntemperature: 19.00000 (float)\\nwind: 7.70000 (float)\\n\\n1. Transformer union\\n--------------------\\n 1.0 Select\\n ----------\\n clouds: 0 (int)\\n humidity: 52 (int)\\n pressure: 1,001.00000 (float)\\n temperature: 19.00000 (float)\\n wind: 7.70000 (float)\\n\\n 1.1 get_hour | target_mean_by_station_and_hour\\n ----------------------------------------------\\n target_mean_by_station_and_hour: 7.97175 (float)\\n\\nclouds: 0 (int)\\nhumidity: 52 (int)\\npressure: 1,001.00000 (float)\\ntarget_mean_by_station_and_hour: 7.97175 (float)\\ntemperature: 19.00000 (float)\\nwind: 7.70000 (float)\\n\\n2. StandardScaler\\n-----------------\\nclouds: -1.36138 (float)\\nhumidity: -1.73083 (float)\\npressure: -1.26076 (float)\\ntarget_mean_by_station_and_hour: 0.05496 (float)\\ntemperature: 1.76232 (float)\\nwind: 1.45841 (float)\\n\\n3. LinearRegression\\n-------------------\\nName Value Weight Contribution \\n Intercept 1.00000 6.58252 6.58252 \\n temperature 1.76232 2.47030 4.35345 \\n clouds -1.36138 -1.92255 2.61732 \\ntarget_mean_by_station_and_hour 0.05496 0.54167 0.02977 \\n wind 1.45841 -0.77720 -1.13348 \\n humidity -1.73083 1.44921 -2.50833 \\n pressure -1.26076 3.78529 -4.77234 \\n\\nPrediction: 5.16889' The debug_one method shows what happens to an input set of features, step by step. And now comes the catch. Up until now we've been using the online_score method from the model_selection module. What this does it that it sequentially predicts the output of an observation and updates the model immediately afterwards. This way of doing is often used for evaluating online learning models, but in some cases it is the wrong approach. The following paragraph is extremely important. When evaluating a machine learning model, the goal is to simulate production conditions in order to get a trust-worthy assessment of the performance of the model. In our case, we typically want to forecast the number of bikes available in a station, say, 30 minutes ahead. Then, once the 30 minutes have passed, the true number of available bikes will be available and we will be able to update the model using the features available 30 minutes ago. If you think about, this is exactly how a real-time machine learning system should work. The problem is that this isn't at all what the online_score method, indeed it is simply asking the model to predict the next observation, which is only a few minutes ahead, and then updates the model immediately. We can prove that this is flawed by adding a feature that measures a running average of the very recent values. X_y = datasets . Bikes () model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) + feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ( 0.5 )) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () metric = metrics . MAE () model_selection . progressive_val_score ( X_y , model , metric , print_every = 20_000 ) [20,000] MAE: 20.159286 [40,000] MAE: 10.458898 [60,000] MAE: 7.2759 [80,000] MAE: 5.715397 [100,000] MAE: 4.775094 [120,000] MAE: 4.138421 [140,000] MAE: 3.682591 [160,000] MAE: 3.35015 [180,000] MAE: 3.091398 MAE: 3.06414 The score we got is too good to be true. This is simply because the problem is too easy. What we really want is to evaluate the model by forecasting 30 minutes ahead and only updating the model once the true values are available. This can be done using the online_qa_score method, also from the model_selection module. The \"qa\" part stands for \"question/answer\". The idea is that each observation of the stream of the data is shown twice to the model: once for making a prediction, and once for updating the model when the true value is revealed. The on parameter determines which variable should be used as a timestamp, while the lag parameter controls the duration to wait before revealing the true values to the model. import datetime as dt model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) + feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ( 0.5 )) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () model_selection . progressive_val_score ( X_y = datasets . Bikes (), model = model , metric = metrics . MAE (), moment = 'moment' , delay = dt . timedelta ( minutes = 30 ), print_every = 20_000 ) [20,000] MAE: 2.24812 [40,000] MAE: 2.240287 [60,000] MAE: 2.270287 [80,000] MAE: 2.28649 [100,000] MAE: 2.294264 [120,000] MAE: 2.275891 [140,000] MAE: 2.261411 [160,000] MAE: 2.285978 [180,000] MAE: 2.289353 MAE: 2.29304 The score we now have is much more realistic, as it is comparable with related data science competitions . Moreover, we can see that the model gets better with time, which feels better than the previous situations. The point is that online_qa_score method can be used to simulate a production scenario, and is thus extremely valuable. Now that we have a working pipeline in place, we can attempt to make it more accurate. As a simple example, we'll using a HedgeRegressor from the ensemble module to combine 3 linear regression model trained with different optimizers. The HedgeRegressor will run the 3 models in parallel and assign weights to each model based on their individual performance. from creme import ensemble from creme import optim model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) ) model += feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ( 0.5 )) model |= preprocessing . StandardScaler () model |= ensemble . HedgeRegressor ([ linear_model . LinearRegression ( optim . SGD ()), linear_model . LinearRegression ( optim . RMSProp ()), linear_model . LinearRegression ( optim . Adam ()) ]) model_selection . progressive_val_score ( X_y = datasets . Bikes (), model = model , metric = metrics . MAE (), moment = 'moment' , delay = dt . timedelta ( minutes = 30 ), print_every = 20_000 ) [20,000] MAE: 2.253263 [40,000] MAE: 2.242859 [60,000] MAE: 2.272001 [80,000] MAE: 2.287776 [100,000] MAE: 2.295292 [120,000] MAE: 2.276748 [140,000] MAE: 2.262146 [160,000] MAE: 2.286621 [180,000] MAE: 2.289925 MAE: 2.293604","title":"Bike-sharing forecasting"},{"location":"examples/building-a-simple-time-series-model/","text":"Building a simple time series model \u00b6 % matplotlib inline We'll be using the international airline passenger data available from here . This particular dataset is included with creme in the datasets module. from creme import datasets for x , y in datasets . AirlinePassengers (): print ( x , y ) break {'month': datetime.datetime(1949, 1, 1, 0, 0)} 112 The data is as simple as can be: it consists of a sequence of months and values representing the total number of international airline passengers per month. Our goal is going to be to predict the number of passengers for the next month at each step. Notice that because the dataset is small -- which is usually the case for time series -- we could just fit a model from scratch each month. However for the sake of example we're going to train a single model online. Although the overall performance might be potentially weaker, training a time series model online has the benefit of being scalable if, say, you have have thousands of time series to manage . We'll start with a very simple model where the only feature will be the ordinal date of each month. This should be able to capture some of the underlying trend. from creme import compose from creme import linear_model from creme import preprocessing def get_ordinal_date ( x ): return { 'ordinal_date' : x [ 'month' ] . toordinal ()} model = compose . Pipeline ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ()) ) We'll write down a function to evaluate the model. This will go through each observation in the dataset and update the model as it goes on. The prior predictions will be stored along with the true values and will be plotted together. from creme import metrics import matplotlib.pyplot as plt def evaluate_model ( model ): metric = metrics . Rolling ( metrics . MAE (), 12 ) dates = [] y_trues = [] y_preds = [] for x , y in datasets . AirlinePassengers (): # Obtain the prior prediction and update the model in one go y_pred = model . predict_one ( x ) model . learn_one ( x , y ) # Update the error metric metric . update ( y , y_pred ) # Store the true value and the prediction dates . append ( x [ 'month' ]) y_trues . append ( y ) y_preds . append ( y_pred ) # Plot the results fig , ax = plt . subplots ( figsize = ( 10 , 6 )) ax . grid ( alpha = 0.75 ) ax . plot ( dates , y_trues , lw = 3 , color = '#2ecc71' , alpha = 0.8 , label = 'Ground truth' ) ax . plot ( dates , y_preds , lw = 3 , color = '#e74c3c' , alpha = 0.8 , label = 'Prediction' ) ax . legend () ax . set_title ( metric ) Let's evaluate our first model. evaluate_model ( model ) The model has captured a trend but not the right one. Indeed it thinks the trend is linear whereas we can visually see that the growth of the data increases with time. In other words the second derivative of the series is positive. This is a well know problem in time series forecasting and there are thus many ways to handle it; for example by using a Box-Cox transform . However we are going to do something a bit different, and instead linearly detrend the series using a Detrender . We'll set window_size to 12 in order to use a rolling mean of size 12 for detrending. The Detrender will center the target in 0, which means that we don't need an intercept in our linear regression. We can thus set intercept_lr to 0. from creme import stats from creme import time_series model = compose . Pipeline ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 )), ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) Now let's try and capture the monthly trend by one-hot encoding the month name. import calendar def get_month ( x ): return { calendar . month_name [ month ]: month == x [ 'month' ] . month for month in range ( 1 , 13 ) } model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'month' , compose . FuncTransformer ( get_month )), )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 )) ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) This seems pretty decent. We can take a look at the weights of the linear regression to get an idea of the importance of each feature. model . regressor [ 'lin_reg' ] . weights defaultdict(<creme.optim.initializers.Zeros at 0x1a35286310>, {'January': -9.469216150196292, 'February': -14.616539494085554, 'March': -3.543831579749453, 'April': -2.766323821459067, 'May': -0.6224556651312263, 'June': 13.478341968972643, 'July': 28.772701948655836, 'August': 26.88560005378563, 'September': 4.707302312777268, 'October': -8.705184336916485, 'November': -22.48440069047625, 'December': -13.372426538578132, 'ordinal_date': 13.829969822169433}) As could be expected the months of July and August have the highest weights because these are the months where people typically go on holiday abroad. The month of December has a low weight because this is a month of festivities in most of the Western world where people usually stay at home. Our model seems to understand which months are important, but it fails to see that the importance of each month grows multiplicatively as the years go on. In other words our model is too shy. We can fix this by increasing the learning rate of the LinearRegression 's optimizer. from creme import optim model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'month' , compose . FuncTransformer ( get_month )), )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 0.03 ) )) ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) This is starting to look good! Naturally in production we would tune the learning rate, ideally in real-time. Before finishing, we're going to introduce a cool feature extraction trick based on radial basis function kernels . The one-hot encoding we did on the month is a good idea but if you think about it is a bit rigid. Indeed the value of each feature is going to be 0 or 1, depending on the month of each observation. We're basically saying that the month of September is as distant to the month of August as it is to the month of March. Of course this isn't true, and it would be nice if our features would reflect this. To do so we can simply calculate the distance between the month of each observation and all the months in the calendar. Instead of simply computing the distance linearly, we're going to use a so-called Gaussian radial basic function kernel . This is a bit of a mouthful but for us it boils down to a simple formula, which is: d(i, j) = exp(-\\frac{(i - j)^2}{2\\sigma^2}) d(i, j) = exp(-\\frac{(i - j)^2}{2\\sigma^2}) Intuitively this computes a similarity between two months -- denoted by i i and j j -- which decreases the further apart they are from each other. The sigma sigma parameter can be seen as a hyperparameter than can be tuned -- in the following snippet we'll simply ignore it. The thing to take away is that this results in smoother predictions than when using a one-hot encoding scheme, which is often a desirable property. You can also see trick in action in this nice presentation . import math def get_month_distances ( x ): return { calendar . month_name [ month ]: math . exp ( - ( x [ 'month' ] . month - month ) ** 2 ) for month in range ( 1 , 13 ) } model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'month_distances' , compose . FuncTransformer ( get_month_distances )), )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 0.03 ) )) ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) We've managed to get a good looking prediction curve with a reasonably simple model. What's more our model has the advantage of being interpretable and easy to debug. There surely are more rocks to squeeze (e.g. tune the hyperparameters, use an ensemble model, etc.) but we'll leave that as an exercice to the reader. As a finishing touch we'll rewrite our pipeline using the | operator, which is called a \"pipe\". extract_features = compose . TransformerUnion ( get_ordinal_date , get_month_distances ) scale = preprocessing . StandardScaler () learn = linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 0.03 ) ) model = extract_features | scale | learn model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model )","title":"Building a simple time series model"},{"location":"examples/building-a-simple-time-series-model/#building-a-simple-time-series-model","text":"% matplotlib inline We'll be using the international airline passenger data available from here . This particular dataset is included with creme in the datasets module. from creme import datasets for x , y in datasets . AirlinePassengers (): print ( x , y ) break {'month': datetime.datetime(1949, 1, 1, 0, 0)} 112 The data is as simple as can be: it consists of a sequence of months and values representing the total number of international airline passengers per month. Our goal is going to be to predict the number of passengers for the next month at each step. Notice that because the dataset is small -- which is usually the case for time series -- we could just fit a model from scratch each month. However for the sake of example we're going to train a single model online. Although the overall performance might be potentially weaker, training a time series model online has the benefit of being scalable if, say, you have have thousands of time series to manage . We'll start with a very simple model where the only feature will be the ordinal date of each month. This should be able to capture some of the underlying trend. from creme import compose from creme import linear_model from creme import preprocessing def get_ordinal_date ( x ): return { 'ordinal_date' : x [ 'month' ] . toordinal ()} model = compose . Pipeline ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ()) ) We'll write down a function to evaluate the model. This will go through each observation in the dataset and update the model as it goes on. The prior predictions will be stored along with the true values and will be plotted together. from creme import metrics import matplotlib.pyplot as plt def evaluate_model ( model ): metric = metrics . Rolling ( metrics . MAE (), 12 ) dates = [] y_trues = [] y_preds = [] for x , y in datasets . AirlinePassengers (): # Obtain the prior prediction and update the model in one go y_pred = model . predict_one ( x ) model . learn_one ( x , y ) # Update the error metric metric . update ( y , y_pred ) # Store the true value and the prediction dates . append ( x [ 'month' ]) y_trues . append ( y ) y_preds . append ( y_pred ) # Plot the results fig , ax = plt . subplots ( figsize = ( 10 , 6 )) ax . grid ( alpha = 0.75 ) ax . plot ( dates , y_trues , lw = 3 , color = '#2ecc71' , alpha = 0.8 , label = 'Ground truth' ) ax . plot ( dates , y_preds , lw = 3 , color = '#e74c3c' , alpha = 0.8 , label = 'Prediction' ) ax . legend () ax . set_title ( metric ) Let's evaluate our first model. evaluate_model ( model ) The model has captured a trend but not the right one. Indeed it thinks the trend is linear whereas we can visually see that the growth of the data increases with time. In other words the second derivative of the series is positive. This is a well know problem in time series forecasting and there are thus many ways to handle it; for example by using a Box-Cox transform . However we are going to do something a bit different, and instead linearly detrend the series using a Detrender . We'll set window_size to 12 in order to use a rolling mean of size 12 for detrending. The Detrender will center the target in 0, which means that we don't need an intercept in our linear regression. We can thus set intercept_lr to 0. from creme import stats from creme import time_series model = compose . Pipeline ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 )), ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) Now let's try and capture the monthly trend by one-hot encoding the month name. import calendar def get_month ( x ): return { calendar . month_name [ month ]: month == x [ 'month' ] . month for month in range ( 1 , 13 ) } model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'month' , compose . FuncTransformer ( get_month )), )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 )) ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) This seems pretty decent. We can take a look at the weights of the linear regression to get an idea of the importance of each feature. model . regressor [ 'lin_reg' ] . weights defaultdict(<creme.optim.initializers.Zeros at 0x1a35286310>, {'January': -9.469216150196292, 'February': -14.616539494085554, 'March': -3.543831579749453, 'April': -2.766323821459067, 'May': -0.6224556651312263, 'June': 13.478341968972643, 'July': 28.772701948655836, 'August': 26.88560005378563, 'September': 4.707302312777268, 'October': -8.705184336916485, 'November': -22.48440069047625, 'December': -13.372426538578132, 'ordinal_date': 13.829969822169433}) As could be expected the months of July and August have the highest weights because these are the months where people typically go on holiday abroad. The month of December has a low weight because this is a month of festivities in most of the Western world where people usually stay at home. Our model seems to understand which months are important, but it fails to see that the importance of each month grows multiplicatively as the years go on. In other words our model is too shy. We can fix this by increasing the learning rate of the LinearRegression 's optimizer. from creme import optim model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'month' , compose . FuncTransformer ( get_month )), )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 0.03 ) )) ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) This is starting to look good! Naturally in production we would tune the learning rate, ideally in real-time. Before finishing, we're going to introduce a cool feature extraction trick based on radial basis function kernels . The one-hot encoding we did on the month is a good idea but if you think about it is a bit rigid. Indeed the value of each feature is going to be 0 or 1, depending on the month of each observation. We're basically saying that the month of September is as distant to the month of August as it is to the month of March. Of course this isn't true, and it would be nice if our features would reflect this. To do so we can simply calculate the distance between the month of each observation and all the months in the calendar. Instead of simply computing the distance linearly, we're going to use a so-called Gaussian radial basic function kernel . This is a bit of a mouthful but for us it boils down to a simple formula, which is: d(i, j) = exp(-\\frac{(i - j)^2}{2\\sigma^2}) d(i, j) = exp(-\\frac{(i - j)^2}{2\\sigma^2}) Intuitively this computes a similarity between two months -- denoted by i i and j j -- which decreases the further apart they are from each other. The sigma sigma parameter can be seen as a hyperparameter than can be tuned -- in the following snippet we'll simply ignore it. The thing to take away is that this results in smoother predictions than when using a one-hot encoding scheme, which is often a desirable property. You can also see trick in action in this nice presentation . import math def get_month_distances ( x ): return { calendar . month_name [ month ]: math . exp ( - ( x [ 'month' ] . month - month ) ** 2 ) for month in range ( 1 , 13 ) } model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'month_distances' , compose . FuncTransformer ( get_month_distances )), )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 0.03 ) )) ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) We've managed to get a good looking prediction curve with a reasonably simple model. What's more our model has the advantage of being interpretable and easy to debug. There surely are more rocks to squeeze (e.g. tune the hyperparameters, use an ensemble model, etc.) but we'll leave that as an exercice to the reader. As a finishing touch we'll rewrite our pipeline using the | operator, which is called a \"pipe\". extract_features = compose . TransformerUnion ( get_ordinal_date , get_month_distances ) scale = preprocessing . StandardScaler () learn = linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 0.03 ) ) model = extract_features | scale | learn model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model )","title":"Building a simple time series model"},{"location":"examples/debugging-a-pipeline/","text":"Debugging a pipeline \u00b6 creme encourages users to make use of pipelines. The biggest pain point of pipelines is that it can be hard to understand what's happening to the data, especially when the pipeline is complex. Fortunately the Pipeline class has a debug_one method that can help out. Let's look at a fairly complex pipeline for predicting the number of bikes in 5 bike stations from the city of Toulouse. It doesn't matter if you understand the pipeline or not; the point of this notebook is to learn how to introspect a pipeline. import datetime as dt from creme import compose from creme import datasets from creme import feature_extraction from creme import linear_model from creme import metrics from creme import preprocessing from creme import stats from creme import stream X_y = datasets . Bikes () X_y = stream . simulate_qa ( X_y , moment = 'moment' , delay = dt . timedelta ( minutes = 30 )) def add_time_features ( x ): return { ** x , 'hour' : x [ 'moment' ] . hour , 'day' : x [ 'moment' ] . weekday () } model = add_time_features model |= ( compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) + feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) + feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ()) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () metric = metrics . MAE () questions = {} for i , x , y in X_y : # Question is_question = y is None if is_question : y_pred = model . predict_one ( x ) questions [ i ] = y_pred # Answer else : metric . update ( y , questions [ i ]) model = model . learn_one ( x , y ) if i >= 30000 and i % 30000 == 0 : print ( i , metric ) 30000 MAE: 2.220942 60000 MAE: 2.270271 90000 MAE: 2.301302 120000 MAE: 2.275876 150000 MAE: 2.275224 180000 MAE: 2.289347 We can start by looking at what the pipeline looks by drawing it. model . draw () As mentionned above the Pipeline class has a debug_one method. You can use this at any point you want to visualize what happen to an input x . For example, let's see what happens to the last seen x . model . debug_one ( x ) '0. Input\\n--------\\nclouds: 88 (int)\\ndescription: overcast clouds (str)\\nhumidity: 84 (int)\\nmoment: 2016-10-05 09:57:18 (datetime)\\npressure: 1,017.34000 (float)\\nstation: pomme (str)\\ntemperature: 17.45000 (float)\\nwind: 1.95000 (float)\\n\\n1. add_time_features\\n--------------------\\nclouds: 88 (int)\\nday: 2 (int)\\ndescription: overcast clouds (str)\\nhour: 9 (int)\\nhumidity: 84 (int)\\nmoment: 2016-10-05 09:57:18 (datetime)\\npressure: 1,017.34000 (float)\\nstation: pomme (str)\\ntemperature: 17.45000 (float)\\nwind: 1.95000 (float)\\n\\n2. Transformer union\\n--------------------\\n 2.0 Select\\n ----------\\n clouds: 88 (int)\\n humidity: 84 (int)\\n pressure: 1,017.34000 (float)\\n temperature: 17.45000 (float)\\n wind: 1.95000 (float)\\n\\n 2.1 TargetAgg\\n -------------\\n target_mean_by_station_and_hour: 7.89396 (float)\\n\\n 2.2 TargetAgg1\\n --------------\\n target_ewm_0.5_by_station: 11.80372 (float)\\n\\nclouds: 88 (int)\\nhumidity: 84 (int)\\npressure: 1,017.34000 (float)\\ntarget_ewm_0.5_by_station: 11.80372 (float)\\ntarget_mean_by_station_and_hour: 7.89396 (float)\\ntemperature: 17.45000 (float)\\nwind: 1.95000 (float)\\n\\n3. StandardScaler\\n-----------------\\nclouds: 1.54778 (float)\\nhumidity: 1.16366 (float)\\npressure: 0.04916 (float)\\ntarget_ewm_0.5_by_station: 0.19214 (float)\\ntarget_mean_by_station_and_hour: -0.26013 (float)\\ntemperature: -0.51938 (float)\\nwind: -0.69426 (float)\\n\\n4. LinearRegression\\n-------------------\\nName Value Weight Contribution \\n Intercept 1.00000 9.22316 9.22316 \\n target_ewm_0.5_by_station 0.19214 9.26418 1.78000 \\n humidity 1.16366 1.01252 1.17823 \\n temperature -0.51938 -0.42112 0.21872 \\n wind -0.69426 -0.04088 0.02838 \\n pressure 0.04916 0.18137 0.00892 \\ntarget_mean_by_station_and_hour -0.26013 0.19801 -0.05151 \\n clouds 1.54778 -0.32697 -0.50608 \\n\\nPrediction: 11.87982' The pipeline does quite a few things, but using debug_one shows what happens step by step. This is really useful for checking that the pipeline is behaving as you're expecting it too. Remember that you can debug_one whenever you wish, be it before, during, or after training a model.","title":"Debugging a pipeline"},{"location":"examples/debugging-a-pipeline/#debugging-a-pipeline","text":"creme encourages users to make use of pipelines. The biggest pain point of pipelines is that it can be hard to understand what's happening to the data, especially when the pipeline is complex. Fortunately the Pipeline class has a debug_one method that can help out. Let's look at a fairly complex pipeline for predicting the number of bikes in 5 bike stations from the city of Toulouse. It doesn't matter if you understand the pipeline or not; the point of this notebook is to learn how to introspect a pipeline. import datetime as dt from creme import compose from creme import datasets from creme import feature_extraction from creme import linear_model from creme import metrics from creme import preprocessing from creme import stats from creme import stream X_y = datasets . Bikes () X_y = stream . simulate_qa ( X_y , moment = 'moment' , delay = dt . timedelta ( minutes = 30 )) def add_time_features ( x ): return { ** x , 'hour' : x [ 'moment' ] . hour , 'day' : x [ 'moment' ] . weekday () } model = add_time_features model |= ( compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) + feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) + feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ()) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () metric = metrics . MAE () questions = {} for i , x , y in X_y : # Question is_question = y is None if is_question : y_pred = model . predict_one ( x ) questions [ i ] = y_pred # Answer else : metric . update ( y , questions [ i ]) model = model . learn_one ( x , y ) if i >= 30000 and i % 30000 == 0 : print ( i , metric ) 30000 MAE: 2.220942 60000 MAE: 2.270271 90000 MAE: 2.301302 120000 MAE: 2.275876 150000 MAE: 2.275224 180000 MAE: 2.289347 We can start by looking at what the pipeline looks by drawing it. model . draw () As mentionned above the Pipeline class has a debug_one method. You can use this at any point you want to visualize what happen to an input x . For example, let's see what happens to the last seen x . model . debug_one ( x ) '0. Input\\n--------\\nclouds: 88 (int)\\ndescription: overcast clouds (str)\\nhumidity: 84 (int)\\nmoment: 2016-10-05 09:57:18 (datetime)\\npressure: 1,017.34000 (float)\\nstation: pomme (str)\\ntemperature: 17.45000 (float)\\nwind: 1.95000 (float)\\n\\n1. add_time_features\\n--------------------\\nclouds: 88 (int)\\nday: 2 (int)\\ndescription: overcast clouds (str)\\nhour: 9 (int)\\nhumidity: 84 (int)\\nmoment: 2016-10-05 09:57:18 (datetime)\\npressure: 1,017.34000 (float)\\nstation: pomme (str)\\ntemperature: 17.45000 (float)\\nwind: 1.95000 (float)\\n\\n2. Transformer union\\n--------------------\\n 2.0 Select\\n ----------\\n clouds: 88 (int)\\n humidity: 84 (int)\\n pressure: 1,017.34000 (float)\\n temperature: 17.45000 (float)\\n wind: 1.95000 (float)\\n\\n 2.1 TargetAgg\\n -------------\\n target_mean_by_station_and_hour: 7.89396 (float)\\n\\n 2.2 TargetAgg1\\n --------------\\n target_ewm_0.5_by_station: 11.80372 (float)\\n\\nclouds: 88 (int)\\nhumidity: 84 (int)\\npressure: 1,017.34000 (float)\\ntarget_ewm_0.5_by_station: 11.80372 (float)\\ntarget_mean_by_station_and_hour: 7.89396 (float)\\ntemperature: 17.45000 (float)\\nwind: 1.95000 (float)\\n\\n3. StandardScaler\\n-----------------\\nclouds: 1.54778 (float)\\nhumidity: 1.16366 (float)\\npressure: 0.04916 (float)\\ntarget_ewm_0.5_by_station: 0.19214 (float)\\ntarget_mean_by_station_and_hour: -0.26013 (float)\\ntemperature: -0.51938 (float)\\nwind: -0.69426 (float)\\n\\n4. LinearRegression\\n-------------------\\nName Value Weight Contribution \\n Intercept 1.00000 9.22316 9.22316 \\n target_ewm_0.5_by_station 0.19214 9.26418 1.78000 \\n humidity 1.16366 1.01252 1.17823 \\n temperature -0.51938 -0.42112 0.21872 \\n wind -0.69426 -0.04088 0.02838 \\n pressure 0.04916 0.18137 0.00892 \\ntarget_mean_by_station_and_hour -0.26013 0.19801 -0.05151 \\n clouds 1.54778 -0.32697 -0.50608 \\n\\nPrediction: 11.87982' The pipeline does quite a few things, but using debug_one shows what happens step by step. This is really useful for checking that the pipeline is behaving as you're expecting it too. Remember that you can debug_one whenever you wish, be it before, during, or after training a model.","title":"Debugging a pipeline"},{"location":"examples/imbalanced-learning/","text":"Working with imbalanced data \u00b6 In machine learning it is quite usual to have to deal with imbalanced dataset. This is particularly true in online learning for tasks such as fraud detection and spam classification. In these two cases, which are binary classification problems, there are usually many more 0s than 1s, which generally hinders the performance of the classifiers we thrown at them. As an example we'll use the credit card dataset available in creme . We'll first use a collections.Counter to count the number of 0s and 1s in order to get an idea of the class balance. import collections from creme import datasets X_y = datasets . CreditCard () counts = collections . Counter ( y for _ , y in X_y ) for c , count in counts . items (): print ( f ' { c } : { count } ( { count / sum ( counts . values ()) : .5% } )' ) 0: 284315 (99.82725%) 1: 492 (0.17275%) Baseline \u00b6 The dataset is quite unbalanced. For each 1 there are about 578 0s. Let's now train a logistic regression with default parameters and see how well it does. We'll measure the ROC AUC score. from creme import linear_model from creme import metrics from creme import model_selection from creme import preprocessing X_y = datasets . CreditCard () model = ( preprocessing . StandardScaler () | linear_model . LogisticRegression () ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.891072 Importance weighting \u00b6 The performance is already quite acceptable, but as we will now see we can do even better. The first thing we can do is to add weight to the 1s by using the weight_pos argument of the Log loss function. from creme import optim model = ( preprocessing . StandardScaler () | linear_model . LogisticRegression ( loss = optim . losses . Log ( weight_pos = 5 ) ) ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.914269 Focal loss \u00b6 The deep learning for object detection community has produced a special loss function for imbalaced learning called focal loss . We are doing binary classification, so we can plug the binary version of focal loss into our logistic regression and see how well it fairs. model = ( preprocessing . StandardScaler () | linear_model . LogisticRegression ( loss = optim . losses . BinaryFocalLoss ( 2 , 1 )) ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.913072 Under-sampling the majority class \u00b6 Adding importance weights only works with gradient-based models (which includes neural networks). A more generic, and potentially more effective approach, is to use undersamplig and oversampling. As an example, we'll under-sample the stream so that our logistic regression encounter 20% of 1s and 80% of 0s. Under-sampling has the additional benefit of requiring less training steps, and thus reduces the total training time. from creme import sampling model = ( preprocessing . StandardScaler () | sampling . RandomUnderSampler ( classifier = linear_model . LogisticRegression (), desired_dist = { 0 : . 8 , 1 : . 2 }, seed = 42 ) ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.948824 The RandomUnderSampler class is a wrapper for classifiers. This is represented by a rectangle around the logistic regression bubble when we draw the model. model . draw () Over-sampling the minority class \u00b6 We can also attain the same class distribution by over-sampling the minority class. This will come at cost of having to train with more samples. model = ( preprocessing . StandardScaler () | sampling . RandomOverSampler ( classifier = linear_model . LogisticRegression (), desired_dist = { 0 : . 8 , 1 : . 2 }, seed = 42 ) ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.918082 Sampling with a desired sample size \u00b6 The downside of both RandomUnderSampler and RandomOverSampler is that you don't have any control on the amount of data the classifier trains on. The number of samples is adjusted so that the target distribution can be attained, either by under-sampling or over-sampling. However, you can do both at the same time and choose how much data the classifier will see. To do so, we can use the RandomSampler class. In addition to the desired class distribution, we can specify how much data to train on. The samples will both be under-sampled and over-sampled in order to fit your constraints. This is powerful because it allows you to control both the class distribution and the size of the training data (and thus the training time). In the following example we'll set it so that the model will train with 1 percent of the data. model = ( preprocessing . StandardScaler () | sampling . RandomSampler ( classifier = linear_model . LogisticRegression (), desired_dist = { 0 : . 8 , 1 : . 2 }, sampling_rate =. 01 , seed = 42 ) ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.951296 Hybrid approach \u00b6 As you might have guessed by now, nothing is stopping you from mixing imbalanced learning methods together. As an example, let's combine sampling.RandomUnderSampler and the weight_pos parameter from the optim.losses.Log loss function. model = ( preprocessing . StandardScaler () | sampling . RandomUnderSampler ( classifier = linear_model . LogisticRegression ( loss = optim . losses . Log ( weight_pos = 5 ) ), desired_dist = { 0 : . 8 , 1 : . 2 }, seed = 42 ) ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.968289","title":"Working with imbalanced data"},{"location":"examples/imbalanced-learning/#working-with-imbalanced-data","text":"In machine learning it is quite usual to have to deal with imbalanced dataset. This is particularly true in online learning for tasks such as fraud detection and spam classification. In these two cases, which are binary classification problems, there are usually many more 0s than 1s, which generally hinders the performance of the classifiers we thrown at them. As an example we'll use the credit card dataset available in creme . We'll first use a collections.Counter to count the number of 0s and 1s in order to get an idea of the class balance. import collections from creme import datasets X_y = datasets . CreditCard () counts = collections . Counter ( y for _ , y in X_y ) for c , count in counts . items (): print ( f ' { c } : { count } ( { count / sum ( counts . values ()) : .5% } )' ) 0: 284315 (99.82725%) 1: 492 (0.17275%)","title":"Working with imbalanced data"},{"location":"examples/imbalanced-learning/#baseline","text":"The dataset is quite unbalanced. For each 1 there are about 578 0s. Let's now train a logistic regression with default parameters and see how well it does. We'll measure the ROC AUC score. from creme import linear_model from creme import metrics from creme import model_selection from creme import preprocessing X_y = datasets . CreditCard () model = ( preprocessing . StandardScaler () | linear_model . LogisticRegression () ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.891072","title":"Baseline"},{"location":"examples/imbalanced-learning/#importance-weighting","text":"The performance is already quite acceptable, but as we will now see we can do even better. The first thing we can do is to add weight to the 1s by using the weight_pos argument of the Log loss function. from creme import optim model = ( preprocessing . StandardScaler () | linear_model . LogisticRegression ( loss = optim . losses . Log ( weight_pos = 5 ) ) ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.914269","title":"Importance weighting"},{"location":"examples/imbalanced-learning/#focal-loss","text":"The deep learning for object detection community has produced a special loss function for imbalaced learning called focal loss . We are doing binary classification, so we can plug the binary version of focal loss into our logistic regression and see how well it fairs. model = ( preprocessing . StandardScaler () | linear_model . LogisticRegression ( loss = optim . losses . BinaryFocalLoss ( 2 , 1 )) ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.913072","title":"Focal loss"},{"location":"examples/imbalanced-learning/#under-sampling-the-majority-class","text":"Adding importance weights only works with gradient-based models (which includes neural networks). A more generic, and potentially more effective approach, is to use undersamplig and oversampling. As an example, we'll under-sample the stream so that our logistic regression encounter 20% of 1s and 80% of 0s. Under-sampling has the additional benefit of requiring less training steps, and thus reduces the total training time. from creme import sampling model = ( preprocessing . StandardScaler () | sampling . RandomUnderSampler ( classifier = linear_model . LogisticRegression (), desired_dist = { 0 : . 8 , 1 : . 2 }, seed = 42 ) ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.948824 The RandomUnderSampler class is a wrapper for classifiers. This is represented by a rectangle around the logistic regression bubble when we draw the model. model . draw ()","title":"Under-sampling the majority class"},{"location":"examples/imbalanced-learning/#over-sampling-the-minority-class","text":"We can also attain the same class distribution by over-sampling the minority class. This will come at cost of having to train with more samples. model = ( preprocessing . StandardScaler () | sampling . RandomOverSampler ( classifier = linear_model . LogisticRegression (), desired_dist = { 0 : . 8 , 1 : . 2 }, seed = 42 ) ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.918082","title":"Over-sampling the minority class"},{"location":"examples/imbalanced-learning/#sampling-with-a-desired-sample-size","text":"The downside of both RandomUnderSampler and RandomOverSampler is that you don't have any control on the amount of data the classifier trains on. The number of samples is adjusted so that the target distribution can be attained, either by under-sampling or over-sampling. However, you can do both at the same time and choose how much data the classifier will see. To do so, we can use the RandomSampler class. In addition to the desired class distribution, we can specify how much data to train on. The samples will both be under-sampled and over-sampled in order to fit your constraints. This is powerful because it allows you to control both the class distribution and the size of the training data (and thus the training time). In the following example we'll set it so that the model will train with 1 percent of the data. model = ( preprocessing . StandardScaler () | sampling . RandomSampler ( classifier = linear_model . LogisticRegression (), desired_dist = { 0 : . 8 , 1 : . 2 }, sampling_rate =. 01 , seed = 42 ) ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.951296","title":"Sampling with a desired sample size"},{"location":"examples/imbalanced-learning/#hybrid-approach","text":"As you might have guessed by now, nothing is stopping you from mixing imbalanced learning methods together. As an example, let's combine sampling.RandomUnderSampler and the weight_pos parameter from the optim.losses.Log loss function. model = ( preprocessing . StandardScaler () | sampling . RandomUnderSampler ( classifier = linear_model . LogisticRegression ( loss = optim . losses . Log ( weight_pos = 5 ) ), desired_dist = { 0 : . 8 , 1 : . 2 }, seed = 42 ) ) metric = metrics . ROCAUC () model_selection . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.968289","title":"Hybrid approach"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/","text":"Matrix Factorization for Recommender Systems - Part 1 \u00b6 Table of contents of this tutorial series on matrix factorization for recommender systems: Part 1 - Traditional Matrix Factorization methods for Recommender Systems Part 2 - Factorization Machines and Field-aware Factorization Machines Part 3 - Large scale learning and better predictive power with multiple pass learning Introduction \u00b6 A recommender system is a software tool designed to generate and suggest items or entities to the users. Popular large scale examples include: Amazon (suggesting products) Facebook (suggesting posts in users' news feeds) Spotify (suggesting music) Social recommendation from graph (mostly used by social networks) are not covered in creme . We focus on the general case, item recommendation. This problem can be represented with the user-item matrix: \\normalsize \\begin{matrix} & \\begin{matrix} _1 & _\\cdots & _\\cdots & _\\cdots & _I \\end{matrix} \\\\ \\begin{matrix} _1 \\\\ _\\vdots \\\\ _\\vdots \\\\ _\\vdots \\\\ _U \\end{matrix} & \\begin{bmatrix} {\\color{Red} ?} & 2 & \\cdots & {\\color{Red} ?} & {\\color{Red} ?} \\\\ {\\color{Red} ?} & {\\color{Red} ?} & \\cdots & {\\color{Red} ?} & 4.5 \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ 3 & {\\color{Red} ?} & \\cdots & {\\color{Red} ?} & {\\color{Red} ?} \\\\ {\\color{Red} ?} & {\\color{Red} ?} & \\cdots & 5 & {\\color{Red} ?} \\end{bmatrix} \\end{matrix} \\normalsize \\begin{matrix} & \\begin{matrix} _1 & _\\cdots & _\\cdots & _\\cdots & _I \\end{matrix} \\\\ \\begin{matrix} _1 \\\\ _\\vdots \\\\ _\\vdots \\\\ _\\vdots \\\\ _U \\end{matrix} & \\begin{bmatrix} {\\color{Red} ?} & 2 & \\cdots & {\\color{Red} ?} & {\\color{Red} ?} \\\\ {\\color{Red} ?} & {\\color{Red} ?} & \\cdots & {\\color{Red} ?} & 4.5 \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ 3 & {\\color{Red} ?} & \\cdots & {\\color{Red} ?} & {\\color{Red} ?} \\\\ {\\color{Red} ?} & {\\color{Red} ?} & \\cdots & 5 & {\\color{Red} ?} \\end{bmatrix} \\end{matrix} Where U U and I I are the number of user and item of the system, respectively. A matrix entry represents a user's preference for an item, it can be a rating, a like or dislike, etc. Because of the huge number of users and items compared to the number of observed entries, those matrices are very sparsed (usually less than 1% filled). Matrix Factorization (MF) is a class of collaborative filtering algorithms derived from Singular Value Decomposition (SVD) . MF strength lies in its capacity to able to model high cardinality categorical variables interactions. This subfield boomed during the famous Netflix Prize contest in 2006, when numerous novel variants has been invented and became popular thanks to their attractive accuracy and scalability. MF approach seeks to fill the user-item matrix considering the problem as a matrix completion one. MF core idea assume a latent model learning its own representation of the users and the items in a lower latent dimensional space by factorizing the observed parts of the matrix. A factorized user or item is represented as a vector \\mathbf{v}_u \\mathbf{v}_u or \\mathbf{v}_i \\mathbf{v}_i composed of k k latent factors, with k << U, I k << U, I . Those learnt latent variables represent, for an item the various aspects describing it, and for a user its interests in terms of those aspects. The model then assume a user's choice or fondness is composed of a sum of preferences about the various aspects of the concerned item. This sum being the dot product between the latent vectors of a given user-item pair: \\normalsize \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f} \\normalsize \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f} MF models weights are learnt in an online fashion, often with stochastic gradient descent as it provides relatively fast running time and good accuracy. There is a great and widely popular library named surprise that implements MF models (and others) but in contrast with creme doesn't follow a pure online philosophy (all the data have to be loaded in memory and the API doesn't allow you to update your model with new data). Notes: In recent years, proposed deep learning techniques for recommendation tasks claim state of the art results. However, recent work (August 2019) showed that those promises can't be taken for granted and traditional MF methods are still relevant today. For more information about how the business value of recommender systems is measured and why they are one of the main success stories of machine learning, see the following literature survey (December 2019). Let's start \u00b6 In this tutorial, we are going to explore MF algorithms available in creme and test them on a movie recommendation problem with the MovieLens 100K dataset. This latter is a collection of movie ratings (from 1 to 5) that includes various information about both the items and the users. We can access it from the creme.datasets module: import json from creme import datasets for x , y in datasets . MovieLens100K (): print ( f 'x = { json . dumps ( x , indent = 4 ) } \\n y = { y } ' ) break x = { \"user\": \"259\", \"item\": \"255\", \"timestamp\": 874731910000000000, \"title\": \"My Best Friend's Wedding (1997)\", \"release_date\": 866764800000000000, \"genres\": \"comedy, romance\", \"age\": 21.0, \"gender\": \"M\", \"occupation\": \"student\", \"zip_code\": \"48823\" } y = 4.0 Let's define a routine to evaluate our different models on MovieLens 100K. Mean Absolute Error and Root Mean Squared Error will be our metrics printed alongside model's computation time and memory usage: from creme import metrics from creme.evaluate import progressive_val_score def evaluate ( model ): X_y = datasets . MovieLens100K () metric = metrics . MAE () + metrics . RMSE () _ = progressive_val_score ( X_y , model , metric , print_every = 25_000 , show_time = True , show_memory = True ) Naive prediction \u00b6 It's good practice in machine learning to start with a naive baseline and then iterate from simple things to complex ones observing progress incrementally. Let's start by predicing the target running mean as a first shot: from creme import stats mean = stats . Mean () metric = metrics . MAE () + metrics . RMSE () for i , x_y in enumerate ( datasets . MovieLens100K (), start = 1 ): _ , y = x_y metric . update ( y , mean . get ()) mean . update ( y ) if not i % 25_000 : print ( f '[ { i : ,d } ] { metric } ' ) [25,000] MAE: 0.934259, RMSE: 1.124469 [50,000] MAE: 0.923893, RMSE: 1.105 [75,000] MAE: 0.937359, RMSE: 1.123696 [100,000] MAE: 0.942162, RMSE: 1.125783 Baseline model \u00b6 Now we can do machine learning and explore available models in creme.reco module starting with the baseline model. It extends our naive prediction by adding to the global running mean two bias terms characterizing the user and the item discrepancy from the general tendency. The model equation is defined as: \\normalsize \\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} \\normalsize \\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} This baseline model can be viewed as a linear regression where the intercept is replaced by the target running mean with the users and the items one hot encoded. All machine learning models in creme expect dicts as input with feature names as keys and feature values as values. Specifically, models from creme.reco expect a 'user' and an 'item' entries without any type constraint on their values (i.e. can be strings or numbers), e.g.: x = { 'user' : 'Guido' , 'item' : \"Monty Python's Flying Circus\" } Other entries, if exist, are simply ignored. This is quite usefull as we don't need to spend time and storage doing one hot encoding. from creme import meta from creme import optim from creme import reco baseline_params = { 'optimizer' : optim . SGD ( 0.025 ), 'l2' : 0. , 'initializer' : optim . initializers . Zeros () } model = meta . PredClipper ( regressor = reco . Baseline ( ** baseline_params ), y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761844, RMSE: 0.960972 \u2013 0:00:00 \u2013 170.01 KB [50,000] MAE: 0.753292, RMSE: 0.951223 \u2013 0:00:01 \u2013 238.65 KB [75,000] MAE: 0.754177, RMSE: 0.953376 \u2013 0:00:02 \u2013 282.46 KB [100,000] MAE: 0.754651, RMSE: 0.954148 \u2013 0:00:03 \u2013 306.06 KB We won two tenth of MAE compared to our naive prediction (0.7546 vs 0.9421) meaning that significant information has been learnt by the model. Funk Matrix Factorization (FunkMF) \u00b6 It's the pure form of matrix factorization consisting of only learning the users and items latent representations as discussed in introduction. Simon Funk popularized its stochastic gradient descent optimization in 2006 during the Netflix Prize. The model equation is defined as: \\normalsize \\hat{y}(x) = \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle \\normalsize \\hat{y}(x) = \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle Note: FunkMF is sometimes refered as Probabilistic Matrix Factorization which is an extended probabilistic version. funk_mf_params = { 'n_factors' : 10 , 'optimizer' : optim . SGD ( 0.05 ), 'l2' : 0.1 , 'initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 73 ) } model = meta . PredClipper ( regressor = reco . FunkMF ( ** funk_mf_params ), y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 1.070136, RMSE: 1.397014 \u2013 0:00:01 \u2013 925.42 KB [50,000] MAE: 0.99174, RMSE: 1.290666 \u2013 0:00:02 \u2013 1.11 MB [75,000] MAE: 0.961072, RMSE: 1.250842 \u2013 0:00:04 \u2013 1.31 MB [100,000] MAE: 0.944883, RMSE: 1.227688 \u2013 0:00:05 \u2013 1.48 MB Results are equivalent to our naive prediction (0.9448 vs 0.9421). By only focusing on the users preferences and the items characteristics, the model is limited in his ability to capture different views of the problem. Despite its poor performance alone, this algorithm is quite usefull combined in other models or when we need to build dense representations for other tasks. Biased Matrix Factorization (BiasedMF) \u00b6 It's the combination of the Baseline model and FunkMF. The model equation is defined as: \\normalsize \\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} + \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle \\normalsize \\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} + \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle Note: Biased Matrix Factorization name is used by some people but some others refer to it by SVD or Funk SVD . It's the case of Yehuda Koren and Robert Bell in Recommender Systems Handbook (Chapter 5 Advances in Collaborative Filtering ) and of surprise library. Nevertheless, SVD could be confused with the original Singular Value Decomposition from which it's derived from, and Funk SVD could also be misleading because of the biased part of the model equation which doesn't come from Simon Funk's work. For those reasons, we chose to side with Biased Matrix Factorization which fits more naturally to it. biased_mf_params = { 'n_factors' : 10 , 'bias_optimizer' : optim . SGD ( 0.025 ), 'latent_optimizer' : optim . SGD ( 0.05 ), 'weight_initializer' : optim . initializers . Zeros (), 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 73 ), 'l2_bias' : 0. , 'l2_latent' : 0. } model = meta . PredClipper ( regressor = reco . BiasedMF ( ** biased_mf_params ), y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761818, RMSE: 0.961057 \u2013 0:00:01 \u2013 0.99 MB [50,000] MAE: 0.751667, RMSE: 0.949443 \u2013 0:00:03 \u2013 1.25 MB [75,000] MAE: 0.749653, RMSE: 0.948723 \u2013 0:00:05 \u2013 1.47 MB [100,000] MAE: 0.748559, RMSE: 0.947854 \u2013 0:00:06 \u2013 1.65 MB Results improved (0.7485 vs 0.7546) demonstrating that users and items latent representations bring additional information. To conclude this first tutorial about factorization models, let's review the important parameters to tune when dealing with this family of methods: n_factors : the number of latent factors. The more you set, the more items aspects and users preferences you are going to learn. Too many will cause overfitting, l2 regularization could help. *_optimizer : the optimizers. Classic stochastic gradient descent performs well, finding the good learning rate will make the difference. initializer : the latent weights initialization. Latent vectors have to be initialized with non-constant values. We generally sample them from a zero-mean normal distribution with small standard deviation.","title":"Matrix Factorization for Recommender Systems - Part 1"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#matrix-factorization-for-recommender-systems-part-1","text":"Table of contents of this tutorial series on matrix factorization for recommender systems: Part 1 - Traditional Matrix Factorization methods for Recommender Systems Part 2 - Factorization Machines and Field-aware Factorization Machines Part 3 - Large scale learning and better predictive power with multiple pass learning","title":"Matrix Factorization for Recommender Systems - Part 1"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#introduction","text":"A recommender system is a software tool designed to generate and suggest items or entities to the users. Popular large scale examples include: Amazon (suggesting products) Facebook (suggesting posts in users' news feeds) Spotify (suggesting music) Social recommendation from graph (mostly used by social networks) are not covered in creme . We focus on the general case, item recommendation. This problem can be represented with the user-item matrix: \\normalsize \\begin{matrix} & \\begin{matrix} _1 & _\\cdots & _\\cdots & _\\cdots & _I \\end{matrix} \\\\ \\begin{matrix} _1 \\\\ _\\vdots \\\\ _\\vdots \\\\ _\\vdots \\\\ _U \\end{matrix} & \\begin{bmatrix} {\\color{Red} ?} & 2 & \\cdots & {\\color{Red} ?} & {\\color{Red} ?} \\\\ {\\color{Red} ?} & {\\color{Red} ?} & \\cdots & {\\color{Red} ?} & 4.5 \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ 3 & {\\color{Red} ?} & \\cdots & {\\color{Red} ?} & {\\color{Red} ?} \\\\ {\\color{Red} ?} & {\\color{Red} ?} & \\cdots & 5 & {\\color{Red} ?} \\end{bmatrix} \\end{matrix} \\normalsize \\begin{matrix} & \\begin{matrix} _1 & _\\cdots & _\\cdots & _\\cdots & _I \\end{matrix} \\\\ \\begin{matrix} _1 \\\\ _\\vdots \\\\ _\\vdots \\\\ _\\vdots \\\\ _U \\end{matrix} & \\begin{bmatrix} {\\color{Red} ?} & 2 & \\cdots & {\\color{Red} ?} & {\\color{Red} ?} \\\\ {\\color{Red} ?} & {\\color{Red} ?} & \\cdots & {\\color{Red} ?} & 4.5 \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ 3 & {\\color{Red} ?} & \\cdots & {\\color{Red} ?} & {\\color{Red} ?} \\\\ {\\color{Red} ?} & {\\color{Red} ?} & \\cdots & 5 & {\\color{Red} ?} \\end{bmatrix} \\end{matrix} Where U U and I I are the number of user and item of the system, respectively. A matrix entry represents a user's preference for an item, it can be a rating, a like or dislike, etc. Because of the huge number of users and items compared to the number of observed entries, those matrices are very sparsed (usually less than 1% filled). Matrix Factorization (MF) is a class of collaborative filtering algorithms derived from Singular Value Decomposition (SVD) . MF strength lies in its capacity to able to model high cardinality categorical variables interactions. This subfield boomed during the famous Netflix Prize contest in 2006, when numerous novel variants has been invented and became popular thanks to their attractive accuracy and scalability. MF approach seeks to fill the user-item matrix considering the problem as a matrix completion one. MF core idea assume a latent model learning its own representation of the users and the items in a lower latent dimensional space by factorizing the observed parts of the matrix. A factorized user or item is represented as a vector \\mathbf{v}_u \\mathbf{v}_u or \\mathbf{v}_i \\mathbf{v}_i composed of k k latent factors, with k << U, I k << U, I . Those learnt latent variables represent, for an item the various aspects describing it, and for a user its interests in terms of those aspects. The model then assume a user's choice or fondness is composed of a sum of preferences about the various aspects of the concerned item. This sum being the dot product between the latent vectors of a given user-item pair: \\normalsize \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f} \\normalsize \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f} MF models weights are learnt in an online fashion, often with stochastic gradient descent as it provides relatively fast running time and good accuracy. There is a great and widely popular library named surprise that implements MF models (and others) but in contrast with creme doesn't follow a pure online philosophy (all the data have to be loaded in memory and the API doesn't allow you to update your model with new data). Notes: In recent years, proposed deep learning techniques for recommendation tasks claim state of the art results. However, recent work (August 2019) showed that those promises can't be taken for granted and traditional MF methods are still relevant today. For more information about how the business value of recommender systems is measured and why they are one of the main success stories of machine learning, see the following literature survey (December 2019).","title":"Introduction"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#lets-start","text":"In this tutorial, we are going to explore MF algorithms available in creme and test them on a movie recommendation problem with the MovieLens 100K dataset. This latter is a collection of movie ratings (from 1 to 5) that includes various information about both the items and the users. We can access it from the creme.datasets module: import json from creme import datasets for x , y in datasets . MovieLens100K (): print ( f 'x = { json . dumps ( x , indent = 4 ) } \\n y = { y } ' ) break x = { \"user\": \"259\", \"item\": \"255\", \"timestamp\": 874731910000000000, \"title\": \"My Best Friend's Wedding (1997)\", \"release_date\": 866764800000000000, \"genres\": \"comedy, romance\", \"age\": 21.0, \"gender\": \"M\", \"occupation\": \"student\", \"zip_code\": \"48823\" } y = 4.0 Let's define a routine to evaluate our different models on MovieLens 100K. Mean Absolute Error and Root Mean Squared Error will be our metrics printed alongside model's computation time and memory usage: from creme import metrics from creme.evaluate import progressive_val_score def evaluate ( model ): X_y = datasets . MovieLens100K () metric = metrics . MAE () + metrics . RMSE () _ = progressive_val_score ( X_y , model , metric , print_every = 25_000 , show_time = True , show_memory = True )","title":"Let's start"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#naive-prediction","text":"It's good practice in machine learning to start with a naive baseline and then iterate from simple things to complex ones observing progress incrementally. Let's start by predicing the target running mean as a first shot: from creme import stats mean = stats . Mean () metric = metrics . MAE () + metrics . RMSE () for i , x_y in enumerate ( datasets . MovieLens100K (), start = 1 ): _ , y = x_y metric . update ( y , mean . get ()) mean . update ( y ) if not i % 25_000 : print ( f '[ { i : ,d } ] { metric } ' ) [25,000] MAE: 0.934259, RMSE: 1.124469 [50,000] MAE: 0.923893, RMSE: 1.105 [75,000] MAE: 0.937359, RMSE: 1.123696 [100,000] MAE: 0.942162, RMSE: 1.125783","title":"Naive prediction"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#baseline-model","text":"Now we can do machine learning and explore available models in creme.reco module starting with the baseline model. It extends our naive prediction by adding to the global running mean two bias terms characterizing the user and the item discrepancy from the general tendency. The model equation is defined as: \\normalsize \\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} \\normalsize \\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} This baseline model can be viewed as a linear regression where the intercept is replaced by the target running mean with the users and the items one hot encoded. All machine learning models in creme expect dicts as input with feature names as keys and feature values as values. Specifically, models from creme.reco expect a 'user' and an 'item' entries without any type constraint on their values (i.e. can be strings or numbers), e.g.: x = { 'user' : 'Guido' , 'item' : \"Monty Python's Flying Circus\" } Other entries, if exist, are simply ignored. This is quite usefull as we don't need to spend time and storage doing one hot encoding. from creme import meta from creme import optim from creme import reco baseline_params = { 'optimizer' : optim . SGD ( 0.025 ), 'l2' : 0. , 'initializer' : optim . initializers . Zeros () } model = meta . PredClipper ( regressor = reco . Baseline ( ** baseline_params ), y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761844, RMSE: 0.960972 \u2013 0:00:00 \u2013 170.01 KB [50,000] MAE: 0.753292, RMSE: 0.951223 \u2013 0:00:01 \u2013 238.65 KB [75,000] MAE: 0.754177, RMSE: 0.953376 \u2013 0:00:02 \u2013 282.46 KB [100,000] MAE: 0.754651, RMSE: 0.954148 \u2013 0:00:03 \u2013 306.06 KB We won two tenth of MAE compared to our naive prediction (0.7546 vs 0.9421) meaning that significant information has been learnt by the model.","title":"Baseline model"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#funk-matrix-factorization-funkmf","text":"It's the pure form of matrix factorization consisting of only learning the users and items latent representations as discussed in introduction. Simon Funk popularized its stochastic gradient descent optimization in 2006 during the Netflix Prize. The model equation is defined as: \\normalsize \\hat{y}(x) = \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle \\normalsize \\hat{y}(x) = \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle Note: FunkMF is sometimes refered as Probabilistic Matrix Factorization which is an extended probabilistic version. funk_mf_params = { 'n_factors' : 10 , 'optimizer' : optim . SGD ( 0.05 ), 'l2' : 0.1 , 'initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 73 ) } model = meta . PredClipper ( regressor = reco . FunkMF ( ** funk_mf_params ), y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 1.070136, RMSE: 1.397014 \u2013 0:00:01 \u2013 925.42 KB [50,000] MAE: 0.99174, RMSE: 1.290666 \u2013 0:00:02 \u2013 1.11 MB [75,000] MAE: 0.961072, RMSE: 1.250842 \u2013 0:00:04 \u2013 1.31 MB [100,000] MAE: 0.944883, RMSE: 1.227688 \u2013 0:00:05 \u2013 1.48 MB Results are equivalent to our naive prediction (0.9448 vs 0.9421). By only focusing on the users preferences and the items characteristics, the model is limited in his ability to capture different views of the problem. Despite its poor performance alone, this algorithm is quite usefull combined in other models or when we need to build dense representations for other tasks.","title":"Funk Matrix Factorization (FunkMF)"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#biased-matrix-factorization-biasedmf","text":"It's the combination of the Baseline model and FunkMF. The model equation is defined as: \\normalsize \\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} + \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle \\normalsize \\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} + \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle Note: Biased Matrix Factorization name is used by some people but some others refer to it by SVD or Funk SVD . It's the case of Yehuda Koren and Robert Bell in Recommender Systems Handbook (Chapter 5 Advances in Collaborative Filtering ) and of surprise library. Nevertheless, SVD could be confused with the original Singular Value Decomposition from which it's derived from, and Funk SVD could also be misleading because of the biased part of the model equation which doesn't come from Simon Funk's work. For those reasons, we chose to side with Biased Matrix Factorization which fits more naturally to it. biased_mf_params = { 'n_factors' : 10 , 'bias_optimizer' : optim . SGD ( 0.025 ), 'latent_optimizer' : optim . SGD ( 0.05 ), 'weight_initializer' : optim . initializers . Zeros (), 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 73 ), 'l2_bias' : 0. , 'l2_latent' : 0. } model = meta . PredClipper ( regressor = reco . BiasedMF ( ** biased_mf_params ), y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761818, RMSE: 0.961057 \u2013 0:00:01 \u2013 0.99 MB [50,000] MAE: 0.751667, RMSE: 0.949443 \u2013 0:00:03 \u2013 1.25 MB [75,000] MAE: 0.749653, RMSE: 0.948723 \u2013 0:00:05 \u2013 1.47 MB [100,000] MAE: 0.748559, RMSE: 0.947854 \u2013 0:00:06 \u2013 1.65 MB Results improved (0.7485 vs 0.7546) demonstrating that users and items latent representations bring additional information. To conclude this first tutorial about factorization models, let's review the important parameters to tune when dealing with this family of methods: n_factors : the number of latent factors. The more you set, the more items aspects and users preferences you are going to learn. Too many will cause overfitting, l2 regularization could help. *_optimizer : the optimizers. Classic stochastic gradient descent performs well, finding the good learning rate will make the difference. initializer : the latent weights initialization. Latent vectors have to be initialized with non-constant values. We generally sample them from a zero-mean normal distribution with small standard deviation.","title":"Biased Matrix Factorization (BiasedMF)"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/","text":"Matrix Factorization for Recommender Systems - Part 2 \u00b6 As seen in Part 1 , strength of Matrix Factorization (MF) lies in its ability to deal with sparse and high cardinality categorical variables. In this second tutorial we will have a look at Factorization Machines (FM) algorithm and study how it generalizes the power of MF. Table of contents of this tutorial series on matrix factorization for recommender systems: Part 1 - Traditional Matrix Factorization methods for Recommender Systems Part 2 - Factorization Machines and Field-aware Factorization Machines Part 3 - Large scale learning and better predictive power with multiple pass learning Factorization Machines \u00b6 Steffen Rendel came up in 2010 with Factorization Machines , an algorithm able to handle any real valued feature vector, combining the advantages of general predictors with factorization models. It became quite popular in the field of online advertising, notably after winning several Kaggle competitions. The modeling technique starts with a linear regression to capture the effects of each variable individually: \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} Then are added interaction terms to learn features relations. Instead of learning a single and specific weight per interaction (as in polynomial regression ), a set of latent factors is learnt per feature (as in MF). An interaction is calculated by multiplying involved features product with their latent vectors dot product. The degree of factorization \u2014 or model order \u2014 represents the maximum number of features per interaction considered. The model equation for a factorization machine of degree d d = 2 is defined as: \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} Where \\normalsize \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle \\normalsize \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle is the dot product of j j and j' j' latent vectors: \\normalsize \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{j, f} \\cdot \\mathbf{v}_{j', f} \\normalsize \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{j, f} \\cdot \\mathbf{v}_{j', f} Higher-order FM will be covered in a following section, just note that factorization models express their power in sparse settings, which is also where higher-order interactions are hard to estimate. Strong emphasis must be placed on feature engineering as it allows FM to mimic most factorization models and significantly impact its performance. High cardinality categorical variables one hot encoding is the most frequent step before feeding the model with data. For more efficiency, creme FM implementation considers string values as categorical variables and automatically one hot encode them. FM models have their own module creme.facto . ## Mimic Biased Matrix Factorization (BiasedMF) Let's start with a simple example where we want to reproduce the Biased Matrix Factorization model we trained in the previous tutorial. For a fair comparison with Part 1 example , let's set the same evaluation framework: from creme import datasets from creme import metrics from creme.evaluate import progressive_val_score def evaluate ( model ): X_y = datasets . MovieLens100K () metric = metrics . MAE () + metrics . RMSE () _ = progressive_val_score ( X_y , model , metric , print_every = 25_000 , show_time = True , show_memory = True ) In order to build an equivalent model we need to use the same hyper-parameters. As we can't replace FM intercept by the global running mean we won't be able to build the exact same model: from creme import compose from creme import facto from creme import meta from creme import optim from creme import stats fm_params = { 'n_factors' : 10 , 'weight_optimizer' : optim . SGD ( 0.025 ), 'latent_optimizer' : optim . SGD ( 0.05 ), 'sample_normalization' : False , 'l1_weight' : 0. , 'l2_weight' : 0. , 'l1_latent' : 0. , 'l2_latent' : 0. , 'intercept' : 3 , 'intercept_lr' : . 01 , 'weight_initializer' : optim . initializers . Zeros (), 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor |= facto . FMRegressor ( ** fm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761778, RMSE: 0.960803 \u2013 0:00:04 \u2013 1.14 MB [50,000] MAE: 0.751986, RMSE: 0.949941 \u2013 0:00:08 \u2013 1.33 MB [75,000] MAE: 0.750044, RMSE: 0.948911 \u2013 0:00:12 \u2013 1.54 MB [100,000] MAE: 0.748609, RMSE: 0.947994 \u2013 0:00:16 \u2013 1.73 MB Both MAE are very close to each other (0.7486 vs 0.7485) showing that we almost reproduced reco.BiasedMF algorithm. The cost is a naturally slower running time as FM implementation offers more flexibility. Feature engineering for FM models \u00b6 Let's study the basics of how to properly encode data for FM models. We are going to keep using MovieLens 100K as it provides various feature types: import json for x , y in datasets . MovieLens100K (): print ( f 'x = { json . dumps ( x , indent = 4 ) } \\n y = { y } ' ) break x = { \"user\": \"259\", \"item\": \"255\", \"timestamp\": 874731910000000000, \"title\": \"My Best Friend's Wedding (1997)\", \"release_date\": 866764800000000000, \"genres\": \"comedy, romance\", \"age\": 21.0, \"gender\": \"M\", \"occupation\": \"student\", \"zip_code\": \"48823\" } y = 4.0 The features we are going to add to our model don't improve its predictive power. Nevertheless, they are useful to illustrate different methods of data encoding: Set-categorical variables We have seen that categorical variables are one hot encoded automatically if set to strings, in the other hand, set-categorical variables must be encoded explicitly by the user. A good way of doing so is to assign them a value of 1/m 1/m , where m m is the number of elements of the sample set. It gives the feature a constant \"weight\" across all samples preserving model's stability. Let's create a routine to encode movies genres this way: def split_genres ( x ): genres = x [ 'genres' ] . split ( ', ' ) return { f 'genre_ { genre } ' : 1 / len ( genres ) for genre in genres } Numerical variables In practice, transforming numerical features into categorical ones works better in most cases. Feature binning is the natural way, but finding good bins is sometimes more an art than a science. Let's encode users age with something simple: def bin_age ( x ): if x [ 'age' ] <= 18 : return { 'age_0-18' : 1 } elif x [ 'age' ] <= 32 : return { 'age_19-32' : 1 } elif x [ 'age' ] < 55 : return { 'age_33-54' : 1 } else : return { 'age_55-100' : 1 } Let's put everything together: fm_params = { 'n_factors' : 14 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.05 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . FMRegressor ( ** fm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.759838, RMSE: 0.961281 \u2013 0:00:10 \u2013 1.41 MB [50,000] MAE: 0.751307, RMSE: 0.951391 \u2013 0:00:22 \u2013 1.65 MB [75,000] MAE: 0.750361, RMSE: 0.951393 \u2013 0:00:33 \u2013 1.91 MB [100,000] MAE: 0.749994, RMSE: 0.951435 \u2013 0:00:44 \u2013 2.16 MB Note that using more variables involves factorizing a larger latent space, then increasing the number of latent factors k k often helps capturing more information. Some other feature engineering tips from 3 idiots' winning solution for Kaggle Criteo display ads competition in 2014: Infrequent modalities often bring noise and little information, transforming them into a special tag can help In some cases, sample-wise normalization seems to make the optimization problem easier to be solved Higher-Order Factorization Machines (HOFM) \u00b6 The model equation generalized to any order d \\geq 2 d \\geq 2 is defined as: \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right) \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right) hofm_params = { 'degree' : 3 , 'n_factors' : 12 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.05 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . HOFMRegressor ( ** hofm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761297, RMSE: 0.962054 \u2013 0:00:55 \u2013 2.6 MB [50,000] MAE: 0.751865, RMSE: 0.951499 \u2013 0:01:51 \u2013 3.07 MB [75,000] MAE: 0.750853, RMSE: 0.951526 \u2013 0:02:46 \u2013 3.58 MB [100,000] MAE: 0.750607, RMSE: 0.951982 \u2013 0:03:45 \u2013 4.06 MB As said previously, high-order interactions are often hard to estimate due to too much sparsity, that's why we won't spend too much time here. Field-aware Factorization Machines (FFM) \u00b6 Field-aware variant of FM (FFM) improved the original method by adding the notion of \" fields \". A \" field \" is a group of features that belong to a specific domain (e.g. the \" users \" field, the \" items \" field, or the \" movie genres \" field). FFM restricts itself to pairwise interactions and factorizes separated latent spaces \u2014 one per combination of fields (e.g. users/items, users/movie genres, or items/movie genres) \u2014 instead of a common one shared by all fields. Therefore, each feature has one latent vector per field it can interact with \u2014 so that it can learn the specific effect with each different field. The model equation is defined by: \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_{j}} \\rangle x_{j} x_{j'} \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_{j}} \\rangle x_{j} x_{j'} Where f_j f_j and f_{j'} f_{j'} are the fields corresponding to j j and j' j' features, respectively. ffm_params = { 'n_factors' : 8 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.05 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . FFMRegressor ( ** ffm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.757718, RMSE: 0.958158 \u2013 0:00:17 \u2013 3.01 MB [50,000] MAE: 0.749502, RMSE: 0.948065 \u2013 0:00:33 \u2013 3.56 MB [75,000] MAE: 0.749275, RMSE: 0.948918 \u2013 0:00:49 \u2013 4.16 MB [100,000] MAE: 0.749542, RMSE: 0.949769 \u2013 0:01:06 \u2013 4.7 MB Note that FFM usually needs to learn smaller number of latent factors k k than FM as each latent vector only deals with one field. Field-weighted Factorization Machines (FwFM) \u00b6 Field-weighted Factorization Machines (FwFM) address FFM memory issues caused by its large number of parameters, which is in the order of feature number times field number . As FFM, FwFM is an extension of FM restricted to pairwise interactions, but instead of factorizing separated latent spaces, it learns a specific weight r_{f_j, f_{j'}} r_{f_j, f_{j'}} for each field combination modelling the interaction strength. The model equation is defined as: \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} r_{f_j, f_{j'}} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} r_{f_j, f_{j'}} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} fwfm_params = { 'n_factors' : 10 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'seed' : 73 , } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . FwFMRegressor ( ** fwfm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761539, RMSE: 0.962241 \u2013 0:00:19 \u2013 1.16 MB [50,000] MAE: 0.754089, RMSE: 0.953181 \u2013 0:00:40 \u2013 1.35 MB [75,000] MAE: 0.754806, RMSE: 0.954979 \u2013 0:01:00 \u2013 1.56 MB [100,000] MAE: 0.755404, RMSE: 0.95604 \u2013 0:01:20 \u2013 1.75 MB","title":"Matrix Factorization for Recommender Systems - Part 2"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/#matrix-factorization-for-recommender-systems-part-2","text":"As seen in Part 1 , strength of Matrix Factorization (MF) lies in its ability to deal with sparse and high cardinality categorical variables. In this second tutorial we will have a look at Factorization Machines (FM) algorithm and study how it generalizes the power of MF. Table of contents of this tutorial series on matrix factorization for recommender systems: Part 1 - Traditional Matrix Factorization methods for Recommender Systems Part 2 - Factorization Machines and Field-aware Factorization Machines Part 3 - Large scale learning and better predictive power with multiple pass learning","title":"Matrix Factorization for Recommender Systems - Part 2"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/#factorization-machines","text":"Steffen Rendel came up in 2010 with Factorization Machines , an algorithm able to handle any real valued feature vector, combining the advantages of general predictors with factorization models. It became quite popular in the field of online advertising, notably after winning several Kaggle competitions. The modeling technique starts with a linear regression to capture the effects of each variable individually: \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} Then are added interaction terms to learn features relations. Instead of learning a single and specific weight per interaction (as in polynomial regression ), a set of latent factors is learnt per feature (as in MF). An interaction is calculated by multiplying involved features product with their latent vectors dot product. The degree of factorization \u2014 or model order \u2014 represents the maximum number of features per interaction considered. The model equation for a factorization machine of degree d d = 2 is defined as: \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} Where \\normalsize \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle \\normalsize \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle is the dot product of j j and j' j' latent vectors: \\normalsize \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{j, f} \\cdot \\mathbf{v}_{j', f} \\normalsize \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{j, f} \\cdot \\mathbf{v}_{j', f} Higher-order FM will be covered in a following section, just note that factorization models express their power in sparse settings, which is also where higher-order interactions are hard to estimate. Strong emphasis must be placed on feature engineering as it allows FM to mimic most factorization models and significantly impact its performance. High cardinality categorical variables one hot encoding is the most frequent step before feeding the model with data. For more efficiency, creme FM implementation considers string values as categorical variables and automatically one hot encode them. FM models have their own module creme.facto . ## Mimic Biased Matrix Factorization (BiasedMF) Let's start with a simple example where we want to reproduce the Biased Matrix Factorization model we trained in the previous tutorial. For a fair comparison with Part 1 example , let's set the same evaluation framework: from creme import datasets from creme import metrics from creme.evaluate import progressive_val_score def evaluate ( model ): X_y = datasets . MovieLens100K () metric = metrics . MAE () + metrics . RMSE () _ = progressive_val_score ( X_y , model , metric , print_every = 25_000 , show_time = True , show_memory = True ) In order to build an equivalent model we need to use the same hyper-parameters. As we can't replace FM intercept by the global running mean we won't be able to build the exact same model: from creme import compose from creme import facto from creme import meta from creme import optim from creme import stats fm_params = { 'n_factors' : 10 , 'weight_optimizer' : optim . SGD ( 0.025 ), 'latent_optimizer' : optim . SGD ( 0.05 ), 'sample_normalization' : False , 'l1_weight' : 0. , 'l2_weight' : 0. , 'l1_latent' : 0. , 'l2_latent' : 0. , 'intercept' : 3 , 'intercept_lr' : . 01 , 'weight_initializer' : optim . initializers . Zeros (), 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor |= facto . FMRegressor ( ** fm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761778, RMSE: 0.960803 \u2013 0:00:04 \u2013 1.14 MB [50,000] MAE: 0.751986, RMSE: 0.949941 \u2013 0:00:08 \u2013 1.33 MB [75,000] MAE: 0.750044, RMSE: 0.948911 \u2013 0:00:12 \u2013 1.54 MB [100,000] MAE: 0.748609, RMSE: 0.947994 \u2013 0:00:16 \u2013 1.73 MB Both MAE are very close to each other (0.7486 vs 0.7485) showing that we almost reproduced reco.BiasedMF algorithm. The cost is a naturally slower running time as FM implementation offers more flexibility.","title":"Factorization Machines"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/#feature-engineering-for-fm-models","text":"Let's study the basics of how to properly encode data for FM models. We are going to keep using MovieLens 100K as it provides various feature types: import json for x , y in datasets . MovieLens100K (): print ( f 'x = { json . dumps ( x , indent = 4 ) } \\n y = { y } ' ) break x = { \"user\": \"259\", \"item\": \"255\", \"timestamp\": 874731910000000000, \"title\": \"My Best Friend's Wedding (1997)\", \"release_date\": 866764800000000000, \"genres\": \"comedy, romance\", \"age\": 21.0, \"gender\": \"M\", \"occupation\": \"student\", \"zip_code\": \"48823\" } y = 4.0 The features we are going to add to our model don't improve its predictive power. Nevertheless, they are useful to illustrate different methods of data encoding: Set-categorical variables We have seen that categorical variables are one hot encoded automatically if set to strings, in the other hand, set-categorical variables must be encoded explicitly by the user. A good way of doing so is to assign them a value of 1/m 1/m , where m m is the number of elements of the sample set. It gives the feature a constant \"weight\" across all samples preserving model's stability. Let's create a routine to encode movies genres this way: def split_genres ( x ): genres = x [ 'genres' ] . split ( ', ' ) return { f 'genre_ { genre } ' : 1 / len ( genres ) for genre in genres } Numerical variables In practice, transforming numerical features into categorical ones works better in most cases. Feature binning is the natural way, but finding good bins is sometimes more an art than a science. Let's encode users age with something simple: def bin_age ( x ): if x [ 'age' ] <= 18 : return { 'age_0-18' : 1 } elif x [ 'age' ] <= 32 : return { 'age_19-32' : 1 } elif x [ 'age' ] < 55 : return { 'age_33-54' : 1 } else : return { 'age_55-100' : 1 } Let's put everything together: fm_params = { 'n_factors' : 14 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.05 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . FMRegressor ( ** fm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.759838, RMSE: 0.961281 \u2013 0:00:10 \u2013 1.41 MB [50,000] MAE: 0.751307, RMSE: 0.951391 \u2013 0:00:22 \u2013 1.65 MB [75,000] MAE: 0.750361, RMSE: 0.951393 \u2013 0:00:33 \u2013 1.91 MB [100,000] MAE: 0.749994, RMSE: 0.951435 \u2013 0:00:44 \u2013 2.16 MB Note that using more variables involves factorizing a larger latent space, then increasing the number of latent factors k k often helps capturing more information. Some other feature engineering tips from 3 idiots' winning solution for Kaggle Criteo display ads competition in 2014: Infrequent modalities often bring noise and little information, transforming them into a special tag can help In some cases, sample-wise normalization seems to make the optimization problem easier to be solved","title":"Feature engineering for FM models"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/#higher-order-factorization-machines-hofm","text":"The model equation generalized to any order d \\geq 2 d \\geq 2 is defined as: \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right) \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right) hofm_params = { 'degree' : 3 , 'n_factors' : 12 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.05 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . HOFMRegressor ( ** hofm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761297, RMSE: 0.962054 \u2013 0:00:55 \u2013 2.6 MB [50,000] MAE: 0.751865, RMSE: 0.951499 \u2013 0:01:51 \u2013 3.07 MB [75,000] MAE: 0.750853, RMSE: 0.951526 \u2013 0:02:46 \u2013 3.58 MB [100,000] MAE: 0.750607, RMSE: 0.951982 \u2013 0:03:45 \u2013 4.06 MB As said previously, high-order interactions are often hard to estimate due to too much sparsity, that's why we won't spend too much time here.","title":"Higher-Order Factorization Machines (HOFM)"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/#field-aware-factorization-machines-ffm","text":"Field-aware variant of FM (FFM) improved the original method by adding the notion of \" fields \". A \" field \" is a group of features that belong to a specific domain (e.g. the \" users \" field, the \" items \" field, or the \" movie genres \" field). FFM restricts itself to pairwise interactions and factorizes separated latent spaces \u2014 one per combination of fields (e.g. users/items, users/movie genres, or items/movie genres) \u2014 instead of a common one shared by all fields. Therefore, each feature has one latent vector per field it can interact with \u2014 so that it can learn the specific effect with each different field. The model equation is defined by: \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_{j}} \\rangle x_{j} x_{j'} \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_{j}} \\rangle x_{j} x_{j'} Where f_j f_j and f_{j'} f_{j'} are the fields corresponding to j j and j' j' features, respectively. ffm_params = { 'n_factors' : 8 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.05 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . FFMRegressor ( ** ffm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.757718, RMSE: 0.958158 \u2013 0:00:17 \u2013 3.01 MB [50,000] MAE: 0.749502, RMSE: 0.948065 \u2013 0:00:33 \u2013 3.56 MB [75,000] MAE: 0.749275, RMSE: 0.948918 \u2013 0:00:49 \u2013 4.16 MB [100,000] MAE: 0.749542, RMSE: 0.949769 \u2013 0:01:06 \u2013 4.7 MB Note that FFM usually needs to learn smaller number of latent factors k k than FM as each latent vector only deals with one field.","title":"Field-aware Factorization Machines (FFM)"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/#field-weighted-factorization-machines-fwfm","text":"Field-weighted Factorization Machines (FwFM) address FFM memory issues caused by its large number of parameters, which is in the order of feature number times field number . As FFM, FwFM is an extension of FM restricted to pairwise interactions, but instead of factorizing separated latent spaces, it learns a specific weight r_{f_j, f_{j'}} r_{f_j, f_{j'}} for each field combination modelling the interaction strength. The model equation is defined as: \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} r_{f_j, f_{j'}} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} r_{f_j, f_{j'}} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} fwfm_params = { 'n_factors' : 10 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'seed' : 73 , } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . FwFMRegressor ( ** fwfm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761539, RMSE: 0.962241 \u2013 0:00:19 \u2013 1.16 MB [50,000] MAE: 0.754089, RMSE: 0.953181 \u2013 0:00:40 \u2013 1.35 MB [75,000] MAE: 0.754806, RMSE: 0.954979 \u2013 0:01:00 \u2013 1.56 MB [100,000] MAE: 0.755404, RMSE: 0.95604 \u2013 0:01:20 \u2013 1.75 MB","title":"Field-weighted Factorization Machines (FwFM)"},{"location":"examples/matrix-factorization-for-recommender-systems-part-3/","text":"Matrix Factorization for Recommender Systems - Part 3 \u00b6 Table of contents of this tutorial series on matrix factorization for recommender systems: Part 1 - Traditional Matrix Factorization methods for Recommender Systems Part 2 - Factorization Machines and Field-aware Factorization Machines Part 3 - Large scale learning and better predictive power with multiple pass learning To do.","title":"Matrix Factorization for Recommender Systems - Part 3"},{"location":"examples/matrix-factorization-for-recommender-systems-part-3/#matrix-factorization-for-recommender-systems-part-3","text":"Table of contents of this tutorial series on matrix factorization for recommender systems: Part 1 - Traditional Matrix Factorization methods for Recommender Systems Part 2 - Factorization Machines and Field-aware Factorization Machines Part 3 - Large scale learning and better predictive power with multiple pass learning To do.","title":"Matrix Factorization for Recommender Systems - Part 3"},{"location":"examples/quantile-regression-uncertainty/","text":"Handling uncertainty with quantile regression \u00b6 % matplotlib inline Quantile regression is useful when you're not so much interested in the accuracy of your model, but rather you want your model to be good at ranking observations correctly. The typical way to perform quantile regression is to use a special loss function, namely the quantile loss. The quantile loss takes a parameter, \\alpha \\alpha (alpha), which indicates which quantile the model should be targeting. In the case of \\alpha = 0.5 \\alpha = 0.5 , then this is equivalent to asking the model to predict the median value of the target, and not the most likely value which would be the mean. A nice thing we can do with quantile regression is to produce a prediction interval for each prediction. Indeed, if we predict the lower and upper quantiles of the target then we will be able to obtain a \"trust region\" in between which the true value is likely to belong. Of course, the likeliness will depend on the chosen quantiles. For a slightly more detailed explanation see this blog post. As an example, let us take the simple time series model we built in another notebook . Instead of predicting the mean value of the target distribution, we will predict the 5th, 50th, 95th quantiles. This will require training three separate models, so we will encapsulate the model building logic in a function called make_model . We also have to slightly adapt the training loop, but not by much. Finally, we will draw the prediction interval along with the predictions from for 50th quantile (i.e. the median) and the true values. import calendar import math import matplotlib.pyplot as plt from creme import compose from creme import datasets from creme import linear_model from creme import metrics from creme import optim from creme import preprocessing from creme import stats from creme import time_series def get_ordinal_date ( x ): return { 'ordinal_date' : x [ 'month' ] . toordinal ()} def get_month_distances ( x ): return { calendar . month_name [ month ]: math . exp ( - ( x [ 'month' ] . month - month ) ** 2 ) for month in range ( 1 , 13 ) } def make_model ( alpha ): extract_features = compose . TransformerUnion ( get_ordinal_date , get_month_distances ) scale = preprocessing . StandardScaler () learn = linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 3 ), loss = optim . losses . Quantile ( alpha = alpha ) ) model = extract_features | scale | learn model = time_series . Detrender ( regressor = model , window_size = 12 ) return model metric = metrics . MAE () models = { 'lower' : make_model ( alpha = 0.05 ), 'center' : make_model ( alpha = 0.5 ), 'upper' : make_model ( alpha = 0.95 ) } dates = [] y_trues = [] y_preds = { 'lower' : [], 'center' : [], 'upper' : [] } for x , y in datasets . AirlinePassengers (): y_trues . append ( y ) dates . append ( x [ 'month' ]) for name , model in models . items (): y_preds [ name ] . append ( model . predict_one ( x )) model . learn_one ( x , y ) # Update the error metric metric . update ( y , y_preds [ 'center' ][ - 1 ]) # Plot the results fig , ax = plt . subplots ( figsize = ( 10 , 6 )) ax . grid ( alpha = 0.75 ) ax . plot ( dates , y_trues , lw = 3 , color = '#2ecc71' , alpha = 0.8 , label = 'Truth' ) ax . plot ( dates , y_preds [ 'center' ], lw = 3 , color = '#e74c3c' , alpha = 0.8 , label = 'Prediction' ) ax . fill_between ( dates , y_preds [ 'lower' ], y_preds [ 'upper' ], color = '#e74c3c' , alpha = 0.3 , label = 'Prediction interval' ) ax . legend () ax . set_title ( metric ); An important thing to note is that the prediction interval we obtained should not be confused with a confidence interval. Simply put, a prediction interval represents uncertainty for where the true value lies, whereas a confidence interval encapsulates the uncertainty on the prediction. You can find out more by reading this CrossValidated post.","title":"Handling uncertainty with quantile regression"},{"location":"examples/quantile-regression-uncertainty/#handling-uncertainty-with-quantile-regression","text":"% matplotlib inline Quantile regression is useful when you're not so much interested in the accuracy of your model, but rather you want your model to be good at ranking observations correctly. The typical way to perform quantile regression is to use a special loss function, namely the quantile loss. The quantile loss takes a parameter, \\alpha \\alpha (alpha), which indicates which quantile the model should be targeting. In the case of \\alpha = 0.5 \\alpha = 0.5 , then this is equivalent to asking the model to predict the median value of the target, and not the most likely value which would be the mean. A nice thing we can do with quantile regression is to produce a prediction interval for each prediction. Indeed, if we predict the lower and upper quantiles of the target then we will be able to obtain a \"trust region\" in between which the true value is likely to belong. Of course, the likeliness will depend on the chosen quantiles. For a slightly more detailed explanation see this blog post. As an example, let us take the simple time series model we built in another notebook . Instead of predicting the mean value of the target distribution, we will predict the 5th, 50th, 95th quantiles. This will require training three separate models, so we will encapsulate the model building logic in a function called make_model . We also have to slightly adapt the training loop, but not by much. Finally, we will draw the prediction interval along with the predictions from for 50th quantile (i.e. the median) and the true values. import calendar import math import matplotlib.pyplot as plt from creme import compose from creme import datasets from creme import linear_model from creme import metrics from creme import optim from creme import preprocessing from creme import stats from creme import time_series def get_ordinal_date ( x ): return { 'ordinal_date' : x [ 'month' ] . toordinal ()} def get_month_distances ( x ): return { calendar . month_name [ month ]: math . exp ( - ( x [ 'month' ] . month - month ) ** 2 ) for month in range ( 1 , 13 ) } def make_model ( alpha ): extract_features = compose . TransformerUnion ( get_ordinal_date , get_month_distances ) scale = preprocessing . StandardScaler () learn = linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 3 ), loss = optim . losses . Quantile ( alpha = alpha ) ) model = extract_features | scale | learn model = time_series . Detrender ( regressor = model , window_size = 12 ) return model metric = metrics . MAE () models = { 'lower' : make_model ( alpha = 0.05 ), 'center' : make_model ( alpha = 0.5 ), 'upper' : make_model ( alpha = 0.95 ) } dates = [] y_trues = [] y_preds = { 'lower' : [], 'center' : [], 'upper' : [] } for x , y in datasets . AirlinePassengers (): y_trues . append ( y ) dates . append ( x [ 'month' ]) for name , model in models . items (): y_preds [ name ] . append ( model . predict_one ( x )) model . learn_one ( x , y ) # Update the error metric metric . update ( y , y_preds [ 'center' ][ - 1 ]) # Plot the results fig , ax = plt . subplots ( figsize = ( 10 , 6 )) ax . grid ( alpha = 0.75 ) ax . plot ( dates , y_trues , lw = 3 , color = '#2ecc71' , alpha = 0.8 , label = 'Truth' ) ax . plot ( dates , y_preds [ 'center' ], lw = 3 , color = '#e74c3c' , alpha = 0.8 , label = 'Prediction' ) ax . fill_between ( dates , y_preds [ 'lower' ], y_preds [ 'upper' ], color = '#e74c3c' , alpha = 0.3 , label = 'Prediction interval' ) ax . legend () ax . set_title ( metric ); An important thing to note is that the prediction interval we obtained should not be confused with a confidence interval. Simply put, a prediction interval represents uncertainty for where the true value lies, whereas a confidence interval encapsulates the uncertainty on the prediction. You can find out more by reading this CrossValidated post.","title":"Handling uncertainty with quantile regression"},{"location":"examples/the-art-of-using-pipelines/","text":"The art of using pipelines \u00b6 Pipelines are a natural way to think about a machine learning system. Indeed with some practice a data scientist can visualise data \"flowing\" through a series of steps. The input is typically some raw data which has to be processed in some manner. The goal is to represent the data in such a way that is can be ingested by a machine learning algorithm. Along the way some steps will extract features, while others will normalize the data and remove undesirable elements. Pipelines are simple, and yet they are a powerful way of designing sophisticated machine learning systems. Both scikit-learn and pandas make it possible to use pipelines. However it's quite rare to see pipelines being used in practice (at least on Kaggle). Sometimes you get to see people using scikit-learn's pipeline module, however the pipe method from pandas is sadly underappreciated. A big reason why pipelines are not given much love is that it's easier to think of batch learning in terms of a script or a notebook. Indeed many people doing data science seem to prefer a procedural style to a declarative style. Moreover in practice pipelines can be a bit rigid if one wishes to do non-orthodox operations. Although pipelines may be a bit of an odd fit for batch learning, they make complete sense when they are used for online learning. Indeed the UNIX philosophy has advocated the use of pipelines for data processing for many decades. If you can visualise data as a stream of observations then using pipelines should make a lot of sense to you. We'll attempt to convince you by writing a machine learning algorithm in a procedural way and then converting it to a declarative pipeline in small steps. Hopefully by the end you'll be convinced, or not! In this notebook we'll manipulate data from the Kaggle Recruit Restaurants Visitor Forecasting competition . The data is directly available through creme 's datasets module. from pprint import pprint from creme import datasets for x , y in datasets . Restaurants (): pprint ( x ) pprint ( y ) break {'area_name': 'T\u014dky\u014d-to Nerima-ku Toyotamakita', 'date': datetime.datetime(2016, 1, 1, 0, 0), 'genre_name': 'Izakaya', 'is_holiday': True, 'latitude': 35.7356234, 'longitude': 139.6516577, 'store_id': 'air_04341b588bde96cd'} 10 We'll start by building and running a model using a procedural coding style. The performance of the model doesn't matter, we're simply interested in the design of the model. from creme import feature_extraction from creme import linear_model from creme import metrics from creme import preprocessing from creme import stats means = ( feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )) ) scaler = preprocessing . StandardScaler () lin_reg = linear_model . LinearRegression () metric = metrics . MAE () for x , y in datasets . Restaurants (): # Derive date features x [ 'weekday' ] = x [ 'date' ] . weekday () x [ 'is_weekend' ] = x [ 'date' ] . weekday () in ( 5 , 6 ) # Process the rolling means of the target for mean in means : x = { ** x , ** mean . transform_one ( x )} mean . learn_one ( x , y ) # Remove the key/value pairs that aren't features for key in [ 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ]: x . pop ( key ) # Rescale the data x = scaler . learn_one ( x ) . transform_one ( x ) # Fit the linear regression y_pred = lin_reg . predict_one ( x ) lin_reg . learn_one ( x , y ) # Update the metric using the out-of-fold prediction metric . update ( y , y_pred ) print ( metric ) MAE: 8.465114 We're not using many features. We can print the last x to get an idea of the features (don't forget they've been scaled!) pprint ( x ) {'is_holiday': -0.23103573677646685, 'is_weekend': 1.6249280076334165, 'target_rollingmean_14_by_store_id': -1.4125913815779154, 'target_rollingmean_21_by_store_id': -1.3980979075298519, 'target_rollingmean_7_by_store_id': -1.3502314499809096, 'weekday': 1.0292832579142892} The above chunk of code is quite explicit but it's a bit verbose. The whole point of libraries such as creme is to make life easier for users. Moreover there's too much space for users to mess up the order in which things are done, which increases the chance of there being target leakage. We'll now rewrite our model in a declarative fashion using a pipeline \u00e0 la sklearn . from creme import compose def get_date_features ( x ): weekday = x [ 'date' ] . weekday () return { 'weekday' : weekday , 'is_weekend' : weekday in ( 5 , 6 )} model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'date_features' , compose . FuncTransformer ( get_date_features )), ( 'last_7_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 ))), ( 'last_14_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 ))), ( 'last_21_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 ))) )), ( 'drop_non_features' , compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ()) ) metric = metrics . MAE () for x , y in datasets . Restaurants (): # Make a prediction without using the target y_pred = model . predict_one ( x ) # Update the model using the target model . learn_one ( x , y ) # Update the metric using the out-of-fold prediction metric . update ( y , y_pred ) print ( metric ) MAE: 8.38533 We use a Pipeline to arrange each step in a sequential order. A TransformerUnion is used to merge multiple feature extractors into a single transformer. The for loop is now much shorter and is thus easier to grok: we get the out-of-fold prediction, we fit the model, and finally we update the metric. This way of evaluating a model is typical of online learning, and so we put it wrapped it inside a function called online_score part of the model_selection module. We can use it to replace the for loop. from creme import model_selection model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'date_features' , compose . FuncTransformer ( get_date_features )), ( 'last_7_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 ))), ( 'last_14_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 ))), ( 'last_21_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 ))) )), ( 'drop_non_features' , compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ()) ) model_selection . progressive_val_score ( X_y = datasets . Restaurants (), model = model , metric = metrics . MAE ()) MAE: 8.38533 Notice that you couldn't have used the online_score method if you wrote the model in a procedural manner. Our code is getting shorter, but it's still a bit difficult on the eyes. Indeed there is a lot of boilerplate code associated with pipelines that can get tedious to write. However creme has some special tricks up it's sleeve to save you from a lot of pain. The first trick is that the name of each step in the pipeline can be omitted. If no name is given for a step then creme automatically infers one. model = compose . Pipeline ( compose . TransformerUnion ( compose . FuncTransformer ( get_date_features ), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )) ), compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ), preprocessing . StandardScaler (), linear_model . LinearRegression () ) model_selection . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Under the hood a Pipeline inherits from collections.OrderedDict . Indeed this makes sense because if you think about it a Pipeline is simply a sequence of steps where each step has a name. The reason we mention this is because it means you can manipulate a Pipeline the same way you would manipulate an ordinary dict . For instance we can print the name of each step by using the keys method. for name in model . steps : print ( name ) TransformerUnion Discard StandardScaler LinearRegression The first step is a FeatureUnion and it's string representation contains the string representation of each of it's elements. Not having to write names saves up some time and space and is certainly less tedious. The next trick is that we can use mathematical operators to compose our pipeline. For example we can use the + operator to merge Transformer s into a TransformerUnion . model = compose . Pipeline ( compose . FuncTransformer ( get_date_features ) + \\ feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )) + \\ feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )) + \\ feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )), compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ), preprocessing . StandardScaler (), linear_model . LinearRegression () ) model_selection . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Likewhise we can use the | operator to assemble steps into a Pipeline . model = ( compose . FuncTransformer ( get_date_features ) + feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )) + feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )) + feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )) ) to_discard = [ 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ] model = model | compose . Discard ( * to_discard ) | preprocessing . StandardScaler () model |= linear_model . LinearRegression () model_selection . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Hopefully you'll agree that this is a powerful way to express machine learning pipelines. For some people this should be quite remeniscent of the UNIX pipe operator. One final trick we want to mention is that functions are automatically wrapped with a FuncTransformer , which can be quite handy. model = get_date_features for n in [ 7 , 14 , 21 ]: model += feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( n )) model |= compose . Discard ( * to_discard ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () model_selection . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Naturally some may prefer the procedural style we first used because they find it easier to work with. It all depends on your style and you should use what you feel comfortable with. However we encourage you to use operators because we believe that this will increase the readability of your code, which is very important. To each their own! Before finishing we can take a look at what our pipeline looks graphically. model . draw ()","title":"The art of using pipelines"},{"location":"examples/the-art-of-using-pipelines/#the-art-of-using-pipelines","text":"Pipelines are a natural way to think about a machine learning system. Indeed with some practice a data scientist can visualise data \"flowing\" through a series of steps. The input is typically some raw data which has to be processed in some manner. The goal is to represent the data in such a way that is can be ingested by a machine learning algorithm. Along the way some steps will extract features, while others will normalize the data and remove undesirable elements. Pipelines are simple, and yet they are a powerful way of designing sophisticated machine learning systems. Both scikit-learn and pandas make it possible to use pipelines. However it's quite rare to see pipelines being used in practice (at least on Kaggle). Sometimes you get to see people using scikit-learn's pipeline module, however the pipe method from pandas is sadly underappreciated. A big reason why pipelines are not given much love is that it's easier to think of batch learning in terms of a script or a notebook. Indeed many people doing data science seem to prefer a procedural style to a declarative style. Moreover in practice pipelines can be a bit rigid if one wishes to do non-orthodox operations. Although pipelines may be a bit of an odd fit for batch learning, they make complete sense when they are used for online learning. Indeed the UNIX philosophy has advocated the use of pipelines for data processing for many decades. If you can visualise data as a stream of observations then using pipelines should make a lot of sense to you. We'll attempt to convince you by writing a machine learning algorithm in a procedural way and then converting it to a declarative pipeline in small steps. Hopefully by the end you'll be convinced, or not! In this notebook we'll manipulate data from the Kaggle Recruit Restaurants Visitor Forecasting competition . The data is directly available through creme 's datasets module. from pprint import pprint from creme import datasets for x , y in datasets . Restaurants (): pprint ( x ) pprint ( y ) break {'area_name': 'T\u014dky\u014d-to Nerima-ku Toyotamakita', 'date': datetime.datetime(2016, 1, 1, 0, 0), 'genre_name': 'Izakaya', 'is_holiday': True, 'latitude': 35.7356234, 'longitude': 139.6516577, 'store_id': 'air_04341b588bde96cd'} 10 We'll start by building and running a model using a procedural coding style. The performance of the model doesn't matter, we're simply interested in the design of the model. from creme import feature_extraction from creme import linear_model from creme import metrics from creme import preprocessing from creme import stats means = ( feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )) ) scaler = preprocessing . StandardScaler () lin_reg = linear_model . LinearRegression () metric = metrics . MAE () for x , y in datasets . Restaurants (): # Derive date features x [ 'weekday' ] = x [ 'date' ] . weekday () x [ 'is_weekend' ] = x [ 'date' ] . weekday () in ( 5 , 6 ) # Process the rolling means of the target for mean in means : x = { ** x , ** mean . transform_one ( x )} mean . learn_one ( x , y ) # Remove the key/value pairs that aren't features for key in [ 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ]: x . pop ( key ) # Rescale the data x = scaler . learn_one ( x ) . transform_one ( x ) # Fit the linear regression y_pred = lin_reg . predict_one ( x ) lin_reg . learn_one ( x , y ) # Update the metric using the out-of-fold prediction metric . update ( y , y_pred ) print ( metric ) MAE: 8.465114 We're not using many features. We can print the last x to get an idea of the features (don't forget they've been scaled!) pprint ( x ) {'is_holiday': -0.23103573677646685, 'is_weekend': 1.6249280076334165, 'target_rollingmean_14_by_store_id': -1.4125913815779154, 'target_rollingmean_21_by_store_id': -1.3980979075298519, 'target_rollingmean_7_by_store_id': -1.3502314499809096, 'weekday': 1.0292832579142892} The above chunk of code is quite explicit but it's a bit verbose. The whole point of libraries such as creme is to make life easier for users. Moreover there's too much space for users to mess up the order in which things are done, which increases the chance of there being target leakage. We'll now rewrite our model in a declarative fashion using a pipeline \u00e0 la sklearn . from creme import compose def get_date_features ( x ): weekday = x [ 'date' ] . weekday () return { 'weekday' : weekday , 'is_weekend' : weekday in ( 5 , 6 )} model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'date_features' , compose . FuncTransformer ( get_date_features )), ( 'last_7_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 ))), ( 'last_14_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 ))), ( 'last_21_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 ))) )), ( 'drop_non_features' , compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ()) ) metric = metrics . MAE () for x , y in datasets . Restaurants (): # Make a prediction without using the target y_pred = model . predict_one ( x ) # Update the model using the target model . learn_one ( x , y ) # Update the metric using the out-of-fold prediction metric . update ( y , y_pred ) print ( metric ) MAE: 8.38533 We use a Pipeline to arrange each step in a sequential order. A TransformerUnion is used to merge multiple feature extractors into a single transformer. The for loop is now much shorter and is thus easier to grok: we get the out-of-fold prediction, we fit the model, and finally we update the metric. This way of evaluating a model is typical of online learning, and so we put it wrapped it inside a function called online_score part of the model_selection module. We can use it to replace the for loop. from creme import model_selection model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'date_features' , compose . FuncTransformer ( get_date_features )), ( 'last_7_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 ))), ( 'last_14_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 ))), ( 'last_21_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 ))) )), ( 'drop_non_features' , compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ()) ) model_selection . progressive_val_score ( X_y = datasets . Restaurants (), model = model , metric = metrics . MAE ()) MAE: 8.38533 Notice that you couldn't have used the online_score method if you wrote the model in a procedural manner. Our code is getting shorter, but it's still a bit difficult on the eyes. Indeed there is a lot of boilerplate code associated with pipelines that can get tedious to write. However creme has some special tricks up it's sleeve to save you from a lot of pain. The first trick is that the name of each step in the pipeline can be omitted. If no name is given for a step then creme automatically infers one. model = compose . Pipeline ( compose . TransformerUnion ( compose . FuncTransformer ( get_date_features ), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )) ), compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ), preprocessing . StandardScaler (), linear_model . LinearRegression () ) model_selection . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Under the hood a Pipeline inherits from collections.OrderedDict . Indeed this makes sense because if you think about it a Pipeline is simply a sequence of steps where each step has a name. The reason we mention this is because it means you can manipulate a Pipeline the same way you would manipulate an ordinary dict . For instance we can print the name of each step by using the keys method. for name in model . steps : print ( name ) TransformerUnion Discard StandardScaler LinearRegression The first step is a FeatureUnion and it's string representation contains the string representation of each of it's elements. Not having to write names saves up some time and space and is certainly less tedious. The next trick is that we can use mathematical operators to compose our pipeline. For example we can use the + operator to merge Transformer s into a TransformerUnion . model = compose . Pipeline ( compose . FuncTransformer ( get_date_features ) + \\ feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )) + \\ feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )) + \\ feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )), compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ), preprocessing . StandardScaler (), linear_model . LinearRegression () ) model_selection . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Likewhise we can use the | operator to assemble steps into a Pipeline . model = ( compose . FuncTransformer ( get_date_features ) + feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )) + feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )) + feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )) ) to_discard = [ 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ] model = model | compose . Discard ( * to_discard ) | preprocessing . StandardScaler () model |= linear_model . LinearRegression () model_selection . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Hopefully you'll agree that this is a powerful way to express machine learning pipelines. For some people this should be quite remeniscent of the UNIX pipe operator. One final trick we want to mention is that functions are automatically wrapped with a FuncTransformer , which can be quite handy. model = get_date_features for n in [ 7 , 14 , 21 ]: model += feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( n )) model |= compose . Discard ( * to_discard ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () model_selection . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Naturally some may prefer the procedural style we first used because they find it easier to work with. It all depends on your style and you should use what you feel comfortable with. However we encourage you to use operators because we believe that this will increase the readability of your code, which is very important. To each their own! Before finishing we can take a look at what our pipeline looks graphically. model . draw ()","title":"The art of using pipelines"},{"location":"releases/0.0.2/","text":"0.0.2 - 2019-02-13 \u00b6 PyPI GitHub compat \u00b6 Added sklearn wrappers. ensemble \u00b6 Added ensemble.HedgeClassifier . feature_selection \u00b6 Added feature_selection.RandomDiscarder . feature_extraction \u00b6 Added feature_extraction.TargetEncoder . impute \u00b6 Added impute.NumericImputer . optim \u00b6 Added optim.AbsoluteLoss . Added optim.HingeLoss . Added optim.EpsilonInsensitiveHingeLoss . stats \u00b6 Added stats.NUnique . Added stats.Min . Added stats.Max . Added stats.PeakToPeak . Added stats.Kurtosis . Added stats.Skew . Added stats.Sum . Added stats.EWMean . Made sure the running statistics produce the same results as pandas.DataFrame.rolling method.","title":"0.0.2 - 2019-02-13"},{"location":"releases/0.0.2/#002-2019-02-13","text":"PyPI GitHub","title":"0.0.2 - 2019-02-13"},{"location":"releases/0.0.2/#compat","text":"Added sklearn wrappers.","title":"compat"},{"location":"releases/0.0.2/#ensemble","text":"Added ensemble.HedgeClassifier .","title":"ensemble"},{"location":"releases/0.0.2/#feature_selection","text":"Added feature_selection.RandomDiscarder .","title":"feature_selection"},{"location":"releases/0.0.2/#feature_extraction","text":"Added feature_extraction.TargetEncoder .","title":"feature_extraction"},{"location":"releases/0.0.2/#impute","text":"Added impute.NumericImputer .","title":"impute"},{"location":"releases/0.0.2/#optim","text":"Added optim.AbsoluteLoss . Added optim.HingeLoss . Added optim.EpsilonInsensitiveHingeLoss .","title":"optim"},{"location":"releases/0.0.2/#stats","text":"Added stats.NUnique . Added stats.Min . Added stats.Max . Added stats.PeakToPeak . Added stats.Kurtosis . Added stats.Skew . Added stats.Sum . Added stats.EWMean . Made sure the running statistics produce the same results as pandas.DataFrame.rolling method.","title":"stats"},{"location":"releases/0.0.3/","text":"0.0.3 - 2019-03-21 \u00b6 PyPI GitHub base \u00b6 Calling fit_one now returns the calling instance, not the out-of-fold prediction/transform; fit_predict_one , fit_predict_proba_one , and fit_transform_one are available to reproduce the previous behavior. Binary classifiers now output a dict with probabilities for False and True when calling predict_proba_one , which solves the interface issues of having multi-class classifiers do binary classification. compat \u00b6 Added compat.convert_creme_to_sklearn . compose \u00b6 Added compose.BoxCoxTransformRegressor . Added compose.TargetModifierRegressor . datasets \u00b6 Added datasets.fetch_restaurants . Added datasets.load_airline . dist \u00b6 Added dist.Multinomial . Added dist.Normal . ensemble \u00b6 Added ensemble.BaggingRegressor . feature_extraction \u00b6 Added feature_extraction.TargetGroupBy . impute \u00b6 Added impute.CategoricalImputer . linear_model \u00b6 Added linear_model.FMRegressor . Removed all the passive-aggressive estimators. metrics \u00b6 Added metrics.Accuracy . Added metrics.MAE . Added metrics.MSE . Added metrics.RMSE . Added metrics.RMSLE . Added metrics.SMAPE . Added metrics.Precision . Added metrics.Recall . Added metrics.F1 . model_selection \u00b6 model_selection.online_score can now be passed a metrics.Metric instead of an sklearn metric; it also checks that the provided metric can be used with the accompanying model. naive_bayes \u00b6 Added naive_bayes.GaussianNB . optim \u00b6 Added optim.PassiveAggressiveI . Added optim.PassiveAggressiveII . preprocessing \u00b6 Added preprocessing.Discarder . Added preprocessing.PolynomialExtender . Added preprocessing.FuncTransformer . reco \u00b6 Added reco.SVD . stats \u00b6 Added stats.Mode . Added stats.Quantile . Added stats.RollingQuantile . Added stats.Entropy . Added stats.RollingMin . Added stats.RollingMax . Added stats.RollingMode . Added stats.RollingSum . Added stats.RollingPeakToPeak . stream \u00b6 Added stream.iter_csv . tree \u00b6 Added tree.MondrianTreeClassifier . Added tree.MondrianTreeRegressor .","title":"0.0.3 - 2019-03-21"},{"location":"releases/0.0.3/#003-2019-03-21","text":"PyPI GitHub","title":"0.0.3 - 2019-03-21"},{"location":"releases/0.0.3/#base","text":"Calling fit_one now returns the calling instance, not the out-of-fold prediction/transform; fit_predict_one , fit_predict_proba_one , and fit_transform_one are available to reproduce the previous behavior. Binary classifiers now output a dict with probabilities for False and True when calling predict_proba_one , which solves the interface issues of having multi-class classifiers do binary classification.","title":"base"},{"location":"releases/0.0.3/#compat","text":"Added compat.convert_creme_to_sklearn .","title":"compat"},{"location":"releases/0.0.3/#compose","text":"Added compose.BoxCoxTransformRegressor . Added compose.TargetModifierRegressor .","title":"compose"},{"location":"releases/0.0.3/#datasets","text":"Added datasets.fetch_restaurants . Added datasets.load_airline .","title":"datasets"},{"location":"releases/0.0.3/#dist","text":"Added dist.Multinomial . Added dist.Normal .","title":"dist"},{"location":"releases/0.0.3/#ensemble","text":"Added ensemble.BaggingRegressor .","title":"ensemble"},{"location":"releases/0.0.3/#feature_extraction","text":"Added feature_extraction.TargetGroupBy .","title":"feature_extraction"},{"location":"releases/0.0.3/#impute","text":"Added impute.CategoricalImputer .","title":"impute"},{"location":"releases/0.0.3/#linear_model","text":"Added linear_model.FMRegressor . Removed all the passive-aggressive estimators.","title":"linear_model"},{"location":"releases/0.0.3/#metrics","text":"Added metrics.Accuracy . Added metrics.MAE . Added metrics.MSE . Added metrics.RMSE . Added metrics.RMSLE . Added metrics.SMAPE . Added metrics.Precision . Added metrics.Recall . Added metrics.F1 .","title":"metrics"},{"location":"releases/0.0.3/#model_selection","text":"model_selection.online_score can now be passed a metrics.Metric instead of an sklearn metric; it also checks that the provided metric can be used with the accompanying model.","title":"model_selection"},{"location":"releases/0.0.3/#naive_bayes","text":"Added naive_bayes.GaussianNB .","title":"naive_bayes"},{"location":"releases/0.0.3/#optim","text":"Added optim.PassiveAggressiveI . Added optim.PassiveAggressiveII .","title":"optim"},{"location":"releases/0.0.3/#preprocessing","text":"Added preprocessing.Discarder . Added preprocessing.PolynomialExtender . Added preprocessing.FuncTransformer .","title":"preprocessing"},{"location":"releases/0.0.3/#reco","text":"Added reco.SVD .","title":"reco"},{"location":"releases/0.0.3/#stats","text":"Added stats.Mode . Added stats.Quantile . Added stats.RollingQuantile . Added stats.Entropy . Added stats.RollingMin . Added stats.RollingMax . Added stats.RollingMode . Added stats.RollingSum . Added stats.RollingPeakToPeak .","title":"stats"},{"location":"releases/0.0.3/#stream","text":"Added stream.iter_csv .","title":"stream"},{"location":"releases/0.0.3/#tree","text":"Added tree.MondrianTreeClassifier . Added tree.MondrianTreeRegressor .","title":"tree"},{"location":"releases/0.1.0/","text":"0.1.0 - 2019-05-08 \u00b6 PyPI GitHub base \u00b6 Removed the fit_predict_one estimator method. Removed the fit_predict_proba_one estimator method. Removed the fit_transform_one estimator method. compat \u00b6 Added compat.convert_sklearn_to_creme . compat.convert_creme_to_sklearn now returns an sklearn.pipeline.Pipeline when provided with a compose.Pipeline . compose \u00b6 Added compose.Discard . Added compose.Select . Added compose.SplitRegressor . The draw method of compose.Pipeline now works properly for arbitrary amounts of nesting, including multiple nested compose.FeatureUnion . datasets \u00b6 Added datasets.fetch_electricity . dummy \u00b6 Added dummy.NoChangeClassifier . Added dummy.PriorClassifier . Added dummy.StatisticRegressor . feature_extraction \u00b6 Added feature_extraction.Differ . Renamed feature_extraction.GroupBy to feature_extraction.Agg . Renamed feature_extraction.TargetGroupBy to feature_extraction.TargetAgg . feature_selection \u00b6 Added feature_selection.SelectKBest . Added feature_selection.VarianceThreshold . impute \u00b6 Added impute.StatImputer . Removed impute.CategoricalImputer . Removed impute.NumericImputer . linear_model \u00b6 Added linear_model.PAClassifier . Added linear_model.PARegressor . Added linear_model.SoftmaxRegression . metrics \u00b6 Added metrics.ConfusionMatrix . Added metrics.CrossEntropy . Added metrics.MacroF1 . Added metrics.MacroPrecision . Added metrics.MacroRecall . Added metrics.MicroF1 . Added metrics.MicroPrecision . Added metrics.MicroRecall . Each metric now has a bigger_is_better property to indicate if a high value is better than a low one or not. optim \u00b6 Added optim.OptimalLR . Added optim.CrossEntropy . Removed optim.PassiveAggressiveI . Removed optim.PassiveAggressiveII . preprocessing \u00b6 Removed preprocessing.Discarder . Added on and sparse parameters to preprocessing.OneHotEncoder . stats \u00b6 Added stats.Covariance . Added stats.PearsonCorrelation . Added stats.SmoothMean . utils \u00b6 Added utils.check_estimator . Added utils.Histogram . Added utils.SortedWindow . Added utils.Window .","title":"0.1.0 - 2019-05-08"},{"location":"releases/0.1.0/#010-2019-05-08","text":"PyPI GitHub","title":"0.1.0 - 2019-05-08"},{"location":"releases/0.1.0/#base","text":"Removed the fit_predict_one estimator method. Removed the fit_predict_proba_one estimator method. Removed the fit_transform_one estimator method.","title":"base"},{"location":"releases/0.1.0/#compat","text":"Added compat.convert_sklearn_to_creme . compat.convert_creme_to_sklearn now returns an sklearn.pipeline.Pipeline when provided with a compose.Pipeline .","title":"compat"},{"location":"releases/0.1.0/#compose","text":"Added compose.Discard . Added compose.Select . Added compose.SplitRegressor . The draw method of compose.Pipeline now works properly for arbitrary amounts of nesting, including multiple nested compose.FeatureUnion .","title":"compose"},{"location":"releases/0.1.0/#datasets","text":"Added datasets.fetch_electricity .","title":"datasets"},{"location":"releases/0.1.0/#dummy","text":"Added dummy.NoChangeClassifier . Added dummy.PriorClassifier . Added dummy.StatisticRegressor .","title":"dummy"},{"location":"releases/0.1.0/#feature_extraction","text":"Added feature_extraction.Differ . Renamed feature_extraction.GroupBy to feature_extraction.Agg . Renamed feature_extraction.TargetGroupBy to feature_extraction.TargetAgg .","title":"feature_extraction"},{"location":"releases/0.1.0/#feature_selection","text":"Added feature_selection.SelectKBest . Added feature_selection.VarianceThreshold .","title":"feature_selection"},{"location":"releases/0.1.0/#impute","text":"Added impute.StatImputer . Removed impute.CategoricalImputer . Removed impute.NumericImputer .","title":"impute"},{"location":"releases/0.1.0/#linear_model","text":"Added linear_model.PAClassifier . Added linear_model.PARegressor . Added linear_model.SoftmaxRegression .","title":"linear_model"},{"location":"releases/0.1.0/#metrics","text":"Added metrics.ConfusionMatrix . Added metrics.CrossEntropy . Added metrics.MacroF1 . Added metrics.MacroPrecision . Added metrics.MacroRecall . Added metrics.MicroF1 . Added metrics.MicroPrecision . Added metrics.MicroRecall . Each metric now has a bigger_is_better property to indicate if a high value is better than a low one or not.","title":"metrics"},{"location":"releases/0.1.0/#optim","text":"Added optim.OptimalLR . Added optim.CrossEntropy . Removed optim.PassiveAggressiveI . Removed optim.PassiveAggressiveII .","title":"optim"},{"location":"releases/0.1.0/#preprocessing","text":"Removed preprocessing.Discarder . Added on and sparse parameters to preprocessing.OneHotEncoder .","title":"preprocessing"},{"location":"releases/0.1.0/#stats","text":"Added stats.Covariance . Added stats.PearsonCorrelation . Added stats.SmoothMean .","title":"stats"},{"location":"releases/0.1.0/#utils","text":"Added utils.check_estimator . Added utils.Histogram . Added utils.SortedWindow . Added utils.Window .","title":"utils"},{"location":"releases/0.2.0/","text":"0.2.0 - 2019-05-27 \u00b6 PyPI GitHub compose \u00b6 compose.Pipeline now has a debug_one . compose.Discard and compose.Select now take variadic inputs, which means you don't have to provide a list of features to exclude/include. datasets \u00b6 Added datasets.fetch_bikes feature_extraction \u00b6 Classes that inherit from feature_extraction.VectorizerMixin can now directly be passed str instances instead of dict instances. feature_extraction.Agg and feature_extraction.TargetAgg can now aggregate on multiple attributes. metrics \u00b6 Added RollingAccuracy Added RollingCrossEntropy Added RollingF1 Added RollingLogLoss Added RollingMacroF1 Added RollingMacroPrecision Added RollingMacroRecall Added RollingMAE Added RollingMicroF1 Added RollingMicroPrecision Added RollingMicroRecall Added RollingMSE Added RollingPrecision Added RollingRecall Added RollingRMSE Added RollingRMSLE Added RollingSMAPE model_selection \u00b6 Added model_selection.online_qa_score . proba \u00b6 The dist module has been renamed to proba and is now public, for the moment it contains a single distribution called proba.Gaussian . naive_bayes \u00b6 Added naive_bayes.BernoulliNB . Added naive_bayes.ComplementNB . optim \u00b6 Added optim.AdaBound . tree \u00b6 Added tree.DecisionTreeClassifier . Removed tree.MondrianTreeClassifier and tree.MondrianTreeRegressor because their performance wasn't good enough. stats \u00b6 Added stats.AutoCorrelation . Added stats.EWVar . Rename stats.Variance to stats.Var and stats.RollingVariance to stats.RollingVar . stream \u00b6 Added stream.simulate_qa . utils \u00b6 Added utils.SDFT . Added utils.Skyline . Renamed the window_size parameter to size in utils.Window and utils.SortedWindow .","title":"0.2.0 - 2019-05-27"},{"location":"releases/0.2.0/#020-2019-05-27","text":"PyPI GitHub","title":"0.2.0 - 2019-05-27"},{"location":"releases/0.2.0/#compose","text":"compose.Pipeline now has a debug_one . compose.Discard and compose.Select now take variadic inputs, which means you don't have to provide a list of features to exclude/include.","title":"compose"},{"location":"releases/0.2.0/#datasets","text":"Added datasets.fetch_bikes","title":"datasets"},{"location":"releases/0.2.0/#feature_extraction","text":"Classes that inherit from feature_extraction.VectorizerMixin can now directly be passed str instances instead of dict instances. feature_extraction.Agg and feature_extraction.TargetAgg can now aggregate on multiple attributes.","title":"feature_extraction"},{"location":"releases/0.2.0/#metrics","text":"Added RollingAccuracy Added RollingCrossEntropy Added RollingF1 Added RollingLogLoss Added RollingMacroF1 Added RollingMacroPrecision Added RollingMacroRecall Added RollingMAE Added RollingMicroF1 Added RollingMicroPrecision Added RollingMicroRecall Added RollingMSE Added RollingPrecision Added RollingRecall Added RollingRMSE Added RollingRMSLE Added RollingSMAPE","title":"metrics"},{"location":"releases/0.2.0/#model_selection","text":"Added model_selection.online_qa_score .","title":"model_selection"},{"location":"releases/0.2.0/#proba","text":"The dist module has been renamed to proba and is now public, for the moment it contains a single distribution called proba.Gaussian .","title":"proba"},{"location":"releases/0.2.0/#naive_bayes","text":"Added naive_bayes.BernoulliNB . Added naive_bayes.ComplementNB .","title":"naive_bayes"},{"location":"releases/0.2.0/#optim","text":"Added optim.AdaBound .","title":"optim"},{"location":"releases/0.2.0/#tree","text":"Added tree.DecisionTreeClassifier . Removed tree.MondrianTreeClassifier and tree.MondrianTreeRegressor because their performance wasn't good enough.","title":"tree"},{"location":"releases/0.2.0/#stats","text":"Added stats.AutoCorrelation . Added stats.EWVar . Rename stats.Variance to stats.Var and stats.RollingVariance to stats.RollingVar .","title":"stats"},{"location":"releases/0.2.0/#stream","text":"Added stream.simulate_qa .","title":"stream"},{"location":"releases/0.2.0/#utils","text":"Added utils.SDFT . Added utils.Skyline . Renamed the window_size parameter to size in utils.Window and utils.SortedWindow .","title":"utils"},{"location":"releases/0.3.0/","text":"0.3.0 - 2019-06-23 \u00b6 PyPI GitHub datasets \u00b6 Added datasets.load_chick_weights . decomposition \u00b6 Added decomposition.LDA . ensemble \u00b6 Added ensemble.HedgeRegressor . Added ensemble.StackingBinaryClassifier . metrics \u00b6 Added metrics.FBeta Added metrics.MacroFBeta Added metrics.MicroFBeta Added metrics.MultiFBeta Added metrics.RollingFBeta Added metrics.RollingMacroFBeta Added metrics.RollingMicroFBeta Added metrics.RollingMultiFBeta Added metrics.Jaccard Added metrics.RollingConfusionMatrix Added metrics.RegressionMultiOutput Added metrics.MCC Added metrics.RollingMCC Added metrics.ROCAUC Renamed metrics.F1Score to metrics.F1 . multioutput \u00b6 Added multioutput.ClassifierChain . Added multioutput.RegressorChain . optim \u00b6 Added optim.QuantileLoss Added optim.MiniBatcher . preprocessing \u00b6 Added preprocessing.Normalizer . proba \u00b6 Added proba.Multinomial .","title":"0.3.0 - 2019-06-23"},{"location":"releases/0.3.0/#030-2019-06-23","text":"PyPI GitHub","title":"0.3.0 - 2019-06-23"},{"location":"releases/0.3.0/#datasets","text":"Added datasets.load_chick_weights .","title":"datasets"},{"location":"releases/0.3.0/#decomposition","text":"Added decomposition.LDA .","title":"decomposition"},{"location":"releases/0.3.0/#ensemble","text":"Added ensemble.HedgeRegressor . Added ensemble.StackingBinaryClassifier .","title":"ensemble"},{"location":"releases/0.3.0/#metrics","text":"Added metrics.FBeta Added metrics.MacroFBeta Added metrics.MicroFBeta Added metrics.MultiFBeta Added metrics.RollingFBeta Added metrics.RollingMacroFBeta Added metrics.RollingMicroFBeta Added metrics.RollingMultiFBeta Added metrics.Jaccard Added metrics.RollingConfusionMatrix Added metrics.RegressionMultiOutput Added metrics.MCC Added metrics.RollingMCC Added metrics.ROCAUC Renamed metrics.F1Score to metrics.F1 .","title":"metrics"},{"location":"releases/0.3.0/#multioutput","text":"Added multioutput.ClassifierChain . Added multioutput.RegressorChain .","title":"multioutput"},{"location":"releases/0.3.0/#optim","text":"Added optim.QuantileLoss Added optim.MiniBatcher .","title":"optim"},{"location":"releases/0.3.0/#preprocessing","text":"Added preprocessing.Normalizer .","title":"preprocessing"},{"location":"releases/0.3.0/#proba","text":"Added proba.Multinomial .","title":"proba"},{"location":"releases/0.4.1/","text":"0.4.1 - 2019-10-23 \u00b6 PyPI GitHub base \u00b6 Tests are now much more extensive, thanks mostly to the newly added estimator tags. compose \u00b6 Added compose.Renamer . datasets \u00b6 Added fetch_kdd99_http . Added fetch_sms . Added fetch_trec07p . ensemble \u00b6 Removed ensemble.HedgeBinaryClassifier because it's performance was subpar. Removed ensemble.GroupRegressor , as this should be a special case of ensemble.StackingRegressor . feature_extraction \u00b6 Fixed a bug where feature_extraction.CountVectorizer and feature_extraction.TFIDFVectorizer couldn't be pickled. linear_model \u00b6 linear_model.LogisticRegression and linear_model.LinearRegression now have an intercept_lr parameter. metrics \u00b6 Metrics can now be composed using the + operator, which is useful for evaluating multiple metrics at the same time. Added metrics.Rolling , which eliminates the need for a specific rolling implementation for each metric. Each metric can now be passed a sample_weight argument. Added metrics.WeightedF1 . Added metrics.WeightedFBeta . Added metrics.WeightedPrecision . Added metrics.WeightedRecall . neighbors \u00b6 Added neighbors.KNeighborsRegressor . Added neighbors.KNeighborsClassifier . optim \u00b6 Added optim.AdaMax . The optim module has been reorganized into submodules; namely optim.schedulers , optim.initializers , and optim.losses . The top-level now only contains optimizers. Some classes have been renamed accordingly. See the documentation for details. Renamed optim.VanillaSGD to optim.SGD . stats \u00b6 Added stats.IQR . Added stats.RollingIQR . Cythonized stats.Mean and stats.Var . stream \u00b6 Added stream.shuffle . stream.iter_csv now has fraction and seed parameters to sample rows, deterministically or not. Renamed stream.iter_numpy to stream.iter_array . stream.iter_csv can now read from gzipped files. time_series \u00b6 time_series.Detrender now has a window_size parameter for detrending with a rolling mean. tree \u00b6 Added tree.RandomForestClassifier . utils \u00b6 Fixed a bug where utils.dot could take longer than necessary.","title":"0.4.1 - 2019-10-23"},{"location":"releases/0.4.1/#041-2019-10-23","text":"PyPI GitHub","title":"0.4.1 - 2019-10-23"},{"location":"releases/0.4.1/#base","text":"Tests are now much more extensive, thanks mostly to the newly added estimator tags.","title":"base"},{"location":"releases/0.4.1/#compose","text":"Added compose.Renamer .","title":"compose"},{"location":"releases/0.4.1/#datasets","text":"Added fetch_kdd99_http . Added fetch_sms . Added fetch_trec07p .","title":"datasets"},{"location":"releases/0.4.1/#ensemble","text":"Removed ensemble.HedgeBinaryClassifier because it's performance was subpar. Removed ensemble.GroupRegressor , as this should be a special case of ensemble.StackingRegressor .","title":"ensemble"},{"location":"releases/0.4.1/#feature_extraction","text":"Fixed a bug where feature_extraction.CountVectorizer and feature_extraction.TFIDFVectorizer couldn't be pickled.","title":"feature_extraction"},{"location":"releases/0.4.1/#linear_model","text":"linear_model.LogisticRegression and linear_model.LinearRegression now have an intercept_lr parameter.","title":"linear_model"},{"location":"releases/0.4.1/#metrics","text":"Metrics can now be composed using the + operator, which is useful for evaluating multiple metrics at the same time. Added metrics.Rolling , which eliminates the need for a specific rolling implementation for each metric. Each metric can now be passed a sample_weight argument. Added metrics.WeightedF1 . Added metrics.WeightedFBeta . Added metrics.WeightedPrecision . Added metrics.WeightedRecall .","title":"metrics"},{"location":"releases/0.4.1/#neighbors","text":"Added neighbors.KNeighborsRegressor . Added neighbors.KNeighborsClassifier .","title":"neighbors"},{"location":"releases/0.4.1/#optim","text":"Added optim.AdaMax . The optim module has been reorganized into submodules; namely optim.schedulers , optim.initializers , and optim.losses . The top-level now only contains optimizers. Some classes have been renamed accordingly. See the documentation for details. Renamed optim.VanillaSGD to optim.SGD .","title":"optim"},{"location":"releases/0.4.1/#stats","text":"Added stats.IQR . Added stats.RollingIQR . Cythonized stats.Mean and stats.Var .","title":"stats"},{"location":"releases/0.4.1/#stream","text":"Added stream.shuffle . stream.iter_csv now has fraction and seed parameters to sample rows, deterministically or not. Renamed stream.iter_numpy to stream.iter_array . stream.iter_csv can now read from gzipped files.","title":"stream"},{"location":"releases/0.4.1/#time_series","text":"time_series.Detrender now has a window_size parameter for detrending with a rolling mean.","title":"time_series"},{"location":"releases/0.4.1/#tree","text":"Added tree.RandomForestClassifier .","title":"tree"},{"location":"releases/0.4.1/#utils","text":"Fixed a bug where utils.dot could take longer than necessary.","title":"utils"},{"location":"releases/0.4.3/","text":"0.4.3 - 2019-10-27 \u00b6 PyPI GitHub base \u00b6 Model that inherit from base.Wrapper (e.g. tree.RandomForestClassifier ) can now be pickled. datasets \u00b6 Added datasets.fetch_credit_card . utils \u00b6 Added the utils.math sub-module. tree \u00b6 Fixed the debug_one method of tree.DecisionTreeClassifier .","title":"0.4.3 - 2019-10-27"},{"location":"releases/0.4.3/#043-2019-10-27","text":"PyPI GitHub","title":"0.4.3 - 2019-10-27"},{"location":"releases/0.4.3/#base","text":"Model that inherit from base.Wrapper (e.g. tree.RandomForestClassifier ) can now be pickled.","title":"base"},{"location":"releases/0.4.3/#datasets","text":"Added datasets.fetch_credit_card .","title":"datasets"},{"location":"releases/0.4.3/#utils","text":"Added the utils.math sub-module.","title":"utils"},{"location":"releases/0.4.3/#tree","text":"Fixed the debug_one method of tree.DecisionTreeClassifier .","title":"tree"},{"location":"releases/0.4.4/","text":"0.4.4 - 2019-11-11 \u00b6 PyPI GitHub This release was mainly made to provide access to wheels <https://pythonwheels.com/> _ for Windows and MacOS. ensemble \u00b6 Added ensemble.AdaBoostClassifier . linear_model \u00b6 Added a clip_gradient parameter to linear_model.LinearRegression and linear_model.LogisticRegression . Gradient clipping was already implemented, but the maximum absolute value can now be set by the user. The intercept_lr parameter of linear_model.LinearRegression and linear_model.LogisticRegression can now be passed an instance of optim.schedulers.Scheduler as well as a float . metrics \u00b6 Fixed metrics.SMAPE , the implementation was missing a multiplication by 2. optim \u00b6 Added optim.schedulers.Optimal produces results that are identical to sklearn.linear_model.SGDRegressor and sklearn.linear_model.SGDClassifier when setting their learning_rate parameter to 'optimal' . time_series \u00b6 Added time_series.SNARIMAX , a generic model which encompasses well-known time series models such as ARIMA and NARX.","title":"0.4.4 - 2019-11-11"},{"location":"releases/0.4.4/#044-2019-11-11","text":"PyPI GitHub This release was mainly made to provide access to wheels <https://pythonwheels.com/> _ for Windows and MacOS.","title":"0.4.4 - 2019-11-11"},{"location":"releases/0.4.4/#ensemble","text":"Added ensemble.AdaBoostClassifier .","title":"ensemble"},{"location":"releases/0.4.4/#linear_model","text":"Added a clip_gradient parameter to linear_model.LinearRegression and linear_model.LogisticRegression . Gradient clipping was already implemented, but the maximum absolute value can now be set by the user. The intercept_lr parameter of linear_model.LinearRegression and linear_model.LogisticRegression can now be passed an instance of optim.schedulers.Scheduler as well as a float .","title":"linear_model"},{"location":"releases/0.4.4/#metrics","text":"Fixed metrics.SMAPE , the implementation was missing a multiplication by 2.","title":"metrics"},{"location":"releases/0.4.4/#optim","text":"Added optim.schedulers.Optimal produces results that are identical to sklearn.linear_model.SGDRegressor and sklearn.linear_model.SGDClassifier when setting their learning_rate parameter to 'optimal' .","title":"optim"},{"location":"releases/0.4.4/#time_series","text":"Added time_series.SNARIMAX , a generic model which encompasses well-known time series models such as ARIMA and NARX.","title":"time_series"},{"location":"releases/0.5.0/","text":"0.5.0 - 2020-03-13 \u00b6 PyPI GitHub compat \u00b6 Added compat.PyTorch2CremeRegressor . compat.SKL2CremeRegressor and compat.SKL2CremeClassifier now have an optional batch_size parameter in order to perform mini-batching. compose \u00b6 Renamed compose.Whitelister to compose.Select . Renamed compose.Blacklister to compose.Discard . facto \u00b6 Added facto.FFMClassifier . Added facto.FFMRegressor . Added facto.FwFMClassifier . Added facto.FwFMRegressor . Added facto.HOFMClassifier . Added facto.HOFMRegressor . Refactored facto.FMClassifier . Refactored facto.FMRegressor . feature_selection \u00b6 Added feature_selection.PoissonInclusion . Removed feature_selection.RandomDiscarder as it didn't make much sense. feature_extraction \u00b6 Renamed feature_extraction.CountVectorizer to feature_extraction.BagOfWords . Renamed feature_extraction.TFIDFVectorizer to feature_extraction.TFIDF . Added preprocessor and ngram_range parameters to feature_extraction.BagOfWords . Added preprocessor and ngram_range parameters to feature_extraction.TFIDF . datasets \u00b6 The datasets module has been overhauled. Each dataset is now a class (e.g. fetch_electricity has become datasets.Elec2 ). Added datasets.TrumpApproval . Added datasets.MaliciousURL . Added datasets.gen.SEA . Added datasets.Higgs . Added datasets.MovieLens100K . Added datasets.Bananas . Added datasets.Taxis . Added datasets.ImageSegments . Added datasets.SMTP impute \u00b6 Added impute.PreviousImputer . linear_model \u00b6 linear_model.FMClassifier has been moved to the facto module. linear_model.FMRegressor has been moved to the facto module. Added linear_model.ALMAClassifier . metrics \u00b6 Added metrics.ClassificationReport . Added metrics.TimeRolling . The implementation of metrics.ROCAUC was incorrect. Using the trapezoidal rule instead of Simpson's rule seems to be more robust. metrics.PerClass has been removed; it is recommended that you use metrics.ClassificationReport instead as it gives a better overview. meta \u00b6 Moved meta.TransformedTargetRegressor and meta.BoxCoxRegressor to this module (they were previously in the compose module). Added meta.PredClipper model_selection \u00b6 Added model_selection.expand_param_grid to generate a list of models from a grid of parameters. Added the model_selection.successive_halving method for selecting hyperparameters. The online_score and online_qa_score methods have been merged into a single method named model_selection.progressive_val_score . preprocessing \u00b6 Added preprocessing.RBFSampler . Added preprocessing.MaxAbsScaler . Added preprocessing.RobustScaler . Added preprocessing.Binarizer . Added with_mean and with_std parameters to preprocessing.StandardScaler . optim \u00b6 Added optim.losses.BinaryFocalLoss . Added the optim.AMSGrad optimizer. Added the optim.Nadam optimizer. Added optim.losses.Poisson . Fixed a performance bug in optim.NesterovMomentum . reco \u00b6 Added reco.FunkMF . Renamed reco.SVD to reco.BiasedMF . Renamed reco.SGDBaseline to reco.Baseline . Models now expect a dict input with user and item fields. sampling \u00b6 Added sampling.RandomUnderSampler . Added sampling.RandomOverSampler . Added sampling.RandomSampler . Added sampling.HardSamplingClassifier . Added sampling.HardSamplingRegressor . stats \u00b6 Added stats.AbsMax . Added stats.RollingAbsMax . stream \u00b6 Added stream.iter_libsvm . stream.iter_csv now supports reading from '.zip' files. Added stream.Cache . Added a drop parameter to stream.iter_csv to discard fields.","title":"0.5.0 - 2020-03-13"},{"location":"releases/0.5.0/#050-2020-03-13","text":"PyPI GitHub","title":"0.5.0 - 2020-03-13"},{"location":"releases/0.5.0/#compat","text":"Added compat.PyTorch2CremeRegressor . compat.SKL2CremeRegressor and compat.SKL2CremeClassifier now have an optional batch_size parameter in order to perform mini-batching.","title":"compat"},{"location":"releases/0.5.0/#compose","text":"Renamed compose.Whitelister to compose.Select . Renamed compose.Blacklister to compose.Discard .","title":"compose"},{"location":"releases/0.5.0/#facto","text":"Added facto.FFMClassifier . Added facto.FFMRegressor . Added facto.FwFMClassifier . Added facto.FwFMRegressor . Added facto.HOFMClassifier . Added facto.HOFMRegressor . Refactored facto.FMClassifier . Refactored facto.FMRegressor .","title":"facto"},{"location":"releases/0.5.0/#feature_selection","text":"Added feature_selection.PoissonInclusion . Removed feature_selection.RandomDiscarder as it didn't make much sense.","title":"feature_selection"},{"location":"releases/0.5.0/#feature_extraction","text":"Renamed feature_extraction.CountVectorizer to feature_extraction.BagOfWords . Renamed feature_extraction.TFIDFVectorizer to feature_extraction.TFIDF . Added preprocessor and ngram_range parameters to feature_extraction.BagOfWords . Added preprocessor and ngram_range parameters to feature_extraction.TFIDF .","title":"feature_extraction"},{"location":"releases/0.5.0/#datasets","text":"The datasets module has been overhauled. Each dataset is now a class (e.g. fetch_electricity has become datasets.Elec2 ). Added datasets.TrumpApproval . Added datasets.MaliciousURL . Added datasets.gen.SEA . Added datasets.Higgs . Added datasets.MovieLens100K . Added datasets.Bananas . Added datasets.Taxis . Added datasets.ImageSegments . Added datasets.SMTP","title":"datasets"},{"location":"releases/0.5.0/#impute","text":"Added impute.PreviousImputer .","title":"impute"},{"location":"releases/0.5.0/#linear_model","text":"linear_model.FMClassifier has been moved to the facto module. linear_model.FMRegressor has been moved to the facto module. Added linear_model.ALMAClassifier .","title":"linear_model"},{"location":"releases/0.5.0/#metrics","text":"Added metrics.ClassificationReport . Added metrics.TimeRolling . The implementation of metrics.ROCAUC was incorrect. Using the trapezoidal rule instead of Simpson's rule seems to be more robust. metrics.PerClass has been removed; it is recommended that you use metrics.ClassificationReport instead as it gives a better overview.","title":"metrics"},{"location":"releases/0.5.0/#meta","text":"Moved meta.TransformedTargetRegressor and meta.BoxCoxRegressor to this module (they were previously in the compose module). Added meta.PredClipper","title":"meta"},{"location":"releases/0.5.0/#model_selection","text":"Added model_selection.expand_param_grid to generate a list of models from a grid of parameters. Added the model_selection.successive_halving method for selecting hyperparameters. The online_score and online_qa_score methods have been merged into a single method named model_selection.progressive_val_score .","title":"model_selection"},{"location":"releases/0.5.0/#preprocessing","text":"Added preprocessing.RBFSampler . Added preprocessing.MaxAbsScaler . Added preprocessing.RobustScaler . Added preprocessing.Binarizer . Added with_mean and with_std parameters to preprocessing.StandardScaler .","title":"preprocessing"},{"location":"releases/0.5.0/#optim","text":"Added optim.losses.BinaryFocalLoss . Added the optim.AMSGrad optimizer. Added the optim.Nadam optimizer. Added optim.losses.Poisson . Fixed a performance bug in optim.NesterovMomentum .","title":"optim"},{"location":"releases/0.5.0/#reco","text":"Added reco.FunkMF . Renamed reco.SVD to reco.BiasedMF . Renamed reco.SGDBaseline to reco.Baseline . Models now expect a dict input with user and item fields.","title":"reco"},{"location":"releases/0.5.0/#sampling","text":"Added sampling.RandomUnderSampler . Added sampling.RandomOverSampler . Added sampling.RandomSampler . Added sampling.HardSamplingClassifier . Added sampling.HardSamplingRegressor .","title":"sampling"},{"location":"releases/0.5.0/#stats","text":"Added stats.AbsMax . Added stats.RollingAbsMax .","title":"stats"},{"location":"releases/0.5.0/#stream","text":"Added stream.iter_libsvm . stream.iter_csv now supports reading from '.zip' files. Added stream.Cache . Added a drop parameter to stream.iter_csv to discard fields.","title":"stream"},{"location":"releases/0.5.1/","text":"0.5.1 - 2020-03-29 \u00b6 PyPI GitHub compose \u00b6 compose.Pipeline and compose.TransformerUnion now variadic arguments as input instead of a list. This doesn't change anything when using the shorthand operators | and + . model_selection \u00b6 Removed model_selection.successive_halving Added model_selection.SuccessiveHalvingRegressor and model_selection.SuccessiveHalvingClassifier stream \u00b6 Added a copy parameter to stream.simulate_qa in order to handle unwanted feature modifications. tree \u00b6 Added a curtail_under parameter to tree.DecisionTreeClassifier . The speed and accuracy of both tree.DecisionTreeClassifier and tree.RandomForestClassifier has been slightly improved for numerical attributes. The esthetics of the tree.DecisionTreeClassifier.draw method have been improved.","title":"0.5.1 - 2020-03-29"},{"location":"releases/0.5.1/#051-2020-03-29","text":"PyPI GitHub","title":"0.5.1 - 2020-03-29"},{"location":"releases/0.5.1/#compose","text":"compose.Pipeline and compose.TransformerUnion now variadic arguments as input instead of a list. This doesn't change anything when using the shorthand operators | and + .","title":"compose"},{"location":"releases/0.5.1/#model_selection","text":"Removed model_selection.successive_halving Added model_selection.SuccessiveHalvingRegressor and model_selection.SuccessiveHalvingClassifier","title":"model_selection"},{"location":"releases/0.5.1/#stream","text":"Added a copy parameter to stream.simulate_qa in order to handle unwanted feature modifications.","title":"stream"},{"location":"releases/0.5.1/#tree","text":"Added a curtail_under parameter to tree.DecisionTreeClassifier . The speed and accuracy of both tree.DecisionTreeClassifier and tree.RandomForestClassifier has been slightly improved for numerical attributes. The esthetics of the tree.DecisionTreeClassifier.draw method have been improved.","title":"tree"},{"location":"releases/0.6.0/","text":"0.6.0 - 2020-06-09 \u00b6 base \u00b6 Added a new base class called SupervisedTransformer from which supervised transformers inherit from. Before this, supervised transformers has a is_supervised property. compose \u00b6 Added compose.SelectType , which allows selecting feature subsets based on their type. Added a score_one method to compose.Pipeline so that estimators from the anomaly module can be pipelined. Added compose.Grouper , which allows applying transformers within different subgroups. datasets \u00b6 Added datasets.Music , which is a dataset for multi-output binary classification. Added datasets.synth.Friedman , which is synthetic regression dataset. The datasets.gen module has been renamed to datasets.synth Each dataset now has a __repr__ method which displays some descriptive information. Added datasets.Insects , which has 10 variants. feature_extraction \u00b6 feature_extraction.Differ has been deprecated. We might put it back in a future if we find a better design. impute \u00b6 impute.StatImputer has been completely refactored. metrics \u00b6 In metrics.SMAPE , instead of raising a ZeroDivisionError , the convention is now to use 0 when both y_true and y_pred are equal to 0. model_selection \u00b6 Added the possibility to configure how the progress is printed in model_selection.progressive_val_score . For instance, the progress can now be printed to a file by providing the file argument. multiclass \u00b6 Added multiclass.OutputCodeClassifier . Added multiclass.OneVsOneClassifier . multioutput \u00b6 Fixed a bug where multioutput.ClassifierChain and multioutput.RegressorChain could not be pickled. stats \u00b6 Added stats.Shift , which can be used to compute statistics over a shifted version of a variable. Added stats.Link , which can be used to compose univariate statistics. Univariate statistics can now be composed via the | operator. Renamed stats.Covariance to stats.Cov . Renamed stats.PearsonCorrelation to stats.PearsonCorr . Renamed stats.AutoCorrelation to stats.AutoCorr . Added stats.RollingCov , which computes covariance between two variables over a window. Added stats.RollingPearsonCorr , which computes the Pearson correlation over a window. stream \u00b6 Added a stream.iter_sql utility method to work with SQLAlchemy. The target_name parameter of stream.iter_csv has been renamed to target . It can now be passed a list of values in order to support multi-output scenarios. Added stream.iter_arff for handling ARFF files. tree \u00b6 Cancelled the behavior where tree.DecisionTreeRegressor would raise an exception when no split was found.","title":"0.6.0 - 2020-06-09"},{"location":"releases/0.6.0/#060-2020-06-09","text":"","title":"0.6.0 - 2020-06-09"},{"location":"releases/0.6.0/#base","text":"Added a new base class called SupervisedTransformer from which supervised transformers inherit from. Before this, supervised transformers has a is_supervised property.","title":"base"},{"location":"releases/0.6.0/#compose","text":"Added compose.SelectType , which allows selecting feature subsets based on their type. Added a score_one method to compose.Pipeline so that estimators from the anomaly module can be pipelined. Added compose.Grouper , which allows applying transformers within different subgroups.","title":"compose"},{"location":"releases/0.6.0/#datasets","text":"Added datasets.Music , which is a dataset for multi-output binary classification. Added datasets.synth.Friedman , which is synthetic regression dataset. The datasets.gen module has been renamed to datasets.synth Each dataset now has a __repr__ method which displays some descriptive information. Added datasets.Insects , which has 10 variants.","title":"datasets"},{"location":"releases/0.6.0/#feature_extraction","text":"feature_extraction.Differ has been deprecated. We might put it back in a future if we find a better design.","title":"feature_extraction"},{"location":"releases/0.6.0/#impute","text":"impute.StatImputer has been completely refactored.","title":"impute"},{"location":"releases/0.6.0/#metrics","text":"In metrics.SMAPE , instead of raising a ZeroDivisionError , the convention is now to use 0 when both y_true and y_pred are equal to 0.","title":"metrics"},{"location":"releases/0.6.0/#model_selection","text":"Added the possibility to configure how the progress is printed in model_selection.progressive_val_score . For instance, the progress can now be printed to a file by providing the file argument.","title":"model_selection"},{"location":"releases/0.6.0/#multiclass","text":"Added multiclass.OutputCodeClassifier . Added multiclass.OneVsOneClassifier .","title":"multiclass"},{"location":"releases/0.6.0/#multioutput","text":"Fixed a bug where multioutput.ClassifierChain and multioutput.RegressorChain could not be pickled.","title":"multioutput"},{"location":"releases/0.6.0/#stats","text":"Added stats.Shift , which can be used to compute statistics over a shifted version of a variable. Added stats.Link , which can be used to compose univariate statistics. Univariate statistics can now be composed via the | operator. Renamed stats.Covariance to stats.Cov . Renamed stats.PearsonCorrelation to stats.PearsonCorr . Renamed stats.AutoCorrelation to stats.AutoCorr . Added stats.RollingCov , which computes covariance between two variables over a window. Added stats.RollingPearsonCorr , which computes the Pearson correlation over a window.","title":"stats"},{"location":"releases/0.6.0/#stream","text":"Added a stream.iter_sql utility method to work with SQLAlchemy. The target_name parameter of stream.iter_csv has been renamed to target . It can now be passed a list of values in order to support multi-output scenarios. Added stream.iter_arff for handling ARFF files.","title":"stream"},{"location":"releases/0.6.0/#tree","text":"Cancelled the behavior where tree.DecisionTreeRegressor would raise an exception when no split was found.","title":"tree"},{"location":"releases/0.6.1/","text":"0.6.1 - 2020-06-10 \u00b6 compose \u00b6 Fixed a bug that occurred when part of a compose.Transformer was a compose.Pipeline and wasn't properly handled.","title":"0.6.1 - 2020-06-10"},{"location":"releases/0.6.1/#061-2020-06-10","text":"","title":"0.6.1 - 2020-06-10"},{"location":"releases/0.6.1/#compose","text":"Fixed a bug that occurred when part of a compose.Transformer was a compose.Pipeline and wasn't properly handled.","title":"compose"},{"location":"releases/unreleased/","text":"Unreleased \u00b6 base \u00b6 The base.BinaryClassifier and base.MultiClassifier have been merge into base.Classifier . The 'binary_only' tag is now used to indicate whether or not a classifier support multi-class classification or not. compose \u00b6 Fixed some bugs related to mini-batching in compose.Pipeline . datasets \u00b6 Added datasets.SolarFlare , which is a small multi-output regression dataset. decomposition \u00b6 decomposition.LDA now takes as input word counts instead of raw text. expert \u00b6 Created this new module, which will regroup methods that perform expert learning, which boils down to managing multiple models. Moved ensemble.StackingBinaryClassifier to expert.StackingClassifier . Moved model_selection.SuccessiveHalvingClassifier to expert.SuccessiveHalvingClassifier . Moved model_selection.SuccessiveHalvingRegressor to expert.SuccessiveHalvingRegressor . Moved ensemble.HedgeRegressor to ensemble.EWARegressor . evaluate \u00b6 Created this new module, which will contains methods for evaluating models. feature_extraction \u00b6 Moved preprocessing.PolynomialExtender to feature_extraction.PolynomialExtender . Moved preprocessing.RBFSampler to feature_extraction.RBFSampler . linear_model \u00b6 Added linear_model.Perceptron , which is implemented as a special case of logistic regression. model_selection \u00b6 Deleted this module. multiclass \u00b6 multiclass.OneVsRestClassifier now supports mini-batching. optim \u00b6 Removed optim.MiniBatcher . Implemented optim.Averager , which allows doing averaged stochastic gradient descent. Removed optim.Perceptron . utils \u00b6 Moved model_selection.expand_param_grid to utils.expand_param_grid .","title":"Unreleased"},{"location":"releases/unreleased/#unreleased","text":"","title":"Unreleased"},{"location":"releases/unreleased/#base","text":"The base.BinaryClassifier and base.MultiClassifier have been merge into base.Classifier . The 'binary_only' tag is now used to indicate whether or not a classifier support multi-class classification or not.","title":"base"},{"location":"releases/unreleased/#compose","text":"Fixed some bugs related to mini-batching in compose.Pipeline .","title":"compose"},{"location":"releases/unreleased/#datasets","text":"Added datasets.SolarFlare , which is a small multi-output regression dataset.","title":"datasets"},{"location":"releases/unreleased/#decomposition","text":"decomposition.LDA now takes as input word counts instead of raw text.","title":"decomposition"},{"location":"releases/unreleased/#expert","text":"Created this new module, which will regroup methods that perform expert learning, which boils down to managing multiple models. Moved ensemble.StackingBinaryClassifier to expert.StackingClassifier . Moved model_selection.SuccessiveHalvingClassifier to expert.SuccessiveHalvingClassifier . Moved model_selection.SuccessiveHalvingRegressor to expert.SuccessiveHalvingRegressor . Moved ensemble.HedgeRegressor to ensemble.EWARegressor .","title":"expert"},{"location":"releases/unreleased/#evaluate","text":"Created this new module, which will contains methods for evaluating models.","title":"evaluate"},{"location":"releases/unreleased/#feature_extraction","text":"Moved preprocessing.PolynomialExtender to feature_extraction.PolynomialExtender . Moved preprocessing.RBFSampler to feature_extraction.RBFSampler .","title":"feature_extraction"},{"location":"releases/unreleased/#linear_model","text":"Added linear_model.Perceptron , which is implemented as a special case of logistic regression.","title":"linear_model"},{"location":"releases/unreleased/#model_selection","text":"Deleted this module.","title":"model_selection"},{"location":"releases/unreleased/#multiclass","text":"multiclass.OneVsRestClassifier now supports mini-batching.","title":"multiclass"},{"location":"releases/unreleased/#optim","text":"Removed optim.MiniBatcher . Implemented optim.Averager , which allows doing averaged stochastic gradient descent. Removed optim.Perceptron .","title":"optim"},{"location":"releases/unreleased/#utils","text":"Moved model_selection.expand_param_grid to utils.expand_param_grid .","title":"utils"},{"location":"user-guide/feature-extraction/","text":"Feature extraction \u00b6 To do.","title":"Feature extraction"},{"location":"user-guide/feature-extraction/#feature-extraction","text":"To do.","title":"Feature extraction"},{"location":"user-guide/hyperparameter-tuning/","text":"Hyperparameter tuning \u00b6 To do.","title":"Hyperparameter tuning"},{"location":"user-guide/hyperparameter-tuning/#hyperparameter-tuning","text":"To do.","title":"Hyperparameter tuning"},{"location":"user-guide/mini-batching/","text":"Mini-batching \u00b6 In its purest form, online machine learning encompasses models which learn with one sample at a time. This is the design which is used in creme . The main downside of single-instance processing is that it doesn't scale to big data. Indeed, processing one sample at a time means that we are able to use vectorisation and other computational tools that are taken for granted in batch learning. On top of this, processing a large dataset in creme essentially involves a Python for loop, which might be too slow for some usecases. However, this doesn't mean that creme is slow. In fact, for processing a single instance, creme is actually a couple of orders of magnitude faster than libraries such as scikit-learn, PyTorch, and Tensorflow. The reason why is because creme is designed from the ground up to process a single instance, whereas the majority of other libraries choose to care about batches of data. Both approaches offer different compromises, and the best choice depends on your usecase. In order to propose the best of both worlds, creme offers some limited support for mini-batch learning. Some of creme 's estimators implement *_many methods on top of their *_one counterparts. For instance, preprocessing.StandardScaler has a learn_many method as well as a transform_many method, in addition to learn_one and transform_one . Each mini-batch method takes as input a pandas.DataFrame . Supervised estimators also take as input a pandas.Series of target values. We choose to use pandas.DataFrames over numpy.ndarrays because of the simple fact that the former allows us to name each feature. This in turn allows us to offer a uniform interface for both single instance and mini-batch learning. As an example, we will build a simple pipeline that scales the data and trains a logistic regression. Indeed, the compose.Pipeline class can be applied to mini-batches, as long as each step is able to do so. from creme import compose from creme import linear_model from creme import preprocessing model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) For this example, we will use datasets.Higgs . from creme import datasets dataset = datasets . Higgs () if not dataset . is_downloaded : dataset . download () dataset Downloading https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz (2.62 GB) Higgs dataset Task Binary classification Number of samples 11,000,000 Number of features 28 Sparse False Path /Users/mhalford/creme_data/Higgs/HIGGS.csv.gz URL https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz Size 2.55 GB Downloaded False The easiest way to read the data in a mini-batch fashion is to use the read_csv from pandas . import pandas as pd names = [ 'target' , 'lepton pT' , 'lepton eta' , 'lepton phi' , 'missing energy magnitude' , 'missing energy phi' , 'jet 1 pt' , 'jet 1 eta' , 'jet 1 phi' , 'jet 1 b-tag' , 'jet 2 pt' , 'jet 2 eta' , 'jet 2 phi' , 'jet 2 b-tag' , 'jet 3 pt' , 'jet 3 eta' , 'jet 3 phi' , 'jet 3 b-tag' , 'jet 4 pt' , 'jet 4 eta' , 'jet 4 phi' , 'jet 4 b-tag' , 'm_jj' , 'm_jjj' , 'm_lv' , 'm_jlv' , 'm_bb' , 'm_wbb' , 'm_wwbb' ] for x in pd . read_csv ( dataset . path , names = names , chunksize = 8096 , nrows = 3e5 ): y = x . pop ( 'target' ) y_pred = model . predict_proba_many ( x ) model . learn_many ( x , y ) If you are familiar with scikit-learn, you might be aware that some of their estimators have a partial_fit method, which is similar to creme's learn_many method. Here are some advantages that creme has over scikit-learn: We guarantee that creme's is just as fast, if not faster than scikit-learn. The differences are negligeable, but are slightly in favor of creme. We take as input dataframes, which allows us to name each feature. The benefit is that you can add/remove/permute features between batches and everything will keep working. Estimators that support mini-batches also support single instance learning. This means that you can enjoy the best of both worlds. For instance, you can train with mini-batches and use predict_one to make predictions. Note that you can check which estimators can process mini-batches programmatically: import importlib import inspect def can_mini_batch ( obj ): return hasattr ( obj , 'learn_many' ) for module in importlib . import_module ( 'creme' ) . __all__ : for name , obj in inspect . getmembers ( importlib . import_module ( f 'creme. { module } ' ), can_mini_batch ): print ( name ) Pipeline LinearRegression LogisticRegression StandardScaler Because mini-batch learning isn't treated as a first-class citizen, some of the creme's functionalities require some work in order to play nicely with mini-batches. For instance, the objects from the metrics module have an update method that take as input a single pair (y_true, y_pred) . This might change in the future, depending on the demand. We plan to promote more models to the mini-batch regime. However, we will only be doing so for the methods that benefit the most from it, as well as those that are most popular. Indeed, creme 's core philosophy will remain to cater to single instance learning.","title":"Mini-batching"},{"location":"user-guide/mini-batching/#mini-batching","text":"In its purest form, online machine learning encompasses models which learn with one sample at a time. This is the design which is used in creme . The main downside of single-instance processing is that it doesn't scale to big data. Indeed, processing one sample at a time means that we are able to use vectorisation and other computational tools that are taken for granted in batch learning. On top of this, processing a large dataset in creme essentially involves a Python for loop, which might be too slow for some usecases. However, this doesn't mean that creme is slow. In fact, for processing a single instance, creme is actually a couple of orders of magnitude faster than libraries such as scikit-learn, PyTorch, and Tensorflow. The reason why is because creme is designed from the ground up to process a single instance, whereas the majority of other libraries choose to care about batches of data. Both approaches offer different compromises, and the best choice depends on your usecase. In order to propose the best of both worlds, creme offers some limited support for mini-batch learning. Some of creme 's estimators implement *_many methods on top of their *_one counterparts. For instance, preprocessing.StandardScaler has a learn_many method as well as a transform_many method, in addition to learn_one and transform_one . Each mini-batch method takes as input a pandas.DataFrame . Supervised estimators also take as input a pandas.Series of target values. We choose to use pandas.DataFrames over numpy.ndarrays because of the simple fact that the former allows us to name each feature. This in turn allows us to offer a uniform interface for both single instance and mini-batch learning. As an example, we will build a simple pipeline that scales the data and trains a logistic regression. Indeed, the compose.Pipeline class can be applied to mini-batches, as long as each step is able to do so. from creme import compose from creme import linear_model from creme import preprocessing model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) For this example, we will use datasets.Higgs . from creme import datasets dataset = datasets . Higgs () if not dataset . is_downloaded : dataset . download () dataset Downloading https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz (2.62 GB) Higgs dataset Task Binary classification Number of samples 11,000,000 Number of features 28 Sparse False Path /Users/mhalford/creme_data/Higgs/HIGGS.csv.gz URL https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz Size 2.55 GB Downloaded False The easiest way to read the data in a mini-batch fashion is to use the read_csv from pandas . import pandas as pd names = [ 'target' , 'lepton pT' , 'lepton eta' , 'lepton phi' , 'missing energy magnitude' , 'missing energy phi' , 'jet 1 pt' , 'jet 1 eta' , 'jet 1 phi' , 'jet 1 b-tag' , 'jet 2 pt' , 'jet 2 eta' , 'jet 2 phi' , 'jet 2 b-tag' , 'jet 3 pt' , 'jet 3 eta' , 'jet 3 phi' , 'jet 3 b-tag' , 'jet 4 pt' , 'jet 4 eta' , 'jet 4 phi' , 'jet 4 b-tag' , 'm_jj' , 'm_jjj' , 'm_lv' , 'm_jlv' , 'm_bb' , 'm_wbb' , 'm_wwbb' ] for x in pd . read_csv ( dataset . path , names = names , chunksize = 8096 , nrows = 3e5 ): y = x . pop ( 'target' ) y_pred = model . predict_proba_many ( x ) model . learn_many ( x , y ) If you are familiar with scikit-learn, you might be aware that some of their estimators have a partial_fit method, which is similar to creme's learn_many method. Here are some advantages that creme has over scikit-learn: We guarantee that creme's is just as fast, if not faster than scikit-learn. The differences are negligeable, but are slightly in favor of creme. We take as input dataframes, which allows us to name each feature. The benefit is that you can add/remove/permute features between batches and everything will keep working. Estimators that support mini-batches also support single instance learning. This means that you can enjoy the best of both worlds. For instance, you can train with mini-batches and use predict_one to make predictions. Note that you can check which estimators can process mini-batches programmatically: import importlib import inspect def can_mini_batch ( obj ): return hasattr ( obj , 'learn_many' ) for module in importlib . import_module ( 'creme' ) . __all__ : for name , obj in inspect . getmembers ( importlib . import_module ( f 'creme. { module } ' ), can_mini_batch ): print ( name ) Pipeline LinearRegression LogisticRegression StandardScaler Because mini-batch learning isn't treated as a first-class citizen, some of the creme's functionalities require some work in order to play nicely with mini-batches. For instance, the objects from the metrics module have an update method that take as input a single pair (y_true, y_pred) . This might change in the future, depending on the demand. We plan to promote more models to the mini-batch regime. However, we will only be doing so for the methods that benefit the most from it, as well as those that are most popular. Indeed, creme 's core philosophy will remain to cater to single instance learning.","title":"Mini-batching"},{"location":"user-guide/model-evaluation/","text":"Model evaluation \u00b6 To do.","title":"Model evaluation"},{"location":"user-guide/model-evaluation/#model-evaluation","text":"To do.","title":"Model evaluation"},{"location":"user-guide/pipelines/","text":"Pipelines \u00b6 Pipelines are an integral part of creme. We encourage their usage and apply them in many of their examples. The compose.Pipeline contains all the logic for building and applying pipelines. A pipeline is essentially a list of estimators that are applied in sequence. The only requirement is that the first n - 1 steps be transformers. The last step can be a regressor, a classifier, a clusterer, a transformer, etc. Here is an example: from creme import compose from creme import linear_model from creme import preprocessing model = compose . Pipeline ( preprocessing . StandardScaler (), preprocessing . PolynomialExtender (), linear_model . LinearRegression () ) You can also use the | operator, as so: model = ( preprocessing . StandardScaler () | preprocessing . PolynomialExtender () | linear_model . LinearRegression () ) Or, equally: model = preprocessing . StandardScaler () model |= preprocessing . PolynomialExtender () model |= linear_model . LinearRegression () A pipeline has a draw method that can be used to visualize it: model . draw () compose.Pipeline inherits from base.Estimator , which means that it has a learn_one method. You would expect learn_one to update each estimator, but that's not actually what happens . Instead, the transformers are updated when predict_one (or predict_proba_one for that matter) is called. Indeed, in online machine learning, we can update the unsupervised parts of our model when a sample arrives. We don't have to wait for the ground truth to arrive in order to update unsupervised estimators that don't depend on it. In other words, in a pipeline, learn_one updates the supervised parts, whilst predict_one updates the unsupervised parts. It's important to be aware of this behavior, as it is quite different to what is done in other libraries that rely on batch machine learning. Here is a small example to illustrate the previous point: from creme import datasets dataset = datasets . TrumpApproval () x , y = next ( iter ( dataset )) x , y ({'ordinal_date': 736389, 'gallup': 43.843213, 'ipsos': 46.19925042857143, 'morning_consult': 48.318749, 'rasmussen': 44.104692, 'you_gov': 43.636914000000004}, 43.75505) Let us call predict_one , which will update each transformer, but won't update the linear regression. model . predict_one ( x ) 0.0 The prediction is nil because each weight of the linear regression is equal to 0. model [ 'StandardScaler' ] . means defaultdict(float, {'ordinal_date': 736389.0, 'gallup': 43.843213, 'ipsos': 46.19925042857143, 'morning_consult': 48.318749, 'rasmussen': 44.104692, 'you_gov': 43.636914000000004}) As we can see, the means of each feature have been updated, even though we called predict_one and not learn_one . Note that if you call transform_one with a pipeline who's last step is not a transformer, then the output from the last transformer (which is thus the penultimate step) will be returned: model . transform_one ( x ) {'ordinal_date': 0.0, 'gallup': 0.0, 'ipsos': 0.0, 'morning_consult': 0.0, 'rasmussen': 0.0, 'you_gov': 0.0, 'ordinal_date*ordinal_date': 0.0, 'ordinal_date*gallup': 0.0, 'ordinal_date*ipsos': 0.0, 'ordinal_date*morning_consult': 0.0, 'ordinal_date*rasmussen': 0.0, 'ordinal_date*you_gov': 0.0, 'gallup*gallup': 0.0, 'gallup*ipsos': 0.0, 'gallup*morning_consult': 0.0, 'gallup*rasmussen': 0.0, 'gallup*you_gov': 0.0, 'ipsos*ipsos': 0.0, 'ipsos*morning_consult': 0.0, 'ipsos*rasmussen': 0.0, 'ipsos*you_gov': 0.0, 'morning_consult*morning_consult': 0.0, 'morning_consult*rasmussen': 0.0, 'morning_consult*you_gov': 0.0, 'rasmussen*rasmussen': 0.0, 'rasmussen*you_gov': 0.0, 'you_gov*you_gov': 0.0} In many cases, you might want to connect a step to multiple steps. For instance, you might to extract different kinds of features from a single input. An elegant way to do this is to use a compose.TransformerUnion . Essentially, the latter is a list of transformers who's results will be merged into a single dict when transform_one is called. As an example let's say that we want to apply a preprocessing.RBFSampler as well as the preprocessing.PolynomialExtender . This may be done as so: model = ( preprocessing . StandardScaler () | ( preprocessing . PolynomialExtender () + preprocessing . RBFSampler ()) | linear_model . LinearRegression () ) model . draw () Note that the + symbol acts as a shorthand notation for creating a compose.TransformerUnion , which means that we could have declared the above pipeline as so: model = ( preprocessing . StandardScaler () | compose . TransformerUnion ( preprocessing . PolynomialExtender (), preprocessing . RBFSampler () ) | linear_model . LinearRegression () ) Pipelines provide the benefit of removing a lot of cruft by taking care of tedious details for you. They also enable to clearly define what steps your model is made of. Finally, having your model in a single object means that you can move it around more easily. Note that you can include user-defined functions in a pipeline by using a compose.FuncTransformer .","title":"Pipelines"},{"location":"user-guide/pipelines/#pipelines","text":"Pipelines are an integral part of creme. We encourage their usage and apply them in many of their examples. The compose.Pipeline contains all the logic for building and applying pipelines. A pipeline is essentially a list of estimators that are applied in sequence. The only requirement is that the first n - 1 steps be transformers. The last step can be a regressor, a classifier, a clusterer, a transformer, etc. Here is an example: from creme import compose from creme import linear_model from creme import preprocessing model = compose . Pipeline ( preprocessing . StandardScaler (), preprocessing . PolynomialExtender (), linear_model . LinearRegression () ) You can also use the | operator, as so: model = ( preprocessing . StandardScaler () | preprocessing . PolynomialExtender () | linear_model . LinearRegression () ) Or, equally: model = preprocessing . StandardScaler () model |= preprocessing . PolynomialExtender () model |= linear_model . LinearRegression () A pipeline has a draw method that can be used to visualize it: model . draw () compose.Pipeline inherits from base.Estimator , which means that it has a learn_one method. You would expect learn_one to update each estimator, but that's not actually what happens . Instead, the transformers are updated when predict_one (or predict_proba_one for that matter) is called. Indeed, in online machine learning, we can update the unsupervised parts of our model when a sample arrives. We don't have to wait for the ground truth to arrive in order to update unsupervised estimators that don't depend on it. In other words, in a pipeline, learn_one updates the supervised parts, whilst predict_one updates the unsupervised parts. It's important to be aware of this behavior, as it is quite different to what is done in other libraries that rely on batch machine learning. Here is a small example to illustrate the previous point: from creme import datasets dataset = datasets . TrumpApproval () x , y = next ( iter ( dataset )) x , y ({'ordinal_date': 736389, 'gallup': 43.843213, 'ipsos': 46.19925042857143, 'morning_consult': 48.318749, 'rasmussen': 44.104692, 'you_gov': 43.636914000000004}, 43.75505) Let us call predict_one , which will update each transformer, but won't update the linear regression. model . predict_one ( x ) 0.0 The prediction is nil because each weight of the linear regression is equal to 0. model [ 'StandardScaler' ] . means defaultdict(float, {'ordinal_date': 736389.0, 'gallup': 43.843213, 'ipsos': 46.19925042857143, 'morning_consult': 48.318749, 'rasmussen': 44.104692, 'you_gov': 43.636914000000004}) As we can see, the means of each feature have been updated, even though we called predict_one and not learn_one . Note that if you call transform_one with a pipeline who's last step is not a transformer, then the output from the last transformer (which is thus the penultimate step) will be returned: model . transform_one ( x ) {'ordinal_date': 0.0, 'gallup': 0.0, 'ipsos': 0.0, 'morning_consult': 0.0, 'rasmussen': 0.0, 'you_gov': 0.0, 'ordinal_date*ordinal_date': 0.0, 'ordinal_date*gallup': 0.0, 'ordinal_date*ipsos': 0.0, 'ordinal_date*morning_consult': 0.0, 'ordinal_date*rasmussen': 0.0, 'ordinal_date*you_gov': 0.0, 'gallup*gallup': 0.0, 'gallup*ipsos': 0.0, 'gallup*morning_consult': 0.0, 'gallup*rasmussen': 0.0, 'gallup*you_gov': 0.0, 'ipsos*ipsos': 0.0, 'ipsos*morning_consult': 0.0, 'ipsos*rasmussen': 0.0, 'ipsos*you_gov': 0.0, 'morning_consult*morning_consult': 0.0, 'morning_consult*rasmussen': 0.0, 'morning_consult*you_gov': 0.0, 'rasmussen*rasmussen': 0.0, 'rasmussen*you_gov': 0.0, 'you_gov*you_gov': 0.0} In many cases, you might want to connect a step to multiple steps. For instance, you might to extract different kinds of features from a single input. An elegant way to do this is to use a compose.TransformerUnion . Essentially, the latter is a list of transformers who's results will be merged into a single dict when transform_one is called. As an example let's say that we want to apply a preprocessing.RBFSampler as well as the preprocessing.PolynomialExtender . This may be done as so: model = ( preprocessing . StandardScaler () | ( preprocessing . PolynomialExtender () + preprocessing . RBFSampler ()) | linear_model . LinearRegression () ) model . draw () Note that the + symbol acts as a shorthand notation for creating a compose.TransformerUnion , which means that we could have declared the above pipeline as so: model = ( preprocessing . StandardScaler () | compose . TransformerUnion ( preprocessing . PolynomialExtender (), preprocessing . RBFSampler () ) | linear_model . LinearRegression () ) Pipelines provide the benefit of removing a lot of cruft by taking care of tedious details for you. They also enable to clearly define what steps your model is made of. Finally, having your model in a single object means that you can move it around more easily. Note that you can include user-defined functions in a pipeline by using a compose.FuncTransformer .","title":"Pipelines"},{"location":"user-guide/reading-data/","text":"Reading data \u00b6 In creme , the features of a sample are stored inside a dictionary, which in Python is called a dict . In other words, we don't use any sophisticated data structure, such as a numpy.ndarray or a pandas.DataFrame . The main advantage of using plain dict s is that it removes the overhead that comes with using the aforementioned data structures. This is important in a streaming context because we want to be able to process many individual samples in rapid succession. Another advantage is that dict s allow us to give names to our features. Finally, dict s are not typed, and can therefore store heterogenous data. Another advantage which we haven't mentionned is that dict s play nicely with Python's standard library. Indeed, Python contains many tools that allow manipulating dict s. For instance, the csv.DictReader can be used to read a CSV file and convert each row to a dict . The stream.iter_csv method from creme is a higher-level function that wraps csv.DictReader and adds a few bells and whistles. As an example, let's read the data from the datasets.Bikes dataset: from creme import datasets bikes = datasets . Bikes () bikes Bikes dataset Task Regression Number of samples 182,470 Number of features 8 Sparse False Path /Users/mhalford/creme_data/Bikes/toulouse_bikes.csv URL https://maxhalford.github.io/files/datasets/toulouse_bikes.zip Size 12.52 MB Downloaded True Let's take a look at the first sample: x , y = next ( iter ( bikes )) x {'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'station': 'metro-canal-du-midi', 'clouds': 75, 'description': 'light rain', 'humidity': 81, 'pressure': 1017.0, 'temperature': 6.54, 'wind': 9.3} As we can see, the values have different types. Let's try to do this ourselves by using stream.iter_csv . from creme import stream X_y = stream . iter_csv ( bikes . path ) x , y = next ( X_y ) x , y ({'moment': '2016-04-01 00:00:07', 'bikes': '1', 'station': 'metro-canal-du-midi', 'clouds': '75', 'description': 'light rain', 'humidity': '81', 'pressure': '1017.0', 'temperature': '6.54', 'wind': '9.3'}, None) There are a couple things that are wrong. First of all, the numeric features have not been casted into numbers. Indeed, by default that stream.iter_csv assumes that everything is a string. A related issue is that the moment field hasn't been parsed into a datetime . Finally, the target field, which is bikes , hasn't been separated from the rest of the features. We can remedy to these issues by setting a few parameters: X_y = stream . iter_csv ( bikes . path , converters = { 'bikes' : int , 'clouds' : int , 'humidity' : int , 'pressure' : float , 'temperature' : float , 'wind' : float }, parse_dates = { 'moment' : '%Y-%m- %d %H:%M:%S' }, target = 'bikes' ) x , y = next ( X_y ) x , y ({'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'station': 'metro-canal-du-midi', 'clouds': 75, 'description': 'light rain', 'humidity': 81, 'pressure': 1017.0, 'temperature': 6.54, 'wind': 9.3}, 1) That's much better. We invite you to take a look at the stream module to see for yourself what other methods are available. Note that creme is first and foremost a machine learning library, and therefore isn't as much concerned about reading data as it is about statistical algorithms. We do however believe that the fact that we use dictionary gives you, the user, a lot of freedom and flexibility. To conclude, let us shortly mention between proactive learning and reactive learning in the specific context of online machine learning. When we loop over a data with a for loop, we have the control over the data and the order in which it arrives. We are proactive in the sense that we, the user, are asking for the data to arrive. In contract, in a reactive situation, we don't have control on the data arrival. A typical example of such a situation is web server, where web requests arrive in an arbitrary order. This is a situation where creme shines. For instance, in a Flask application, you could define a route to make predictions with a creme model as so: import flask app = flask . Flask ( __name__ ) @app . route ( '/' , methods = [ 'GET' ]) def predict (): payload = flask . request . json creme_model = load_model () return creme_model . predict_proba_one ( payload ) Likewise, a model can be updated whenever a request arrives as so: @app . route ( '/' , methods = [ 'POST' ]) def learn (): payload = flask . request . json creme_model = load_model () creme_model . learn_one ( payload [ 'features' ], payload [ 'target' ]) return {}, 201 To summarize, creme can be used in many different ways. The fact that it uses dictionaries to represent features provides a lot of flexibility and space for creativity.","title":"Reading data"},{"location":"user-guide/reading-data/#reading-data","text":"In creme , the features of a sample are stored inside a dictionary, which in Python is called a dict . In other words, we don't use any sophisticated data structure, such as a numpy.ndarray or a pandas.DataFrame . The main advantage of using plain dict s is that it removes the overhead that comes with using the aforementioned data structures. This is important in a streaming context because we want to be able to process many individual samples in rapid succession. Another advantage is that dict s allow us to give names to our features. Finally, dict s are not typed, and can therefore store heterogenous data. Another advantage which we haven't mentionned is that dict s play nicely with Python's standard library. Indeed, Python contains many tools that allow manipulating dict s. For instance, the csv.DictReader can be used to read a CSV file and convert each row to a dict . The stream.iter_csv method from creme is a higher-level function that wraps csv.DictReader and adds a few bells and whistles. As an example, let's read the data from the datasets.Bikes dataset: from creme import datasets bikes = datasets . Bikes () bikes Bikes dataset Task Regression Number of samples 182,470 Number of features 8 Sparse False Path /Users/mhalford/creme_data/Bikes/toulouse_bikes.csv URL https://maxhalford.github.io/files/datasets/toulouse_bikes.zip Size 12.52 MB Downloaded True Let's take a look at the first sample: x , y = next ( iter ( bikes )) x {'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'station': 'metro-canal-du-midi', 'clouds': 75, 'description': 'light rain', 'humidity': 81, 'pressure': 1017.0, 'temperature': 6.54, 'wind': 9.3} As we can see, the values have different types. Let's try to do this ourselves by using stream.iter_csv . from creme import stream X_y = stream . iter_csv ( bikes . path ) x , y = next ( X_y ) x , y ({'moment': '2016-04-01 00:00:07', 'bikes': '1', 'station': 'metro-canal-du-midi', 'clouds': '75', 'description': 'light rain', 'humidity': '81', 'pressure': '1017.0', 'temperature': '6.54', 'wind': '9.3'}, None) There are a couple things that are wrong. First of all, the numeric features have not been casted into numbers. Indeed, by default that stream.iter_csv assumes that everything is a string. A related issue is that the moment field hasn't been parsed into a datetime . Finally, the target field, which is bikes , hasn't been separated from the rest of the features. We can remedy to these issues by setting a few parameters: X_y = stream . iter_csv ( bikes . path , converters = { 'bikes' : int , 'clouds' : int , 'humidity' : int , 'pressure' : float , 'temperature' : float , 'wind' : float }, parse_dates = { 'moment' : '%Y-%m- %d %H:%M:%S' }, target = 'bikes' ) x , y = next ( X_y ) x , y ({'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'station': 'metro-canal-du-midi', 'clouds': 75, 'description': 'light rain', 'humidity': 81, 'pressure': 1017.0, 'temperature': 6.54, 'wind': 9.3}, 1) That's much better. We invite you to take a look at the stream module to see for yourself what other methods are available. Note that creme is first and foremost a machine learning library, and therefore isn't as much concerned about reading data as it is about statistical algorithms. We do however believe that the fact that we use dictionary gives you, the user, a lot of freedom and flexibility. To conclude, let us shortly mention between proactive learning and reactive learning in the specific context of online machine learning. When we loop over a data with a for loop, we have the control over the data and the order in which it arrives. We are proactive in the sense that we, the user, are asking for the data to arrive. In contract, in a reactive situation, we don't have control on the data arrival. A typical example of such a situation is web server, where web requests arrive in an arbitrary order. This is a situation where creme shines. For instance, in a Flask application, you could define a route to make predictions with a creme model as so: import flask app = flask . Flask ( __name__ ) @app . route ( '/' , methods = [ 'GET' ]) def predict (): payload = flask . request . json creme_model = load_model () return creme_model . predict_proba_one ( payload ) Likewise, a model can be updated whenever a request arrives as so: @app . route ( '/' , methods = [ 'POST' ]) def learn (): payload = flask . request . json creme_model = load_model () creme_model . learn_one ( payload [ 'features' ], payload [ 'target' ]) return {}, 201 To summarize, creme can be used in many different ways. The fact that it uses dictionaries to represent features provides a lot of flexibility and space for creativity.","title":"Reading data"}]}